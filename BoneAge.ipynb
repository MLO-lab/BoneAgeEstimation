{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95890c3f",
   "metadata": {},
   "source": [
    "# Bone Age Estimation based on the Spatial Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01b3be",
   "metadata": {},
   "source": [
    "##  0. Libraries and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9eec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d, Rbf\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.optimize import curve_fit, minimize, root_scalar\n",
    "from scipy.special import rel_entr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import gaussian_kde, wasserstein_distance\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.lines import Line2D  \n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "from skimage import measure, morphology\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# Spatial\n",
    "import geopandas as gpd\n",
    "from matplotlib.path import Path\n",
    "from shapely.geometry import Polygon, box\n",
    "\n",
    "# File reader\n",
    "from imaris_ims_file_reader.ims import ims\n",
    "\n",
    "# Machine learning\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    ")\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3544420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory with timestamp\n",
    "results_dir = \"results_bone_age\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define directory to save models\n",
    "model_save_dir = os.path.join(results_dir, \"models\")\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Increase the maximum image size limit\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Set grid off by default\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "# Set default font sizes globally\n",
    "plt.rcParams[\"font.size\"] = 12          # General font size\n",
    "plt.rcParams[\"legend.fontsize\"] = 12    # Legend font size\n",
    "plt.rcParams[\"axes.titlesize\"] = 14      # Title font size\n",
    "plt.rcParams[\"axes.labelsize\"] = 12     # X and Y axis labels\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12    # X-axis tick labels\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12     # Y-axis tick labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40c01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "hsc_rd_colors = {\n",
    "    \"HSCs\": \"#FF0000\", # Red\n",
    "    \"RDs\": \"#808080\" # Grey\n",
    "}\n",
    "\n",
    "hsc_color = {\"HSCs\": \"#FF0000\"}\n",
    "\n",
    "ckits_color_map = {\n",
    "    (\"cKits\", 0): \"#B15928\",  # Brown\n",
    "    (\"cKits\", 1): \"#6A3D9A\",  # Purple\n",
    "    (\"cKits\", 2): \"#FF7F00\",  # Orange\n",
    "    (\"cKits\", 3): \"#FDBF6F\",  # Light Orange\n",
    "    (\"cKits\", 4): \"#A6CEE3\",  # Light Blue\n",
    "    (\"cKits\", 5): \"#FB9A99\",  # Pink\n",
    "    (\"cKits\", 6): \"#CAB2D6\",  # Lavender\n",
    "    (\"cKits\", 7): \"#1F78B4\",  # Blue\n",
    "    (\"cKits\", 8): \"#B2DF8A\",  # Light Green\n",
    "    (\"cKits\", 9): \"#33A02C\"   # Green\n",
    "}\n",
    "\n",
    "cluster_color_map = {\n",
    "    0: \"#B15928\",  # Brown\n",
    "    1: \"#6A3D9A\",  # Purple\n",
    "    2: \"#FF7F00\",  # Orange\n",
    "    3: \"#FDBF6F\",  # Light Orange\n",
    "    4: \"#A6CEE3\",  # Light Blue\n",
    "    5: \"#FB9A99\",  # Pink\n",
    "    6: \"#CAB2D6\",  # Lavender\n",
    "    7: \"#1F78B4\",  # Blue\n",
    "    8: \"#B2DF8A\",  # Light Green\n",
    "    9: \"#33A02C\"   # Green\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Others\n",
    "features_name_dict = {\n",
    "    \"cKit Density Divergence (vs. 3mo)\": \"HeM distribution(vs. 3mo)\",\n",
    "    \"cKit Density Divergence (vs. 12mo)\": \"HeM distribution(vs. 12mo)\",\n",
    "    \"cKit Density Divergence (vs. 20mo)\": \"HeM distribution(vs. 20mo)\",\n",
    "    \"cKit Neighborhood Affinity (vs. 3mo)\": \"HeM neighborhood(vs. 3mo)\",\n",
    "    \"cKit Neighborhood Affinity (vs. 12mo)\": \"HeM neighborhood(vs. 12mo)\",\n",
    "    \"cKit Neighborhood Affinity (vs. 20mo)\": \"HeM neighborhood(vs. 20mo)\",\n",
    "    \"HSC Density Divergence (vs. 3mo)\": \"HSC distribution(vs. 3mo)\",\n",
    "    \"HSC Density Divergence (vs. 12mo)\": \"HSC distribution(vs. 12mo)\",\n",
    "    \"HSC Density Divergence (vs. 20mo)\": \"HSC distribution(vs. 20mo)\",\n",
    "    \"HSC Count\": \"HSC numbers\",\n",
    "    \"HSC Spatial Similarity (vs. 3mo)\": \"HSC-HeM association(vs. 3mo)\",\n",
    "    \"HSC Spatial Similarity (vs. 12mo)\": \"HSC-HeM association(vs. 12mo)\",\n",
    "    \"HSC Spatial Similarity (vs. 20mo)\": \"HSC-HeM association(vs. 20mo)\",\n",
    "    \"HSC Composition (vs. 3mo)\": \"HSC composition(vs. 3mo)\",\n",
    "    \"HSC Composition (vs. 12mo)\": \"HSC composition(vs. 12mo)\",\n",
    "    \"HSC Composition (vs. 20mo)\": \"HSC composition(vs. 20mo)\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de42d46",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "- csv files for the pdf estimation and pseudo age calculation\n",
    "- ims files for metabone creation and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503f226",
   "metadata": {},
   "source": [
    "### 1.1 Load position (csv) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data_bone_age\"\n",
    "# Load the positions files (cKits, HSCs, and RDs)\n",
    "position_files = [f for f in os.listdir(data_dir)]\n",
    "position_dirs = [os.path.join(data_dir, f) for f in position_files if f.endswith(\"csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\"Position.X\", \"Position.Y\", \"Position.Z\", \"age\", \"clusters\",\"bone\"] # based on the columns of data and also the interest\n",
    "\n",
    "# Initialize a dictionary to hold the data\n",
    "positions_dict = {\n",
    "    \"HSCs\": None,\n",
    "    \"RDs\": None,\n",
    "    \"cKits\": None\n",
    "}\n",
    "\n",
    "for position_dir in position_dirs:\n",
    "\n",
    "    # Read the CSV and filter columns\n",
    "    positions = pd.read_csv(position_dir)\n",
    "    positions = positions[columns_to_use]\n",
    "\n",
    "    # Assign the processed data to the appropriate key in the dictionary\n",
    "    if \"hsc\" in position_dir:\n",
    "        positions[\"source\"] = \"HSCs\"\n",
    "        positions_dict[\"HSCs\"] = positions\n",
    "    elif \"rd\" in position_dir:\n",
    "        positions[\"source\"] = \"RDs\"\n",
    "        positions_dict[\"RDs\"] = positions\n",
    "    elif \"ckit\" in position_dir:\n",
    "        positions[\"source\"] = \"cKits\"\n",
    "        positions_dict[\"cKits\"] = positions\n",
    "\n",
    "# Merge the df in the dict\n",
    "positions = pd.concat(positions_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac778dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Drop the data with age 5fu45d\n",
    "positions = positions[positions.age != \"5fu45d\"]\n",
    "\n",
    "# Scale the positions to pixels by dividing by the scaling factor\n",
    "scaling_factor = {\"Position.X\":0.7575, \"Position.Y\":0.7575, \"Position.Z\":2.5}\n",
    "positions[[\"Position.X\", \"Position.Y\", \"Position.Z\"]] = positions[[\"Position.X\", \"Position.Y\", \"Position.Z\"]].div(scaling_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007fce72",
   "metadata": {},
   "source": [
    "### 1.2 Load ims files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the ims files\n",
    "def get_ims_files_by_conditions(data_dir, keys=[\"3mo\", \"12mo\", \"20mo\", \"5fu30d\", \"5fu60d\"], source = \"Bone\"):\n",
    "    # Initialize a dictionary to store .ims files for each key\n",
    "    ims_files = {key: [] for key in keys}\n",
    "    \n",
    "    # Find all directories in data_dir, excluding .ims files\n",
    "    all_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    # Classify the directories into the specified keys\n",
    "    for data_subdir in all_dirs:\n",
    "        sub_files = [os.path.join(data_subdir, f) for f in os.listdir(data_subdir) if f.endswith(\".ims\") and source in f]\n",
    "        # Check which key the directory matches\n",
    "        matched = False\n",
    "        for key in keys:\n",
    "            if key in os.path.basename(data_subdir):\n",
    "                ims_files[key].extend(sub_files)\n",
    "                matched = True\n",
    "                break\n",
    "        # If no key matches, classify as \"other\"\n",
    "        if not matched:\n",
    "            if \"other\" not in ims_files:\n",
    "                ims_files[\"other\"] = []\n",
    "            ims_files[\"other\"].extend(sub_files)\n",
    "\n",
    "    # Return the dictionary with the results\n",
    "    return ims_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5211ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .ims files\n",
    "metabone_ims_files = get_ims_files_by_conditions(data_dir = data_dir, keys=[\"3mo\", \"12mo\", \"20mo\", \"5fu30d\", \"5fu60d\"], source = \"Bone\")\n",
    "ims_files_3mo = metabone_ims_files[\"3mo\"]\n",
    "ims_files_12mo = metabone_ims_files[\"12mo\"]\n",
    "ims_files_20mo = metabone_ims_files[\"20mo\"]\n",
    "ims_files_5fu30d = metabone_ims_files[\"5fu30d\"]\n",
    "ims_files_5fu60d = metabone_ims_files[\"5fu60d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the bone images with non-zero values to dataframes with Position.X and Position.Y columns\n",
    "def image_to_df(image):\n",
    "    # Find the non-zero intensities and their coordinates\n",
    "    non_zero_coords = np.nonzero(image)  # Returns the y, x coordinates where intensity > 0\n",
    "    # intensity_values = image[non_zero_coords]  # Extract the intensity values\n",
    "\n",
    "    # Create a DataFrame from the non-zero coordinates and intensity values\n",
    "    df = pd.DataFrame({\n",
    "        \"source\": \"DAPI\", # depending on the source we can rename it \n",
    "        \"Position.Z\": 0, # non_zero_coords[0],\n",
    "        \"Position.Y\": non_zero_coords[0],\n",
    "        \"Position.X\": non_zero_coords[1]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to process and store images for a given condition\n",
    "def process_bone_images(bone_ims_files):\n",
    "    bone_dict = {}\n",
    "    \n",
    "    for i in bone_ims_files:\n",
    "        # Extract bone name from the file path\n",
    "        bone_name = \"_\".join(i.split(\"/\")[-1].split(\" \")[:2])\n",
    "        \n",
    "        # Load and process the image (assuming ims function is defined elsewhere)\n",
    "        bone_img = ims(i)  # You need to ensure that ims(i) correctly loads the image\n",
    "        # bone_img = bone_img[0, bone_img.shape[1] - 1, :, :, :].max(axis=0).copy()\n",
    "        bone_img = bone_img[0, 1, :, :, :].max(axis=0).copy() # DAPI channel is the second channel\n",
    "        # Morphological operations, labeling and remove small objects\n",
    "        # Threshold the image to create a binary mask\n",
    "        binary_img = bone_img > 0\n",
    "        # Label the binary image\n",
    "        labeled_img = measure.label(binary_img)\n",
    "        # Remove small objects (20000)\n",
    "        cleaned_img = morphology.remove_small_objects(labeled_img, min_size=20000)\n",
    "        # Convert the cleaned image back to binary\n",
    "        bone_img_df = image_to_df(cleaned_img)\n",
    "        # Store the processed image in the dictionary with the extracted name\n",
    "        bone_dict[bone_name] = bone_img_df\n",
    "\n",
    "    return bone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bone_3mo = process_bone_images(ims_files_3mo)\n",
    "bone_12mo = process_bone_images(ims_files_12mo)\n",
    "bone_20mo = process_bone_images(ims_files_20mo)\n",
    "bone_5fu30d = process_bone_images(ims_files_5fu30d)\n",
    "bone_5fu60d = process_bone_images(ims_files_5fu60d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4138d6",
   "metadata": {},
   "source": [
    "### 1.3 Load cluster affinity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the affinity matrices for each condition\n",
    "affinity_matrices_dir = os.path.join(data_dir, \"affinity_matrices\")\n",
    "\n",
    "# Load the affinity matrices\n",
    "affinity_matrices = {}\n",
    "for f in os.listdir(affinity_matrices_dir):\n",
    "    # f should start with cluster, and end with sum.csv\n",
    "    if not f.startswith(\"affinity\") or not f.endswith(\"sum.csv\"):\n",
    "        continue\n",
    "    # Load the affinity matrix (csv)\n",
    "    # add column names 0-9\n",
    "    affinity_matrix = pd.read_csv(os.path.join(affinity_matrices_dir, f), header=None)\n",
    "    affinity_matrix.columns = range(10)\n",
    "    # Extract the condition name\n",
    "    condition_name = f.split(\"_\")[2]\n",
    "    # Store the affinity matrix in the dictionary\n",
    "    affinity_matrices[condition_name] = affinity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acb9f2",
   "metadata": {},
   "source": [
    "## 2. Data Inspection and Preprocessing\n",
    "- visualization\n",
    "- preparation for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the images \n",
    "for ims_files in [ims_files_3mo, ims_files_12mo, ims_files_20mo, ims_files_5fu30d, ims_files_5fu60d]:\n",
    "# for ims_files in [ims_files_5fu30d]:\n",
    "    for i in ims_files:\n",
    "        print(i)\n",
    "        img = ims(i)\n",
    "        print(img.shape)\n",
    "        bone_img = img[0,0,:,:,:].max(axis=0)\n",
    "        bone_img = bone_img > 0\n",
    "\n",
    "        dapi_img = img[0,1,:,:,:].max(axis=0)\n",
    "        dapi_img = dapi_img > 0\n",
    "        \n",
    "        # Label the binary image\n",
    "        labeled_img = measure.label(dapi_img)\n",
    "        # Remove small objects\n",
    "        cleaned_img = morphology.remove_small_objects(labeled_img, min_size=20000)\n",
    "        cleaned_img = cleaned_img > 0\n",
    "        # Show the two images together\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(15, 15))\n",
    "        ax[0].imshow(bone_img, cmap=\"gray\")\n",
    "        ax[0].set_title(\"Bone Image\")\n",
    "        \n",
    "        ax[1].imshow(dapi_img, cmap=\"gray\")\n",
    "        ax[1].set_title(\"DAPI Image\")\n",
    "\n",
    "        ax[2].imshow(cleaned_img, cmap=\"gray\")\n",
    "        ax[2].set_title(\"Cleaned DAPI Image\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bone_dicts = {\n",
    "    \"3mo\": bone_3mo,\n",
    "    \"12mo\": bone_12mo,\n",
    "    \"20mo\": bone_20mo,\n",
    "    \"5fu30d\": bone_5fu30d,\n",
    "    \"5fu60d\": bone_5fu60d\n",
    "}\n",
    "\n",
    "# Initialize a list to hold all processed DataFrames\n",
    "bone_dfs = []\n",
    "\n",
    "# Process each dictionary\n",
    "for age, bone_dict in bone_dicts.items():\n",
    "    for bone, df in bone_dict.items():\n",
    "        # Add the required columns\n",
    "        df[\"age\"] = age\n",
    "        df[\"clusters\"] = None  # Set clusters as NaN\n",
    "        df[\"bone\"] = bone\n",
    "        # Reorder columns to match the positions DataFrame\n",
    "        df = df[[\"Position.X\", \"Position.Y\", \"Position.Z\", \"age\", \"clusters\", \"bone\", \"source\"]]\n",
    "        bone_dfs.append(df)\n",
    "\n",
    "# Concatenate all processed DataFrames along with the positions DataFrame\n",
    "positions = pd.concat([positions] + bone_dfs, ignore_index=True)\n",
    "\n",
    "# Delete bone_3mo, bone_12mo, bone_20mo, bone_5fu30d, bone_5fu60d, bone_dicts, bone_dfs\n",
    "del bone_3mo, bone_12mo, bone_20mo, bone_5fu30d, bone_5fu60d, bone_dicts, bone_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfff52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the bone images and positions based on the visual inspection\n",
    "bone_names = positions[\"bone\"].unique()\n",
    "bone_y_flips = {bone: False for bone in [\"210722KK_st4\", \"210722KK_st6\", \"210722KK_st8\", \"211110KK_st2\", \"211110KK_st5\", \"211128KK_st1\", \"220202KK_st8\", \"211110KK_st6\", \"211110KK_st8\", \"211110KK_st9\"]}\n",
    "bone_y_flips.update({bone: True for bone in bone_names if bone not in bone_y_flips})\n",
    "\n",
    "# Precompute y_min and y_max for all bones\n",
    "y_bounds = positions.groupby(\"bone\")[\"Position.Y\"].agg([\"min\", \"max\"]).to_dict(\"index\")\n",
    "\n",
    "# Apply transformations in a vectorized way\n",
    "positions[\"y_flip\"] = positions[\"bone\"].map(bone_y_flips)\n",
    "positions[\"y_min\"] = positions[\"bone\"].map({bone: bounds[\"min\"] for bone, bounds in y_bounds.items()})\n",
    "positions[\"y_max\"] = positions[\"bone\"].map({bone: bounds[\"max\"] for bone, bounds in y_bounds.items()})\n",
    "\n",
    "# Apply the flipping logic\n",
    "positions[\"Position.Y\"] = np.where(\n",
    "    positions[\"y_flip\"],\n",
    "    positions[\"y_max\"] - positions[\"Position.Y\"],\n",
    "    positions[\"Position.Y\"] - positions[\"y_min\"]\n",
    ")\n",
    "\n",
    "# Drop helper columns\n",
    "positions.drop([\"y_flip\", \"y_min\", \"y_max\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384a9d8",
   "metadata": {},
   "source": [
    "## 3. Bone Alignment and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f97d60",
   "metadata": {},
   "source": [
    "### 3.1 Bone alignment with the reference bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outline of the bone in the x-y plane\n",
    "def find_outline(points, window_size=10):\n",
    "    \"\"\"\n",
    "    Find the outline of a set of points by finding the min and max y-values for each x-value within a window.\n",
    "    The outline is only in the x-y plane.\n",
    "    \n",
    "    Parameters:\n",
    "    points : np.array of shape (n,2) points.\n",
    "    window_size : size of the window to smooth the outline.\n",
    "    Returns:\n",
    "    outline_points : np.array of outline points.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(points, columns=[\"x\", \"y\"])\n",
    "    \n",
    "    min_y_points = []\n",
    "    max_y_points = []\n",
    "\n",
    "    # Sort points by x value\n",
    "    df_sorted = df.sort_values(by=\"x\")\n",
    "    \n",
    "    # Slide over the x values with a window\n",
    "    for i in range(0, len(df_sorted), window_size):\n",
    "        window = df_sorted.iloc[i:i + window_size]\n",
    "        min_y = window.loc[window[\"y\"].idxmin()]\n",
    "        max_y = window.loc[window[\"y\"].idxmax()]\n",
    "        min_y_points.append(min_y)\n",
    "        max_y_points.append(max_y)\n",
    "    \n",
    "    # Ensure the outline is in order\n",
    "    min_y_points = pd.DataFrame(min_y_points).drop_duplicates().sort_values(by=\"x\").values\n",
    "    max_y_points = pd.DataFrame(max_y_points).drop_duplicates().sort_values(by=\"x\", ascending=False).values\n",
    "\n",
    "    # Combine min_y and max_y points and close the loop\n",
    "    outline_points = np.vstack([min_y_points, max_y_points, min_y_points[0]])\n",
    "\n",
    "    return outline_points\n",
    "\n",
    "# Find the center of each bone (outline) and put the center of the bones at the same position\n",
    "def calculate_centroid(outline_points):\n",
    "    \"\"\"\n",
    "    Calculate the centroid of the bone outline.\n",
    "    \n",
    "    Parameters:\n",
    "    outline_points : np.array of shape (n, 2)\n",
    "    \n",
    "    Returns:\n",
    "    centroid : tuple containing (centroid_x, centroid_y)\n",
    "    \"\"\"\n",
    "    # Use the weight centroid\n",
    "    centroid_x = np.mean(outline_points[:, 0])\n",
    "    centroid_y = np.mean(outline_points[:, 1])\n",
    "    \n",
    "    # Using the middle of the x and y values as the centroid\n",
    "    # x_min, x_max = outline_points[:, 0].min(), outline_points[:, 0].max()\n",
    "    # y_min, y_max = outline_points[:, 1].min(), outline_points[:, 1].max()\n",
    "    # centroid_x = (x_min + x_max) / 2\n",
    "    # centroid_y = (y_min + y_max) / 2\n",
    "    \n",
    "    return centroid_x, centroid_y\n",
    "\n",
    "def translate_to_origin(outline_points, centroid):\n",
    "    \"\"\"\n",
    "    Translate the outline points so that the centroid is at the origin.\n",
    "    \n",
    "    Parameters:\n",
    "    outline_points : np.array of shape (n, 2)\n",
    "    centroid : tuple containing (centroid_x, centroid_y)\n",
    "    \n",
    "    Returns:\n",
    "    translated_points : np.array of shape (n, 2)\n",
    "    \"\"\"\n",
    "    translated_points = outline_points.astype(np.float64).copy()\n",
    "    \n",
    "    translated_points[:, 0] -= centroid[0]\n",
    "    translated_points[:, 1] -= centroid[1]\n",
    "    return translated_points\n",
    "\n",
    "# Rescale the bones to the same size (bounding box)(optional)\n",
    "def get_max_dimensions(bone_dicts):\n",
    "    \"\"\"\n",
    "    Find the maximum width and height across all bone outlines in the given dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    bone_dicts : list of dictionaries of bones (where each value is a DataFrame with \"Position.X\" and \"Position.Y\")\n",
    "    \n",
    "    Returns:\n",
    "    max_width : float, maximum width found across all bones\n",
    "    max_height : float, maximum height found across all bones\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "    \n",
    "    for bone_dict in bone_dicts:\n",
    "        for df in bone_dict.values():\n",
    "            # Extract bone points where \"source\" == \"Bone\"\n",
    "            bone_points = df[df[\"source\"] == \"Bone\"][[\"Position.X\", \"Position.Y\"]].values\n",
    "            \n",
    "            # Find min and max values of x and y\n",
    "            min_x, max_x = bone_points[:, 0].min(), bone_points[:, 0].max()\n",
    "            min_y, max_y = bone_points[:, 1].min(), bone_points[:, 1].max()\n",
    "            \n",
    "            # Calculate width and height\n",
    "            width = max_x - min_x\n",
    "            height = max_y - min_y\n",
    "            \n",
    "            # Update maximum width and height if necessary\n",
    "            if width > max_width:\n",
    "                max_width = width\n",
    "            if height > max_height:\n",
    "                max_height = height\n",
    "                \n",
    "    return max_width, max_height\n",
    "\n",
    "def rescale_outline(outline_points, max_width, max_height):\n",
    "    \"\"\"\n",
    "    Rescale the bone outline to fit within the maximum width and height across all bones.\n",
    "    \n",
    "    Parameters:\n",
    "    outline_points : np.array of shape (n, 2)\n",
    "    max_width : float, the maximum width across all bones\n",
    "    max_height : float, the maximum height across all bones\n",
    "    \n",
    "    Returns:\n",
    "    scaled_points : np.array of shape (n, 2)\n",
    "    \"\"\"\n",
    "    min_x, max_x = outline_points[:, 0].min(), outline_points[:, 0].max()\n",
    "    min_y, max_y = outline_points[:, 1].min(), outline_points[:, 1].max()\n",
    "\n",
    "    current_width = max_x - min_x\n",
    "    current_height = max_y - min_y\n",
    "\n",
    "    scale_x = max_width / current_width\n",
    "    scale_y = max_height / current_height\n",
    "\n",
    "    scaled_points = outline_points.copy()\n",
    "    scaled_points[:, 0] *= scale_x\n",
    "    scaled_points[:, 1] *= scale_y\n",
    "\n",
    "    return scaled_points\n",
    "\n",
    "# Calculate the overlap area on the grid inside the outline\n",
    "def calculate_overlap_area(reference_outline, target_outline, resolution=200):\n",
    "    \"\"\"\n",
    "    Calculate the overlap area (in terms of pixels or points) between two outlines.\n",
    "    \n",
    "    Parameters:\n",
    "    reference_outline : np.array of shape (n, 2), outline of the reference bone\n",
    "    target_outline : np.array of shape (n, 2), outline of the target bone\n",
    "    resolution : int, the number of points or pixels to use for the area calculation.\n",
    "    \n",
    "    Returns:\n",
    "    overlap_area : float, the number of pixels or points where the areas overlap.\n",
    "    \"\"\"\n",
    "    # Ensure the outlines are 2D arrays of shape (N, 2)\n",
    "    reference_outline = np.asarray(reference_outline).reshape(-1, 2)\n",
    "    target_outline = np.asarray(target_outline).reshape(-1, 2)\n",
    "    \n",
    "    # Calculate centroids for both bones\n",
    "    reference_centroid = calculate_centroid(reference_outline)\n",
    "    target_centroid = calculate_centroid(target_outline)\n",
    "    \n",
    "    # Translate both bones to center them\n",
    "    reference_outline_centered = translate_to_origin(reference_outline, reference_centroid)\n",
    "    target_outline_centered = translate_to_origin(target_outline, target_centroid)\n",
    "    \n",
    "    # Get bounding box of the reference outline\n",
    "    min_x, max_x = reference_outline_centered[:, 0].min(), reference_outline_centered[:, 0].max()\n",
    "    min_y, max_y = reference_outline_centered[:, 1].min(), reference_outline_centered[:, 1].max()\n",
    "    \n",
    "    # Generate grid of points (pixels) covering the bounding box\n",
    "    x_grid = np.linspace(min_x, max_x, resolution)\n",
    "    y_grid = np.linspace(min_y, max_y, resolution)\n",
    "    xv, yv = np.meshgrid(x_grid, y_grid)\n",
    "    grid_points = np.vstack([xv.ravel(), yv.ravel()]).T\n",
    "\n",
    "    # Create Path objects for the reference and target outlines\n",
    "    reference_path = Path(reference_outline_centered)\n",
    "    target_path = Path(target_outline_centered)\n",
    "    \n",
    "    # Check which points of the grid are inside both outlines\n",
    "    points_in_reference = reference_path.contains_points(grid_points)\n",
    "    points_in_target = target_path.contains_points(grid_points)\n",
    "    \n",
    "    # Calculate the overlap area as the number of points (pixels) inside both outlines\n",
    "    overlap_area = np.sum(points_in_reference & points_in_target)\n",
    "    \n",
    "    return overlap_area\n",
    "\n",
    "def rotate_points(points, angle):\n",
    "    \"\"\"\n",
    "    Rotate a set of points by a given angle.\n",
    "    \n",
    "    Parameters:\n",
    "    points : np.array of shape (n, 2)\n",
    "    angle : float, angle to rotate by in radians\n",
    "    \n",
    "    Returns:\n",
    "    rotated_points : np.array of shape (n, 2)\n",
    "    \"\"\"\n",
    "    # Ensure points are a 2D array of shape (N, 2)\n",
    "    points = np.asarray(points).reshape(-1, 2)\n",
    "    \n",
    "    # Rotation matrix\n",
    "    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], \n",
    "                                [np.sin(angle), np.cos(angle)]])\n",
    "    \n",
    "    # Rotate points\n",
    "    rotated_points = points.dot(rotation_matrix)\n",
    "    \n",
    "    return rotated_points\n",
    "\n",
    "\n",
    "def grid_search_rotation(reference_outline, target_outline, angle_step=np.pi/36):\n",
    "    \"\"\"\n",
    "    Perform a grid search over possible rotation angles to maximize overlap.\n",
    "    \n",
    "    Parameters:\n",
    "    reference_outline : np.array of shape (n, 2), outline of the reference bone\n",
    "    target_outline : np.array of shape (n, 2), outline of the target bone\n",
    "    angle_step : float, step size for angle search (in radians)\n",
    "    \n",
    "    Returns:\n",
    "    best_rotated_points : np.array of shape (n, 2), the rotated target bone outline with maximum overlap\n",
    "    best_angle : float, the optimal rotation angle in radians\n",
    "    \"\"\"\n",
    "    best_angle = None\n",
    "    max_overlap = -np.inf\n",
    "    best_rotated_points = None\n",
    "    \n",
    "    # Iterate over angles between -90 and 90 degrees (in radians)\n",
    "    for angle in np.arange(-np.pi/4, np.pi/4, angle_step):\n",
    "        rotated_outline = rotate_points(target_outline, angle)\n",
    "        overlap = calculate_overlap_area(reference_outline, rotated_outline)\n",
    "        \n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_angle = angle\n",
    "            best_rotated_points = rotated_outline\n",
    "    \n",
    "    # Recenter the final rotated outline\n",
    "    final_centroid = calculate_centroid(best_rotated_points)\n",
    "    best_rotated_points = translate_to_origin(best_rotated_points, final_centroid)\n",
    "    \n",
    "    return best_rotated_points, best_angle, final_centroid\n",
    "\n",
    "# Modify the process_and_align_bones function to accept the max_width and max_height\n",
    "def process_and_align_bones_with_overlap(bone_dict, reference_bone, window_size=500, source_col = \"DAPI\"):\n",
    "    \"\"\"\n",
    "    Process and align all bones from the given dictionary to maximize overlap with a reference bone.\n",
    "    \n",
    "    Parameters:\n",
    "    bone_dict : dict of bones, where each value is a DataFrame with \"Position.X\" and \"Position.Y\".\n",
    "    reference_bone_name : string, the name of the bone to use as the reference for alignment.\n",
    "    \n",
    "    Returns:\n",
    "    aligned_bones : dict of aligned bone outlines\n",
    "    \"\"\"\n",
    "    aligned_bones = {}\n",
    "    aligned_angles = {}\n",
    "    aligned_centroids = {}\n",
    "    # Check the type of the reference_bone_name\n",
    "    # If it is a dataframe, we can use the reference_bone_name directly\n",
    "    if isinstance(reference_bone, pd.DataFrame):\n",
    "        reference_df = reference_bone\n",
    "        reference_points = reference_df[reference_df[\"source\"] == source_col][[\"Position.X\", \"Position.Y\"]].values\n",
    "        reference_outline = find_outline(reference_points, window_size=window_size)\n",
    "        reference_centroid = calculate_centroid(reference_outline)\n",
    "        reference_outline = translate_to_origin(reference_outline, reference_centroid)\n",
    "        \n",
    "        for bone_name, df in bone_dict.items():\n",
    "\n",
    "            # Filter points where \"source\" == \"Bone\"\n",
    "            bone_points = df[df[\"source\"] == source_col][[\"Position.X\", \"Position.Y\"]].values\n",
    "        \n",
    "            # Find the outline of the target bone\n",
    "            target_outline = find_outline(bone_points, window_size=window_size)\n",
    "            # Optimize rotation to maximize overlap with the reference bone\n",
    "            aligned_outline, best_angle, final_centroid = grid_search_rotation(reference_outline, target_outline)\n",
    "            aligned_bones[bone_name] = aligned_outline\n",
    "            aligned_angles[bone_name] = best_angle\n",
    "            aligned_centroids[bone_name] = final_centroid\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reference_bone_name. Must be a DataFrame.\")\n",
    "\n",
    "\n",
    "    return aligned_bones, aligned_angles, aligned_centroids\n",
    "\n",
    "# Perform the alignment for each age group\n",
    "def align_bones_with_centroids_angles(positions_df, aligned_centroids, aligned_angles, exclude_col=\"DAPI\"):\n",
    "    aligned_bones = {}\n",
    "    \n",
    "    for bone_name, df in positions_df.items():\n",
    "        # Filter out rows based on exclude_col first\n",
    "        df_filtered = df[df[\"source\"] != exclude_col].copy()  # Copy only the filtered rows\n",
    "\n",
    "        # Get the centroid and angle for alignment\n",
    "        centroid = aligned_centroids[bone_name]\n",
    "        angle = aligned_angles[bone_name]\n",
    "\n",
    "        # Rotate points\n",
    "        df_filtered[[\"Position.X\", \"Position.Y\"]] = rotate_points(df_filtered[[\"Position.X\", \"Position.Y\"]].values, angle)\n",
    "\n",
    "        # Translate points\n",
    "        df_filtered[\"Position.X\"] -= centroid[0]\n",
    "        df_filtered[\"Position.Y\"] -= centroid[1]\n",
    "\n",
    "        # Save the aligned DataFrame\n",
    "        aligned_bones[bone_name] = df_filtered\n",
    "\n",
    "    return aligned_bones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f13398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the DataFrame to the dictionary to fit the function\n",
    "positions_dfs_dict = {}\n",
    "\n",
    "# Iterate over the unique ages in the DataFrame\n",
    "for age in positions[\"age\"].unique():\n",
    "    # Filter the DataFrame by the current age\n",
    "    age_group = positions[positions[\"age\"] == age]\n",
    "    \n",
    "    # Initialize a dictionary for this age group\n",
    "    positions_dfs_dict[age] = {}\n",
    "    \n",
    "    # Iterate over the unique bones in this age group\n",
    "    for bone in age_group[\"bone\"].unique():\n",
    "        # Filter the DataFrame by the current bone\n",
    "        bone_group = age_group[age_group[\"bone\"] == bone]\n",
    "        \n",
    "        # Assign the filtered DataFrame to the dictionary\n",
    "        # Drop the age and bone\n",
    "        bone_group = bone_group.drop(columns=[\"age\", \"bone\"])\n",
    "        positions_dfs_dict[age][bone] = bone_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ref bone based on the visual inspection\n",
    "ref_bone = positions_dfs_dict[\"3mo\"][\"210722KK_st1\"].copy()\n",
    "\n",
    "# Recenter and find the outline\n",
    "ref_bone_points = ref_bone[ref_bone[\"source\"] == \"DAPI\"][[\"Position.X\", \"Position.Y\"]].values\n",
    "\n",
    "ref_bone_outline = find_outline(ref_bone_points, window_size=500)\n",
    "ref_bone_centroid = calculate_centroid(ref_bone_outline)\n",
    "\n",
    "# Transformed outline\n",
    "ref_bone_outline = translate_to_origin(ref_bone_outline, ref_bone_centroid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the parameters for the alignment\n",
    "positions_3mo_outline, positions_3mo_angles, positions_3mo_centroids = process_and_align_bones_with_overlap(positions_dfs_dict[\"3mo\"], reference_bone = ref_bone, window_size=500, source_col=\"DAPI\")\n",
    "positions_12mo_outline, positions_12mo_angles, positions_12mo_centroids = process_and_align_bones_with_overlap(positions_dfs_dict[\"12mo\"], reference_bone = ref_bone, window_size=500, source_col=\"DAPI\")\n",
    "positions_20mo_outline, positions_20mo_angles, positions_20mo_centroids = process_and_align_bones_with_overlap(positions_dfs_dict[\"20mo\"], reference_bone = ref_bone, window_size=500, source_col=\"DAPI\")\n",
    "positions_5fu30d_outline, positions_5fu30d_angles, positions_5fu30d_centroids = process_and_align_bones_with_overlap(positions_dfs_dict[\"5fu30d\"], reference_bone = ref_bone, window_size=500, source_col=\"DAPI\")\n",
    "positions_5fu60d_outline, positions_5fu60d_angles, positions_5fu60d_centroids = process_and_align_bones_with_overlap(positions_dfs_dict[\"5fu60d\"], reference_bone = ref_bone, window_size=500, source_col=\"DAPI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the alignment for each age group\n",
    "aligned_3mo_bones = align_bones_with_centroids_angles(positions_dfs_dict[\"3mo\"], positions_3mo_centroids, positions_3mo_angles)\n",
    "aligned_12mo_bones = align_bones_with_centroids_angles(positions_dfs_dict[\"12mo\"], positions_12mo_centroids, positions_12mo_angles)\n",
    "aligned_20mo_bones = align_bones_with_centroids_angles(positions_dfs_dict[\"20mo\"], positions_20mo_centroids, positions_20mo_angles)\n",
    "aligned_5fu30d_bones = align_bones_with_centroids_angles(positions_dfs_dict[\"5fu30d\"], positions_5fu30d_centroids, positions_5fu30d_angles)\n",
    "aligned_5fu60d_bones = align_bones_with_centroids_angles(positions_dfs_dict[\"5fu60d\"], positions_5fu60d_centroids, positions_5fu60d_angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del positions_dfs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6a491",
   "metadata": {},
   "source": [
    "### 3.2 Outline smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6425c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_outline(outline, sigma=2):\n",
    "    if not np.array_equal(outline[0], outline[-1]):\n",
    "        outline = np.vstack([outline, outline[0]])\n",
    "    smoothed_x = gaussian_filter1d(outline[:, 0], sigma=sigma)\n",
    "    smoothed_y = gaussian_filter1d(outline[:, 1], sigma=sigma)\n",
    "    smoothed_outline = np.vstack((smoothed_x, smoothed_y)).T\n",
    "    if not np.array_equal(smoothed_outline[0], smoothed_outline[-1]):\n",
    "        smoothed_outline = np.vstack([smoothed_outline, smoothed_outline[0]])\n",
    "    return smoothed_outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed the alinged_bone_outlines\n",
    "positions_3mo_bone_outlines_smoothed = {}\n",
    "positions_12mo_bone_outlines_smoothed = {}\n",
    "positions_20mo_bone_outlines_smoothed = {}\n",
    "positions_5fu30d_bone_outlines_smoothed = {}\n",
    "positions_5fu60d_bone_outlines_smoothed = {}\n",
    "\n",
    "sigma = 50\n",
    "for bone_name, outline in positions_3mo_outline.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    positions_3mo_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in positions_12mo_outline.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    positions_12mo_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in positions_20mo_outline.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    positions_20mo_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in positions_5fu30d_outline.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    positions_5fu30d_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in positions_5fu60d_outline.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    positions_5fu60d_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "\n",
    "ref_outline = smooth_outline(ref_bone_outline, sigma=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f956af7",
   "metadata": {},
   "source": [
    "### 3.3 Bone transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mask of the bone outline\n",
    "def points_in_polygon(x_points, y_points, outline):\n",
    "    path = Path(outline)\n",
    "    points = np.vstack((x_points, y_points)).T\n",
    "    return path.contains_points(points)\n",
    "\n",
    "\n",
    "# Define the function to exclude the data points outside the bone outline\n",
    "def exclude_outside_bone_outline(df, bone_outline, exclude_source =\"DAPI\"):\n",
    "    \"\"\"\n",
    "    Exclude the data points outside the bone outline using GeoPandas with bounding box filtering for more efficiency.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame containing \"Position.X\", \"Position.Y\", \"weights\", and \"source\".\n",
    "    bone_outline : The outline of the bone to limit the KDE calculation within the bone.\n",
    "    exclude_source : The name of the source that used for the bone outline creation.\n",
    "    \n",
    "    Returns:\n",
    "    df_inside : DataFrame containing the data points inside the bone outline.\n",
    "    \"\"\"\n",
    "    # print(df.shape)\n",
    "    df = pd.DataFrame(df[df[\"source\"] != exclude_source])\n",
    "    # print(df.shape)\n",
    "    # Create a GeoDataFrame from the original DataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\"Position.X\"], df[\"Position.Y\"]))\n",
    "\n",
    "    # Convert the bone outline to a Shapely polygon\n",
    "    bone_polygon = Polygon(bone_outline)\n",
    "\n",
    "    # Create a bounding box polygon from the bounds of the bone_polygon\n",
    "    minx, miny, maxx, maxy = bone_polygon.bounds\n",
    "    bounding_box = box(minx, miny, maxx, maxy)\n",
    "\n",
    "    # First, filter by the bounding box of the polygon (faster operation)\n",
    "    gdf_in_bbox = gdf[gdf.geometry.within(bounding_box)]\n",
    "    \n",
    "    # Then, perform the more precise filtering with the actual polygon\n",
    "    gdf_inside = gdf_in_bbox[gdf_in_bbox.within(bone_polygon)]\n",
    "    \n",
    "    # Drop the \"geometry\" column if you don\"t need it in the result\n",
    "    df_inside = gdf_inside.drop(columns=\"geometry\")\n",
    "\n",
    "    return df_inside\n",
    "\n",
    "def get_y_range_at_x(shape_points, x):\n",
    "    \"\"\"\n",
    "    Find the range of y-values where the vertical line at x intersects the shape.\n",
    "    \"\"\"\n",
    "    # Find all edges of the shape where x is between the x-coordinates of the endpoints\n",
    "    y_vals = []\n",
    "    for i in range(len(shape_points)):\n",
    "        p1 = shape_points[i]\n",
    "        p2 = shape_points[(i + 1) % len(shape_points)]  # wrap around the shape points\n",
    "        \n",
    "        # Check if the x value is between p1 and p2\"s x-coordinates\n",
    "        if (p1[0] <= x <= p2[0]) or (p2[0] <= x <= p1[0]):\n",
    "            # Linearly interpolate to find the corresponding y value at x\n",
    "            if p1[0] != p2[0]:  # Avoid division by zero\n",
    "                y = p1[1] + (p2[1] - p1[1]) * (x - p1[0]) / (p2[0] - p1[0])\n",
    "                y_vals.append(y)\n",
    "    \n",
    "    if y_vals:\n",
    "        return min(y_vals), max(y_vals)\n",
    "    else:\n",
    "        return None, None  # No intersection with the shape at this x\n",
    "\n",
    "def create_structured_grid(shape_points, x_num, y_num):\n",
    "    \"\"\"\n",
    "    Create a structured grid by dividing the bounding box of the shape into x_num vertical sections.\n",
    "    Then place y_num points along each vertical grid line where it intersects the shape.\n",
    "    \"\"\"\n",
    "    shape_points = np.array(shape_points)\n",
    "    \n",
    "    # Step 1: Compute the bounding box\n",
    "    min_x, max_x = np.min(shape_points[:, 0]), np.max(shape_points[:, 0])\n",
    "    \n",
    "    # Step 2: Divide the x-range into equal sections\n",
    "    x_vals = np.linspace(min_x, max_x, x_num + 1)  # x_num divisions create x_num + 1 grid lines\n",
    "    # Shift the x_vals to the left by half the grid spacing to center the grid\n",
    "    x_vals = x_vals[1:]  # Remove the first point (left edge of bounding box)\n",
    "    x_vals = x_vals - (x_vals[1] - x_vals[0]) / 2  # Shift left by half the grid spacing\n",
    "    \n",
    "    grid_points = []\n",
    "\n",
    "    # Step 3: For each x grid line, find the y range and then place points\n",
    "    for x in x_vals:  # Skip the first and last lines (already have the bounding box)\n",
    "        y_min, y_max = get_y_range_at_x(shape_points, x)\n",
    "        \n",
    "        if y_min is not None and y_max is not None:\n",
    "            # Get y points by placing y_num points between y_min and y_max\n",
    "            y_vals = np.linspace(y_min, y_max, y_num+1)\n",
    "            \n",
    "            # Shift the y_vals down by half the grid spacing to center the grid\n",
    "            y_vals = y_vals[1:]  # Remove the first point (bottom edge of bounding box)\n",
    "            y_vals = y_vals - (y_vals[1] - y_vals[0]) / 2  # Shift down by half the grid spacing\n",
    "            # Add grid points (x, y) for this vertical line\n",
    "            for y in y_vals: # Skip the first and last points (already have the y_min and y_max)\n",
    "                grid_points.append([x, y])\n",
    "\n",
    "    return np.array(grid_points)\n",
    "\n",
    "# (Not used)\n",
    "def is_point_inside_shape(point, shape_points):\n",
    "    \"\"\"\n",
    "    Determines if a point is inside an irregular shape using ray-casting.\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    n = len(shape_points)\n",
    "    inside = False\n",
    "    p1x, p1y = shape_points[0]\n",
    "    for i in range(n + 1):\n",
    "        p2x, p2y = shape_points[i % n]\n",
    "        if y > min(p1y, p2y):\n",
    "            if y <= max(p1y, p2y):\n",
    "                if x <= max(p1x, p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                    if p1x == p2x or x <= xinters:\n",
    "                        inside = not inside\n",
    "        p1x, p1y = p2x, p2y\n",
    "    return inside\n",
    "\n",
    "\n",
    "def thin_plate_spline_transform(src_points, dst_points):\n",
    "    \"\"\"\n",
    "    Perform Thin Plate Spline (TPS) transformation from src_points to dst_points.\n",
    "    \"\"\"\n",
    "    # Create Radial Basis Function (RBF) interpolators for x and y coordinates\n",
    "    rbf_x = Rbf(src_points[:, 0], src_points[:, 1], dst_points[:, 0], function=\"thin_plate\")\n",
    "    rbf_y = Rbf(src_points[:, 0], src_points[:, 1], dst_points[:, 1], function=\"thin_plate\")\n",
    "    \n",
    "    def transform(points):\n",
    "        new_x = rbf_x(points[:, 0], points[:, 1])\n",
    "        new_y = rbf_y(points[:, 0], points[:, 1])\n",
    "        return np.vstack([new_x, new_y]).T\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def transform_data(data_points, grid_shape_1, grid_shape_2):\n",
    "    \"\"\"\n",
    "    Apply the TPS transformation to the data points based on the grid transformation.\n",
    "    \"\"\"\n",
    "    # Perform Thin Plate Spline (TPS) transformation\n",
    "    tps_transform = thin_plate_spline_transform(grid_shape_2, grid_shape_1)\n",
    "    \n",
    "    # Apply the transformation to the data points\n",
    "    transformed_data_points = tps_transform(data_points)\n",
    "    \n",
    "    return transformed_data_points\n",
    "\n",
    "\n",
    "def filter_bone_positions(df, source_value=None, columns_to_keep=None):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame for a specific source if provided and returns the Position.X, Position.Y columns as a NumPy array.\n",
    "    If source_value is None, return all positions.\n",
    "    \"\"\"\n",
    "    if source_value is not None:\n",
    "        filtered_df = df[df[\"source\"] != source_value]\n",
    "    else:\n",
    "        # Exclude the data with source value \"DAPI\"\n",
    "        filtered_df = df[df[\"source\"] != \"DAPI\"]\n",
    "    positions = filtered_df[[\"Position.X\", \"Position.Y\"]].to_numpy()\n",
    "    if columns_to_keep is not None:\n",
    "        return positions, filtered_df[\"source\"].to_numpy(), filtered_df[columns_to_keep].to_numpy()\n",
    "    else:\n",
    "        return positions, filtered_df[\"source\"].to_numpy()  # Return positions and the source column\n",
    "\n",
    "\n",
    "\n",
    "def transform_bone_positions(outline_dict, position_dict, common_outline, x_num=40, y_num=20, source_value=None, columns_to_keep=None):\n",
    "    \"\"\"\n",
    "    Transforms the bone positions from multiple datasets using Thin Plate Spline (TPS) based on the provided outlines and positions.\n",
    "    Parameters:\n",
    "        outline_dict: Dictionary containing outlines.\n",
    "        position_dict: Dictionary containing bone positions (DataFrames).\n",
    "        common_outline: The common outline (to which the other outlines will be aligned).\n",
    "        x_num: Number of vertical sections for structured grid.\n",
    "        y_num: Number of horizontal points along each vertical section.\n",
    "        source_value: If provided, exclude the data with this source value.\n",
    "        columns_to_keep: If provided, keep the specified columns in the transformed DataFrame.\n",
    "    Returns:\n",
    "        Dictionary containing the transformed bone positions with the \"source\" column retained.\n",
    "    \"\"\"\n",
    "    transformed_dict = {}\n",
    "\n",
    "    # Create the structured grid for the common outline\n",
    "    grid_common_outline = create_structured_grid(common_outline, x_num=x_num, y_num=y_num)\n",
    "\n",
    "    # Loop through each dataset in the position_dict\n",
    "    for dataset_name, position_df in position_dict.items():\n",
    "        # Get the corresponding outline\n",
    "        outline_2 = outline_dict[dataset_name]\n",
    "        \n",
    "        # Create the structured grid for the specific dataset\"s outline\n",
    "        grid_outline_2 = create_structured_grid(outline_2, x_num=x_num, y_num=y_num)\n",
    "\n",
    "        # Filter the positions based on the source (if provided)\n",
    "        # By default, it will exclude the data with source value \"GFP\"\n",
    "        if columns_to_keep is None:\n",
    "            bone_positions_2, source_column = filter_bone_positions(position_df, source_value=source_value) \n",
    "        else:\n",
    "            bone_positions_2, source_column, kept_columns = filter_bone_positions(position_df, source_value=source_value, columns_to_keep=columns_to_keep)\n",
    "\n",
    "        # Transform the filtered bone positions from the dataset outline to the common outline\n",
    "        transformed_bone_positions = transform_data(bone_positions_2, grid_common_outline, grid_outline_2)\n",
    "\n",
    "        # Convert the transformed positions to a DataFrame and include the source column\n",
    "        transformed_df = pd.DataFrame(transformed_bone_positions, columns=[\"Position.X\", \"Position.Y\"])\n",
    "        transformed_df[\"source\"] = source_column  # Add the source column back\n",
    "        transformed_df[\"dataset\"] = dataset_name  # Add the dataset name for reference\n",
    "        if columns_to_keep is not None:\n",
    "            transformed_df[columns_to_keep] = kept_columns \n",
    "        # Store the transformed DataFrame in the result dictionary\n",
    "        transformed_dict[dataset_name] = transformed_df\n",
    "\n",
    "    return transformed_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The num is based on the size of the bone outline\n",
    "x_num = 200\n",
    "y_num = 40\n",
    "\n",
    "# We are using the aligned_bone_date dict instead of using the dataframes, because for bones of the same day, they have different bone outline\n",
    "transformed_3mo_bones = transform_bone_positions(positions_3mo_bone_outlines_smoothed, aligned_3mo_bones, ref_outline, x_num=x_num, y_num=y_num, columns_to_keep=\"clusters\")\n",
    "transformed_12mo_bones = transform_bone_positions(positions_12mo_bone_outlines_smoothed, aligned_12mo_bones, ref_outline, x_num=x_num, y_num=y_num, columns_to_keep=\"clusters\")\n",
    "transformed_20mo_bones = transform_bone_positions(positions_20mo_bone_outlines_smoothed, aligned_20mo_bones, ref_outline, x_num=x_num, y_num=y_num, columns_to_keep=\"clusters\")\n",
    "transformed_5fu30d_bones = transform_bone_positions(positions_5fu30d_bone_outlines_smoothed, aligned_5fu30d_bones, ref_outline, x_num=x_num, y_num=y_num, columns_to_keep=\"clusters\")\n",
    "transformed_5fu60d_bones = transform_bone_positions(positions_5fu60d_bone_outlines_smoothed, aligned_5fu60d_bones, ref_outline, x_num=x_num, y_num=y_num, columns_to_keep=\"clusters\")\n",
    "\n",
    "transformed_3mo_bones_df = pd.concat(transformed_3mo_bones)\n",
    "transformed_12mo_bones_df = pd.concat(transformed_12mo_bones)\n",
    "transformed_20mo_bones_df = pd.concat(transformed_20mo_bones)\n",
    "transformed_5fu30d_bones_df = pd.concat(transformed_5fu30d_bones)\n",
    "transformed_5fu60d_bones_df = pd.concat(transformed_5fu60d_bones)\n",
    "\n",
    "# Exclude the data points outside the bone outline\n",
    "transformed_3mo_bones_df = exclude_outside_bone_outline(transformed_3mo_bones_df, ref_outline)\n",
    "transformed_12mo_bones_df = exclude_outside_bone_outline(transformed_12mo_bones_df, ref_outline)\n",
    "transformed_20mo_bones_df = exclude_outside_bone_outline(transformed_20mo_bones_df, ref_outline)\n",
    "transformed_5fu30d_bones_df = exclude_outside_bone_outline(transformed_5fu30d_bones_df, ref_outline)\n",
    "transformed_5fu60d_bones_df = exclude_outside_bone_outline(transformed_5fu60d_bones_df, ref_outline)\n",
    "\n",
    "del transformed_3mo_bones, transformed_12mo_bones, transformed_20mo_bones, transformed_5fu30d_bones, transformed_5fu60d_bones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28697f8",
   "metadata": {},
   "source": [
    "## 4. Components Calculation and Estimation\n",
    "- calculate the PDF of cKits and HSC\n",
    "- calculate the 2D histogram of HSC\n",
    "- calculate the cluster composition of HSC (based on the raw data or transformed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f3661",
   "metadata": {},
   "source": [
    "### 4.1 PDF estimation (function defintion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bone outline to generate the KDE for each source\n",
    "# Define the KDE function for each source, with weights based on z-aggregated points\n",
    "def kde_for_clusters(df, bw_method=\"scott\", bone_outline = None, binsize = 10):\n",
    "\n",
    "    kde_results = {}\n",
    "    sources = df[\"source\"].unique()\n",
    "    sources = sources[sources != \"GFP\"] # Exclude the bone for the KDE calculation\n",
    "\n",
    "    for source in sources:\n",
    "        # Filter data for the current source\n",
    "        source_data = df[df[\"source\"] == source]\n",
    "        if source == \"cKits\":\n",
    "            for cluster in source_data[\"clusters\"].unique():\n",
    "                cluster_data = source_data[source_data[\"clusters\"] == cluster]\n",
    "\n",
    "                # Group by Position.X and Position.Y, summing weights (or using counts as weights if no weights are given)\n",
    "                if \"weights\" in cluster_data.columns:\n",
    "                    cluster_data_agg = cluster_data.groupby([\"Position.X\", \"Position.Y\"])[\"weights\"].sum().reset_index()\n",
    "                else:\n",
    "                    # If no weights are provided, use the count of occurrences as weights\n",
    "                    cluster_data_agg = cluster_data.groupby([\"Position.X\", \"Position.Y\"]).size().reset_index(name=\"weights\")\n",
    "\n",
    "                # Get the x and y values and aggregated weights\n",
    "                x_vals = cluster_data_agg[\"Position.X\"]\n",
    "                y_vals = cluster_data_agg[\"Position.Y\"]\n",
    "                weights = cluster_data_agg[\"weights\"]\n",
    "                if bone_outline is None:\n",
    "                    x_min, x_max = x_vals.min(), x_vals.max()\n",
    "                    y_min, y_max = y_vals.min(), y_vals.max()\n",
    "                else:\n",
    "                    x_min, y_min = bone_outline.min(axis=0)\n",
    "                    x_max, y_max = bone_outline.max(axis=0)\n",
    "\n",
    "                xi, yi = np.linspace(x_min, x_max, int((x_max - x_min)/binsize)+1), np.linspace(y_min, y_max, int((y_max - y_min)/binsize)+1)\n",
    "                xi, yi = np.meshgrid(xi, yi)\n",
    "                grid_points = np.vstack([xi.flatten(), yi.flatten()])\n",
    "                common_grid = (xi, yi, grid_points)\n",
    "\n",
    "                # Stack the x and y data for KDE input\n",
    "                xy = np.vstack([x_vals, y_vals])\n",
    "\n",
    "                # Perform the KDE with aggregated weights\n",
    "                kde = gaussian_kde(xy, weights=weights, bw_method=bw_method)\n",
    "                kde_values = kde(grid_points).reshape(xi.shape)\n",
    "\n",
    "                # Store the results for each source\n",
    "                kde_results[(source, cluster)] = kde_values\n",
    "        else:\n",
    "            # Group by Position.X and Position.Y, summing weights (or using counts as weights if no weights are given)\n",
    "            if \"weights\" in source_data.columns:\n",
    "                source_data_agg = source_data.groupby([\"Position.X\", \"Position.Y\"])[\"weights\"].sum().reset_index()\n",
    "            else:\n",
    "                # If no weights are provided, use the count of occurrences as weights\n",
    "                source_data_agg = source_data.groupby([\"Position.X\", \"Position.Y\"]).size().reset_index(name=\"weights\")\n",
    "\n",
    "            # Get the x and y values and aggregated weights\n",
    "            x_vals = source_data_agg[\"Position.X\"]\n",
    "            y_vals = source_data_agg[\"Position.Y\"]\n",
    "            weights = source_data_agg[\"weights\"]\n",
    "            if bone_outline is None:\n",
    "                x_min, x_max = x_vals.min(), x_vals.max()\n",
    "                y_min, y_max = y_vals.min(), y_vals.max()\n",
    "            else:\n",
    "                x_min, y_min = bone_outline.min(axis=0)\n",
    "                x_max, y_max = bone_outline.max(axis=0)\n",
    "\n",
    "            xi, yi = np.linspace(x_min, x_max, int((x_max - x_min)/binsize)+1), np.linspace(y_min, y_max, int((y_max - y_min)/binsize)+1)\n",
    "            xi, yi = np.meshgrid(xi, yi)\n",
    "            grid_points = np.vstack([xi.flatten(), yi.flatten()])\n",
    "            common_grid = (xi, yi, grid_points)\n",
    "\n",
    "            # Stack the x and y data for KDE input\n",
    "            xy = np.vstack([x_vals, y_vals])\n",
    "\n",
    "            # Perform the KDE with aggregated weights\n",
    "            kde = gaussian_kde(xy, weights=weights, bw_method=bw_method)\n",
    "            kde_values = kde(grid_points).reshape(xi.shape)\n",
    "\n",
    "            # Store the results for each source\n",
    "            kde_results[source] = kde_values\n",
    "\n",
    "    return kde_results, common_grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d8e42",
   "metadata": {},
   "source": [
    "### 4.2 Cluster composition calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4044d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant columns and ages\n",
    "columns_to_use = [\"Position.X\", \"Position.Y\", \"Position.Z\", \"age\", \"clusters\", \"bone\"]\n",
    "ages = [\"3mo\", \"12mo\", \"20mo\", \"5fu30d\", \"5fu60d\"]\n",
    "\n",
    "# Collect only HSC data\n",
    "hsc_positions = []\n",
    "\n",
    "for position_dir in position_dirs:\n",
    "    if \"hsc\" in position_dir.lower():\n",
    "        df = pd.read_csv(position_dir, usecols=columns_to_use)\n",
    "        df[\"source\"] = \"HSCs\"\n",
    "        hsc_positions.append(df)\n",
    "\n",
    "# Combine all HSC data\n",
    "hsc_combined = pd.concat(hsc_positions, ignore_index=True)\n",
    "# Rename the column \"bone\" as \"dataset\"\n",
    "hsc_combined.rename(columns={\"bone\": \"dataset\"}, inplace=True)\n",
    "\n",
    "# Split into individual variables\n",
    "hsc_3mo_raw = hsc_combined[hsc_combined[\"age\"] == \"3mo\"]\n",
    "hsc_12mo_raw = hsc_combined[hsc_combined[\"age\"] == \"12mo\"]\n",
    "hsc_20mo_raw = hsc_combined[hsc_combined[\"age\"] == \"20mo\"]\n",
    "hsc_5fu30d_raw = hsc_combined[hsc_combined[\"age\"] == \"5fu30d\"]\n",
    "hsc_5fu60d_raw = hsc_combined[hsc_combined[\"age\"] == \"5fu60d\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calulate the the cluster proportions of each dataset\n",
    "def compute_proportions_hsc(df, clusters = np.arange(10)):\n",
    "    df = df[df[\"source\"] == \"HSCs\"].copy()\n",
    "    counts = df.groupby([\"clusters\"]).size().reset_index(name=\"count\")\n",
    "    # Fill the missing clusters with 0\n",
    "    counts = counts.set_index(\"clusters\").reindex(clusters, fill_value=0).reset_index()\n",
    "    # Add the dataset column\n",
    "    # counts[\"dataset\"] = df[\"dataset\"].unique()[0]\n",
    "    # Add the total count\n",
    "    counts[\"total\"] = df.shape[0]\n",
    "    # Add the proportion\n",
    "    counts[\"proportion\"] = counts[\"count\"] / counts[\"total\"]\n",
    "    return counts[[\"clusters\", \"count\", \"total\", \"proportion\"]]\n",
    "\n",
    "# Compute proportions for each dataset\n",
    "proportions_3mo = compute_proportions_hsc(hsc_3mo_raw)\n",
    "proportions_12mo = compute_proportions_hsc(hsc_12mo_raw)\n",
    "proportions_20mo = compute_proportions_hsc(hsc_20mo_raw)\n",
    "proportions_5fu30d = compute_proportions_hsc(hsc_5fu30d_raw)\n",
    "proportions_5fu60d = compute_proportions_hsc(hsc_5fu60d_raw)\n",
    "\n",
    "\n",
    "# Apply linear interpolation for the 3mo, 12mo, and 20mo datasets\n",
    "real_times = np.array([3, 12, 20])\n",
    "fine_times = np.linspace(3, 20, 100)  # Fine-grained time points\n",
    "proportions_3mo[\"proportion\"] = proportions_3mo[\"proportion\"].astype(float)\n",
    "proportions_12mo[\"proportion\"] = proportions_12mo[\"proportion\"].astype(float)\n",
    "proportions_20mo[\"proportion\"] = proportions_20mo[\"proportion\"].astype(float)\n",
    "proportions_5fu30d[\"proportion\"] = proportions_5fu30d[\"proportion\"].astype(float)\n",
    "proportions_5fu60d[\"proportion\"] = proportions_5fu60d[\"proportion\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8a458",
   "metadata": {},
   "source": [
    "## 5 Bone Age Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f38899",
   "metadata": {},
   "source": [
    "### 5.1 Required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b99d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate KL Divergence\n",
    "def calculate_kl_divergence(p, q):\n",
    "    # Ensure no zero values to avoid division by zero or log of zero\n",
    "    p = np.clip(p, 1e-10, None)\n",
    "    q = np.clip(q, 1e-10, None)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "# Function to calculate Jensen-Shannon Divergence (JSD)\n",
    "def calculate_jsd(p, q):\n",
    "    # Compute the average distribution M\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Compute JSD as the average KL divergence between P-M and Q-M\n",
    "    jsd = 0.5 * calculate_kl_divergence(p, m) + 0.5 * calculate_kl_divergence(q, m)\n",
    "    return jsd\n",
    "\n",
    "\n",
    "# Function to compute Wasserstein distance between two matrices (not use for now)\n",
    "def wasserstein_matrix_distance(A, B):\n",
    "    \"\"\"\n",
    "    Computes Wasserstein distance between two affinity matrices.\n",
    "    Applies row-wise Wasserstein distance and averages over all rows.\n",
    "    \"\"\"\n",
    "    num_rows = A.shape[0]\n",
    "    distances = [wasserstein_distance(A.iloc[i, :], B.iloc[i, :]) for i in range(num_rows)]\n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "def wasserstein_distance_per_row(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes Wasserstein distance between two 1D arrays (rows of an affinity matrix).\n",
    "    \n",
    "    Inputs:\n",
    "        A, B: NumPy arrays of shape [10,] representing a single row of affinity values.\n",
    "    \n",
    "    Output:\n",
    "        A single float value representing the Wasserstein distance between the two rows.\n",
    "    \"\"\"\n",
    "    # Ensure both arrays have the same shape\n",
    "    assert A.shape == B.shape, \"Error: The arrays A and B must have the same shape!\"\n",
    "\n",
    "    # Compute Wasserstein Distance\n",
    "    # print(A.shape[0])\n",
    "    bins = np.arange(A.shape[0])\n",
    "    return wasserstein_distance(bins,bins,A, B)\n",
    "\n",
    "# Define a function to calculate the heatmap SSIM\n",
    "def calculate_heatmap_ssim(p, q):\n",
    "    \"\"\"\n",
    "    Calculate Structural Similarity Index (SSIM) between two heatmaps.\n",
    "\n",
    "    Args:\n",
    "        p: 2D array (heatmap) representing the first image.\n",
    "        q: 2D array (heatmap) representing the second image.\n",
    "\n",
    "    Returns:\n",
    "        float: SSIM value between the two heatmaps.\n",
    "    \"\"\"\n",
    "    # Ensure both inputs are numpy arrays\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "\n",
    "    # Check if both heatmaps are entirely zero\n",
    "    if np.all(p == 0) and np.all(q == 0):\n",
    "        return 1.0  # If both are empty, they are identical\n",
    "\n",
    "    # Check if one heatmap is zero and the other is not\n",
    "    if np.all(p == 0) or np.all(q == 0):\n",
    "        return 0.0  # Completely different if one is empty\n",
    "\n",
    "    # Compute SSIM normally\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        return compare_ssim(p, q, data_range=p.max() - p.min(), win_size=3)\n",
    "\n",
    "# Define a function to calculate the Mean Squared Error (MSE)\n",
    "def calculate_mse(p, q):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) between two arrays.\n",
    "\n",
    "    Args:\n",
    "        p: Array of true values.\n",
    "        q: Array of predicted values.\n",
    "\n",
    "    Returns:\n",
    "        float: MSE value.\n",
    "    \"\"\"\n",
    "    return np.mean((p - q) ** 2)\n",
    "\n",
    "\n",
    "def calculate_cross_entropy(p, q):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy H(p, q) = -sum_i p(i)*log(q(i)).\n",
    "    We add a small epsilon to q to avoid log(0).\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10\n",
    "    return -np.sum(p * np.log(q + epsilon))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdced1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before the calculation of the KL Divergence, we need to normalize the kde values to have the sum of 1 (integral of 1)\n",
    "# Normalize the kde values for the HSCs, RDs and cKits\n",
    "def normalize_kde_values(kde_results):\n",
    "    \"\"\"\n",
    "    Normalize the KDE values to have the sum of 1 for each cluster.\n",
    "\n",
    "    Args:\n",
    "        kde_results (dict): Dictionary of KDE values by cluster.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of normalized KDE values by cluster.\n",
    "    \"\"\"\n",
    "    # Create a new dictionary to store the normalized KDE values\n",
    "    normalized_kde_results = {}\n",
    "    \n",
    "    for cluster, kde_values in kde_results.items():\n",
    "        # Normalize the KDE values to have the sum of 1\n",
    "        normalized_kde_values = kde_values / np.sum(kde_values)\n",
    "        normalized_kde_results[cluster] = normalized_kde_values\n",
    "    \n",
    "    return normalized_kde_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exponential function\n",
    "def exponential_func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "# Define linear function\n",
    "def linear_func(x, m, c):\n",
    "    return m * x + c\n",
    "\n",
    "# Constrained linear regression to ensure y > 0\n",
    "def constrained_linear_fit(x, y):\n",
    "    def objective(params):\n",
    "        m, c = params\n",
    "        return np.sum((linear_func(x, m, c) - y) ** 2)  # Minimize squared error\n",
    "    \n",
    "    # Initial guesses for m and c\n",
    "    initial_guess = [1, 1]\n",
    "    \n",
    "    # Constraints: c > 0\n",
    "    constraints = {\"type\": \"ineq\", \"fun\": lambda params: params[1]}  # c > 0\n",
    "    \n",
    "    # Perform optimization\n",
    "    result = minimize(objective, initial_guess, constraints=constraints, method=\"SLSQP\")\n",
    "    \n",
    "    if result.success:\n",
    "        return result.x  # Return fitted m and c\n",
    "    else:\n",
    "        raise RuntimeError(\"Optimization failed for constrained linear regression\")\n",
    "    \n",
    "def find_intersection(func, y_value, fine_ages, method=\"closest\"):\n",
    "    \"\"\"\n",
    "    Find the x-values where a function intersects a given y-value.\n",
    "    If multiple intersection points exist, return their average.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): The function to intersect (e.g., regression or interpolation).\n",
    "        y_value (float): The y-value of the horizontal line.\n",
    "        fine_ages (np.ndarray): The range of x-values to search.\n",
    "        method (str): Method to find the value (\"root\" or \"closest\").\n",
    "\n",
    "    Returns:\n",
    "        float: The averaged x-value of the intersections, or None if no intersection exists.\n",
    "    \"\"\"\n",
    "    if method == \"root\":\n",
    "        # Define the function to find roots (difference between curve and y_value)\n",
    "        def func_to_solve(x):\n",
    "            return func(x) - y_value\n",
    "\n",
    "        # Find all points where the function crosses the horizontal line\n",
    "        intersections = []\n",
    "        for i in range(len(fine_ages) - 1):\n",
    "            x1, x2 = fine_ages[i], fine_ages[i + 1]\n",
    "            try:\n",
    "                result = root_scalar(func_to_solve, bracket=(x1, x2), method=\"brentq\")\n",
    "                if result.converged:\n",
    "                    intersections.append(result.root)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        # Return the average of all intersection points, or None if no intersection found\n",
    "        return np.mean(intersections) if intersections else None\n",
    "\n",
    "    elif method == \"closest\":\n",
    "        # Evaluate the function at all fine_ages\n",
    "        y_values = func(fine_ages)\n",
    "        # Find the index of the closest value\n",
    "        closest_index = np.argmin(np.abs(y_values - y_value))\n",
    "        return fine_ages[closest_index]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'root' or 'closest'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3855ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_sizes(transformed_df):\n",
    "    \"\"\"\n",
    "    Calculate the cluster sizes for each source (HSCs, cKits, RDs) and cluster.\n",
    "    \n",
    "    Parameters:\n",
    "        transformed_df (pd.DataFrame): DataFrame containing `source`, `clusters`, and other data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested dictionary with source as the first-level key and cluster sizes as second-level keys.\n",
    "    \"\"\"\n",
    "    cluster_sizes = {}\n",
    "    sources = transformed_df[\"source\"].unique()\n",
    "    for source in sources:\n",
    "        source_data = transformed_df[transformed_df[\"source\"] == source]\n",
    "        cluster_counts = source_data.groupby(\"clusters\").size().to_dict()\n",
    "        cluster_sizes[source] = cluster_counts\n",
    "    return cluster_sizes\n",
    "\n",
    "def compute_weighted_cluster_sizes(ref_cluster_sizes, cluster_sizes, condition, sources, clusters):\n",
    "    \"\"\"\n",
    "    Compute weighted cluster sizes, normalizing each condition and filling missing clusters with 0.\n",
    "    \n",
    "    Parameters:\n",
    "        ref_cluster_sizes (dict): Nested dictionary with cluster sizes from the reference S2 data,\n",
    "                                with keys like \"3mo_ref\", \"12mo_ref\", \"20mo_ref\".\n",
    "        cluster_sizes (dict): Nested dictionary with cluster sizes per condition (e.g., for selected data).\n",
    "        condition (str): The condition string for the selected data (e.g., \"3mo_selected\").\n",
    "        sources (list): List of sources (e.g., [\"cKits\", \"HSCs\"]).\n",
    "        clusters (list): List of all possible clusters (e.g., [0,1,...,9]).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary mapping each source to a dictionary of weighted cluster sizes.\n",
    "    \"\"\"\n",
    "    cluster_size_input = {}\n",
    "    for source in sources:\n",
    "        # Initialize dictionaries for each age condition\n",
    "        cluster_size_3mo = {cluster: 0 for cluster in clusters}\n",
    "        cluster_size_12mo = {cluster: 0 for cluster in clusters}\n",
    "        cluster_size_20mo = {cluster: 0 for cluster in clusters}\n",
    "        cluster_size_input[source] = {cluster: 0 for cluster in clusters}\n",
    "        \n",
    "        # Helper function: normalize a dictionary of counts to proportions\n",
    "        def normalize_cluster_sizes(cluster_dict):\n",
    "            total = sum(cluster_dict.values())\n",
    "            return {cluster: (size / total if total > 0 else 0) for cluster, size in cluster_dict.items()}\n",
    "        \n",
    "        # Normalize the reference sizes for each age from S2 data (assume keys \"3mo_ref\", \"12mo_ref\", \"20mo_ref\")\n",
    "        cluster_size_3mo.update(normalize_cluster_sizes(ref_cluster_sizes.get(\"3mo_ref\", {}).get(source, {})))\n",
    "        cluster_size_12mo.update(normalize_cluster_sizes(ref_cluster_sizes.get(\"12mo_ref\", {}).get(source, {})))\n",
    "        cluster_size_20mo.update(normalize_cluster_sizes(ref_cluster_sizes.get(\"20mo_ref\", {}).get(source, {})))\n",
    "        # Normalize the selected (test) cluster sizes for the current condition (e.g., \"3mo_selected\")\n",
    "        cluster_size_input_vals = normalize_cluster_sizes(cluster_sizes.get(condition, {}).get(source, {}))\n",
    "        \n",
    "        # Compute the average (reference) cluster size across ages 3mo, 12mo, and 20mo\n",
    "        cluster_size_ref = [\n",
    "            (cluster_size_3mo[cluster] + cluster_size_12mo[cluster] + cluster_size_20mo[cluster]) / 3\n",
    "            for cluster in clusters\n",
    "        ]\n",
    "        # Combine the test and reference values by averaging them\n",
    "        for cluster in clusters:\n",
    "            cluster_size_input[source][cluster] = (cluster_size_input_vals.get(cluster, 0) + \n",
    "                                                    cluster_size_ref[clusters.index(cluster)]) / 2\n",
    "    return cluster_size_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack proportions into a matrix for interpolation\n",
    "def stack_proportions(*props_list):\n",
    "    \"\"\"Returns a (n_clusters, n_timepoints) array\"\"\"\n",
    "    return np.stack([df.sort_values(\"clusters\")[\"proportion\"].values for df in props_list], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530145bb",
   "metadata": {},
   "source": [
    "### 5.2 Training data generation and weight optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a08c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Parameters\n",
    "# ========================\n",
    "lambda_reg = 0.05  # L2 regularization parameter (adjust as needed)\n",
    "\n",
    "# ========================\n",
    "# Load the transformed bone data (for 3mo, 12mo, 20mo, 5FU30d, 5FU60d)\n",
    "# ========================\n",
    "\"\"\" if you have saved the transformed bone dataframes, you can load them here\n",
    "transformed_3mo_bones_df = pd.read_csv(f\"{results_dir}/transformed_3mo_bones_inside_df.csv\")\n",
    "transformed_12mo_bones_df = pd.read_csv(f\"{results_dir}/transformed_12mo_bones_inside_df.csv\")\n",
    "transformed_20mo_bones_df = pd.read_csv(f\"{results_dir}/transformed_20mo_bones_inside_df.csv\")\n",
    "transformed_5fu30d_bones_df = pd.read_csv(f\"{results_dir}/transformed_5fu30d_bones_inside_df.csv\")\n",
    "transformed_5fu60d_bones_df = pd.read_csv(f\"{results_dir}/transformed_5fu60d_bones_inside_df.csv\")\n",
    "\"\"\"\n",
    "\n",
    "# Get unique dataset IDs for each age group\n",
    "datasets_3mo = transformed_3mo_bones_df[\"dataset\"].unique()\n",
    "datasets_12mo = transformed_12mo_bones_df[\"dataset\"].unique()\n",
    "datasets_20mo = transformed_20mo_bones_df[\"dataset\"].unique()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Get all combinations for leave-one-out: one test bone from each age group.\n",
    "# (If there are 4 bones per group, there will be 4*4*4=64 iterations.)\n",
    "# ========================\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "loo_combinations = list(itertools.product(datasets_3mo, datasets_12mo, datasets_20mo))\n",
    "print(f\"Total LOO iterations: {len(loo_combinations)}\")\n",
    "\n",
    "# ========================\n",
    "# Define Loss Functions and Optimization Functions\n",
    "# ========================\n",
    "\n",
    "# Ground truth ages for the normal conditions\n",
    "ground_truth = {\"3mo\": 3, \"12mo\": 12, \"20mo\": 20}\n",
    "\n",
    "\n",
    "def total_loss(weights, optimal_input, ground_truth, reg_lambda):\n",
    "    \"\"\"\n",
    "    Loss function with L2 regularization.\n",
    "    weights: a numpy array (length 5)\n",
    "    optimal_input: dict mapping condition (\"3mo\", \"12mo\", \"20mo\") to a list of 5 numbers.\n",
    "    ground_truth: dict mapping condition to true age.\n",
    "    reg_lambda: regularization parameter.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for condition, target in ground_truth.items():\n",
    "        age_values = np.array(optimal_input[condition])\n",
    "        weighted_avg = np.dot(weights, age_values)\n",
    "        loss += (weighted_avg - target) ** 2\n",
    "    loss += reg_lambda * np.sum(np.square(weights))\n",
    "    return loss\n",
    "\n",
    "def weight_constraint(weights):\n",
    "    return np.sum(weights) - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce5b8c",
   "metadata": {},
   "source": [
    "### 5.3 Training data generation for the Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LOO-CV to generate training data for prediction \n",
    "# instead of using interpolation assumptions\n",
    "# ========================\n",
    "# Main LOO-CV Loop\n",
    "# ========================\n",
    "epoch_index = 1\n",
    "affinity_flag = \"CE\" # \"CE\" or \"MSE\"\n",
    "\n",
    "# Define the dataframe to save the training data collected from LOO-CV\n",
    "# 1. Define the columns\n",
    "# epoch, cKit_pdf_3mo, cKit_pdf_12mo, cKit_pdf_20mo, HSC_pdf_3mo, HSC_pdf_12mo, HSC_pdf_20mo, HSC_num, HSC_hist_3mo, HSC_hist_12mo, HSC_hist_20mo, cKit_affinity_3mo, cKit_affinity_12mo, cKit_affinity_20mo, ground_truth\n",
    "# training_data_columns = [\"epoch\", \"cKit_pdf_3mo\", \"cKit_pdf_12mo\", \"cKit_pdf_20mo\", \"HSC_pdf_3mo\", \"HSC_pdf_12mo\", \"HSC_pdf_20mo\", \"HSC_num\", \"HSC_hist_3mo\", \"HSC_hist_12mo\", \"HSC_hist_20mo\", \"cKit_affinity_3mo\", \"cKit_affinity_12mo\", \"cKit_affinity_20mo\", \"ground_truth\"]\n",
    "training_data_list = []\n",
    "\n",
    "# Read the affinity matrices for each condition\n",
    "affinity_matrices_dir = os.path.join(data_dir, \"affinity_matrices\")\n",
    "\n",
    "# Load the affinity matrices\n",
    "affinity_matrices = {}\n",
    "for f in os.listdir(affinity_matrices_dir):\n",
    "    # f should start with cluster, and end with sum.csv\n",
    "    if not f.startswith(\"affinity\") or not f.endswith(\"sum.csv\"):\n",
    "        continue\n",
    "    # Load the affinity matrix (csv)\n",
    "    # add column names 0-9\n",
    "    affinity_matrix = pd.read_csv(os.path.join(affinity_matrices_dir, f), header=None)\n",
    "    affinity_matrix.columns = range(10)\n",
    "    # Extract the condition name\n",
    "    condition_name = f.split(\"_\")[2]\n",
    "    # Store the affinity matrix in the dictionary\n",
    "    affinity_matrices[condition_name] = affinity_matrix\n",
    "    \n",
    "\n",
    "data_all = pd.DataFrame()\n",
    "for test_3mo, test_12mo, test_20mo in loo_combinations:\n",
    "    # Create a subfolder for this epoch\n",
    "    epoch_folder = os.path.join(results_dir, f\"epoch_{epoch_index}\")\n",
    "    if not os.path.exists(epoch_folder):\n",
    "        os.makedirs(epoch_folder)\n",
    "    \n",
    "    print(f\"Epoch {epoch_index}: Test datasets: 3mo={test_3mo}, 12mo={test_12mo}, 20mo={test_20mo}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Separate test and reference data\n",
    "    # -------------------------------\n",
    "    # For each age group, test data = rows with dataset == test_x, reference = rows with dataset != test_x.\n",
    "    selected_3mo_df = transformed_3mo_bones_df[transformed_3mo_bones_df[\"dataset\"] == test_3mo]\n",
    "    selected_12mo_df = transformed_12mo_bones_df[transformed_12mo_bones_df[\"dataset\"] == test_12mo]\n",
    "    selected_20mo_df = transformed_20mo_bones_df[transformed_20mo_bones_df[\"dataset\"] == test_20mo]\n",
    "\n",
    "    # Save the selected test datasets for record\n",
    "    selected_3mo_df.to_csv(os.path.join(epoch_folder, \"selected_3mo_bones_inside_df.csv\"), index=False)\n",
    "    selected_12mo_df.to_csv(os.path.join(epoch_folder, \"selected_12mo_bones_inside_df.csv\"), index=False)\n",
    "    selected_20mo_df.to_csv(os.path.join(epoch_folder, \"selected_20mo_bones_inside_df.csv\"), index=False)\n",
    "    \n",
    "    # Define reference datasets (exclude the test dataset)\n",
    "    ref_3mo_df = transformed_3mo_bones_df[transformed_3mo_bones_df[\"dataset\"] != test_3mo]\n",
    "    ref_12mo_df = transformed_12mo_bones_df[transformed_12mo_bones_df[\"dataset\"] != test_12mo]\n",
    "    ref_20mo_df = transformed_20mo_bones_df[transformed_20mo_bones_df[\"dataset\"] != test_20mo]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 2. Process the reference data to compute features (KDE, histograms, affinity matrices)\n",
    "    # -------------------------------\n",
    "    \n",
    "    kde_results_3mo_ref, _ = kde_for_clusters(ref_3mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_12mo_ref, _ = kde_for_clusters(ref_12mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_20mo_ref, _ = kde_for_clusters(ref_20mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    \n",
    "    kde_results_3mo_selected, _ = kde_for_clusters(selected_3mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_12mo_selected, _ = kde_for_clusters(selected_12mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_20mo_selected, _ = kde_for_clusters(selected_20mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    \n",
    "    # Optional: Save the KDE results\n",
    "    \"\"\"\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_ref.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_3mo_ref, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_ref.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_12mo_ref, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_ref.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_20mo_ref, f)\n",
    "        \n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_3mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_12mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_20mo_selected, f)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read the KDE results from the saved files\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_ref.pkl\"), \"rb\") as f:\n",
    "        kde_results_3mo_ref = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_ref.pkl\"), \"rb\") as f:\n",
    "        kde_results_12mo_ref = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_ref.pkl\"), \"rb\") as f:\n",
    "        kde_results_20mo_ref = pickle.load(f)\n",
    "        \n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_selected.pkl\"), \"rb\") as f:\n",
    "        kde_results_3mo_selected = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_selected.pkl\"), \"rb\") as f:\n",
    "        kde_results_12mo_selected = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_selected.pkl\"), \"rb\") as f:\n",
    "        kde_results_20mo_selected = pickle.load(f)\n",
    "    \"\"\"  \n",
    "    # Normalize the KDE results for cKits and HSCs\n",
    "    kde_results_3mo_ref_normalized = normalize_kde_values(kde_results_3mo_ref)\n",
    "    kde_results_12mo_ref_normalized = normalize_kde_values(kde_results_12mo_ref)\n",
    "    kde_results_20mo_ref_normalized = normalize_kde_values(kde_results_20mo_ref)\n",
    "    \n",
    "    # Normalize the KDE results for selected bones\n",
    "    kde_results_3mo_selected_normalized = normalize_kde_values(kde_results_3mo_selected)\n",
    "    kde_results_12mo_selected_normalized = normalize_kde_values(kde_results_12mo_selected)\n",
    "    kde_results_20mo_selected_normalized = normalize_kde_values(kde_results_20mo_selected)\n",
    "\n",
    "    \n",
    "    # Instead of using hsc histograms, we will use hsc cluster composition instead\n",
    "    hsc_cluster_comp_3mo_ref = compute_proportions_hsc(ref_3mo_df)\n",
    "    hsc_cluster_comp_12mo_ref = compute_proportions_hsc(ref_12mo_df)\n",
    "    hsc_cluster_comp_20mo_ref = compute_proportions_hsc(ref_20mo_df)\n",
    "    hsc_cluster_comp_3mo_selected = compute_proportions_hsc(selected_3mo_df)\n",
    "    hsc_cluster_comp_12mo_selected = compute_proportions_hsc(selected_12mo_df)\n",
    "    hsc_cluster_comp_20mo_selected = compute_proportions_hsc(selected_20mo_df)\n",
    "    \n",
    "    # Save the HSC cluster composition results as csv\n",
    "    \"\"\"\n",
    "    hsc_cluster_comp_3mo_ref.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_3mo_ref.csv\"), index=False)\n",
    "    hsc_cluster_comp_12mo_ref.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_12mo_ref.csv\"), index=False)\n",
    "    hsc_cluster_comp_20mo_ref.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_20mo_ref.csv\"), index=False)\n",
    "    hsc_cluster_comp_3mo_selected.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_3mo_selected.csv\"), index=False)\n",
    "    hsc_cluster_comp_12mo_selected.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_12mo_selected.csv\"), index=False)\n",
    "    hsc_cluster_comp_20mo_selected.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_20mo_selected.csv\"), index=False)\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # Process affinity matrices for reference.\n",
    "    # -------------------------------\n",
    "    affinity_matrices_selected = {}\n",
    "    affinity_mat_selected_dict = {\"3mo\": test_3mo, \"12mo\": test_12mo, \"20mo\": test_20mo}\n",
    "\n",
    "    # Read the affinity matrices for the selected datasets\n",
    "    for age, bone_name in affinity_mat_selected_dict.items():\n",
    "        # Read the affinity matrix without header or index\n",
    "        affinity_matrix = pd.read_csv(f\"{affinity_matrices_dir}/affinity_age_{age}_bone_{bone_name}.csv\", header=None, delimiter=r\"\\s+\")\n",
    "        affinity_matrix.columns = range(10)\n",
    "        affinity_matrices_selected[age] = affinity_matrix\n",
    "    \n",
    "\n",
    "    affinity_matrices_3mo_ref = (affinity_matrices.get(\"3mo\") - affinity_matrices_selected[\"3mo\"]) / 3\n",
    "    affinity_matrices_12mo_ref = (affinity_matrices.get(\"12mo\") - affinity_matrices_selected[\"12mo\"]) / 3\n",
    "    affinity_matrices_20mo_ref = (affinity_matrices.get(\"20mo\") - affinity_matrices_selected[\"20mo\"]) / 3\n",
    "    \n",
    "    # Normalize the affinity matrices by row to fit the assumption of Wasserstein distance\n",
    "    affinity_matrices_3mo_ref = affinity_matrices_3mo_ref.div(affinity_matrices_3mo_ref.sum(axis=1), axis=0)\n",
    "    affinity_matrices_12mo_ref = affinity_matrices_12mo_ref.div(affinity_matrices_12mo_ref.sum(axis=1), axis=0)\n",
    "    affinity_matrices_20mo_ref = affinity_matrices_20mo_ref.div(affinity_matrices_20mo_ref.sum(axis=1), axis=0)\n",
    "\n",
    "    # Save the reference affinity matrices to CSV files (without header or index)\n",
    "    \"\"\"\n",
    "    affinity_matrices_3mo_ref.to_csv(os.path.join(epoch_folder, \"affinity_matrices_3mo_ref_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_12mo_ref.to_csv(os.path.join(epoch_folder, \"affinity_matrices_12mo_ref_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_20mo_ref.to_csv(os.path.join(epoch_folder, \"affinity_matrices_20mo_ref_normalized.csv\"), index=False, header=False)\n",
    "    \"\"\"\n",
    "    # Normalize the selected affinity matrices by row (for CE and MSE)\n",
    "    affinity_matrices_selected[\"3mo\"] = affinity_matrices_selected[\"3mo\"].div(affinity_matrices_selected[\"3mo\"].sum(axis=1), axis=0)\n",
    "    affinity_matrices_selected[\"12mo\"] = affinity_matrices_selected[\"12mo\"].div(affinity_matrices_selected[\"12mo\"].sum(axis=1), axis=0)\n",
    "    affinity_matrices_selected[\"20mo\"] = affinity_matrices_selected[\"20mo\"].div(affinity_matrices_selected[\"20mo\"].sum(axis=1), axis=0)\n",
    "    \n",
    "    # Save the affinity matrices for the selected datasets\n",
    "    \"\"\"\n",
    "    affinity_matrices_selected[\"3mo\"].to_csv(os.path.join(epoch_folder, \"affinity_matrices_3mo_selected_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_selected[\"12mo\"].to_csv(os.path.join(epoch_folder, \"affinity_matrices_12mo_selected_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_selected[\"20mo\"].to_csv(os.path.join(epoch_folder, \"affinity_matrices_20mo_selected_normalized.csv\"), index=False, header=False)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. Compute features for the selected datasets (test data)\n",
    "    # -------------------------------\n",
    "    clusters = list(range(10))\n",
    "    # 3.1 Compute feature values (without using the weights) for all selected datasets\n",
    "\n",
    "    # 3.1.1 Calculate the cKit KDE-based feature values (KL divergence)\n",
    "    cKit_3mo_3mo_kl_divs = {}\n",
    "    cKit_3mo_12mo_kl_divs = {}\n",
    "    cKit_3mo_20mo_kl_divs = {}\n",
    "\n",
    "    cKit_12mo_3mo_kl_divs = {}\n",
    "    cKit_12mo_12mo_kl_divs = {}\n",
    "    cKit_12mo_20mo_kl_divs = {}\n",
    "\n",
    "    cKit_20mo_3mo_kl_divs = {}\n",
    "    cKit_20mo_12mo_kl_divs = {}\n",
    "    cKit_20mo_20mo_kl_divs = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cKit_3mo_3mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_3mo_selected_normalized[(\"cKits\", cluster)], kde_results_3mo_ref_normalized[(\"cKits\", cluster)])\n",
    "        cKit_3mo_12mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_3mo_selected_normalized[(\"cKits\", cluster)], kde_results_12mo_ref_normalized[(\"cKits\", cluster)])\n",
    "        cKit_3mo_20mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_3mo_selected_normalized[(\"cKits\", cluster)], kde_results_20mo_ref_normalized[(\"cKits\", cluster)])\n",
    "\n",
    "        cKit_12mo_3mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_12mo_selected_normalized[(\"cKits\", cluster)], kde_results_3mo_ref_normalized[(\"cKits\", cluster)])\n",
    "        cKit_12mo_12mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_12mo_selected_normalized[(\"cKits\", cluster)], kde_results_12mo_ref_normalized[(\"cKits\", cluster)])\n",
    "        cKit_12mo_20mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_12mo_selected_normalized[(\"cKits\", cluster)], kde_results_20mo_ref_normalized[(\"cKits\", cluster)])\n",
    "\n",
    "        cKit_20mo_3mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_20mo_selected_normalized[(\"cKits\", cluster)], kde_results_3mo_ref_normalized[(\"cKits\", cluster)])\n",
    "        cKit_20mo_12mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_20mo_selected_normalized[(\"cKits\", cluster)], kde_results_12mo_ref_normalized[(\"cKits\", cluster)])\n",
    "        cKit_20mo_20mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_20mo_selected_normalized[(\"cKits\", cluster)], kde_results_20mo_ref_normalized[(\"cKits\", cluster)])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # 3.1.2 Calculate the HSC cluster composition based features (jsd)\n",
    "    p_3mo_ref = hsc_cluster_comp_3mo_ref.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    p_12mo_ref = hsc_cluster_comp_12mo_ref.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    p_20mo_ref = hsc_cluster_comp_20mo_ref.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    \n",
    "    p_3mo_selected = hsc_cluster_comp_3mo_selected.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    p_12mo_selected = hsc_cluster_comp_12mo_selected.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    p_20mo_selected = hsc_cluster_comp_20mo_selected.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    \n",
    "    HSC_3mo_3mo_cluster_comp = jensenshannon(p_3mo_selected, p_3mo_ref) **2\n",
    "    HSC_3mo_12mo_cluster_comp = jensenshannon(p_3mo_selected, p_12mo_ref) **2\n",
    "    HSC_3mo_20mo_cluster_comp = jensenshannon(p_3mo_selected, p_20mo_ref) **2\n",
    "    HSC_12mo_3mo_cluster_comp = jensenshannon(p_12mo_selected, p_3mo_ref) **2\n",
    "    HSC_12mo_12mo_cluster_comp = jensenshannon(p_12mo_selected, p_12mo_ref) **2\n",
    "    HSC_12mo_20mo_cluster_comp = jensenshannon(p_12mo_selected, p_20mo_ref) **2\n",
    "    HSC_20mo_3mo_cluster_comp = jensenshannon(p_20mo_selected, p_3mo_ref) **2\n",
    "    HSC_20mo_12mo_cluster_comp = jensenshannon(p_20mo_selected, p_12mo_ref) **2\n",
    "    HSC_20mo_20mo_cluster_comp = jensenshannon(p_20mo_selected, p_20mo_ref) **2\n",
    "    \n",
    "    \n",
    "    # 3.1.3 Calculate the HSC numbers\n",
    "    HSC_3mo_selected_num = selected_3mo_df[selected_3mo_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "    HSC_12mo_selected_num = selected_12mo_df[selected_12mo_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "    HSC_20mo_selected_num = selected_20mo_df[selected_20mo_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "\n",
    "\n",
    "    # 3.1.4 Calculate the HSC KDE-based feature values (KL divergence)\n",
    "    HSC_3mo_3mo_kl_div = calculate_kl_divergence(kde_results_3mo_selected_normalized[\"HSCs\"], kde_results_3mo_ref_normalized[\"HSCs\"])\n",
    "    HSC_3mo_12mo_kl_div = calculate_kl_divergence(kde_results_3mo_selected_normalized[\"HSCs\"], kde_results_12mo_ref_normalized[\"HSCs\"])\n",
    "    HSC_3mo_20mo_kl_div = calculate_kl_divergence(kde_results_3mo_selected_normalized[\"HSCs\"], kde_results_20mo_ref_normalized[\"HSCs\"])\n",
    "\n",
    "    HSC_12mo_3mo_kl_div = calculate_kl_divergence(kde_results_12mo_selected_normalized[\"HSCs\"], kde_results_3mo_ref_normalized[\"HSCs\"])\n",
    "    HSC_12mo_12mo_kl_div = calculate_kl_divergence(kde_results_12mo_selected_normalized[\"HSCs\"], kde_results_12mo_ref_normalized[\"HSCs\"])\n",
    "    HSC_12mo_20mo_kl_div = calculate_kl_divergence(kde_results_12mo_selected_normalized[\"HSCs\"], kde_results_20mo_ref_normalized[\"HSCs\"])\n",
    "\n",
    "    HSC_20mo_3mo_kl_div = calculate_kl_divergence(kde_results_20mo_selected_normalized[\"HSCs\"], kde_results_3mo_ref_normalized[\"HSCs\"])\n",
    "    HSC_20mo_12mo_kl_div = calculate_kl_divergence(kde_results_20mo_selected_normalized[\"HSCs\"], kde_results_12mo_ref_normalized[\"HSCs\"])\n",
    "    HSC_20mo_20mo_kl_div = calculate_kl_divergence(kde_results_20mo_selected_normalized[\"HSCs\"], kde_results_20mo_ref_normalized[\"HSCs\"])\n",
    "\n",
    "\n",
    "    # 3.1.5 Calculate the cKit affinity-based feature values (Cross Entropy)\n",
    "    cKit_3mo_3mo_ces = {}\n",
    "    cKit_3mo_12mo_ces = {}\n",
    "    cKit_3mo_20mo_ces = {}\n",
    "\n",
    "    cKit_12mo_3mo_ces = {}\n",
    "    cKit_12mo_12mo_ces = {}\n",
    "    cKit_12mo_20mo_ces = {}\n",
    "\n",
    "    cKit_20mo_3mo_ces = {}\n",
    "    cKit_20mo_12mo_ces = {}\n",
    "    cKit_20mo_20mo_ces = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        # Matched comparisons\n",
    "        cKit_3mo_3mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"3mo\"].iloc[cluster, :], affinity_matrices_3mo_ref.iloc[cluster, :])\n",
    "        cKit_12mo_12mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"12mo\"].iloc[cluster, :], affinity_matrices_12mo_ref.iloc[cluster, :])\n",
    "        cKit_20mo_20mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"20mo\"].iloc[cluster, :], affinity_matrices_20mo_ref.iloc[cluster, :])\n",
    "\n",
    "        # Unmatched comparisons\n",
    "        cKit_3mo_12mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"3mo\"].iloc[cluster, :], affinity_matrices_12mo_ref.iloc[cluster, :])\n",
    "        cKit_3mo_20mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"3mo\"].iloc[cluster, :], affinity_matrices_20mo_ref.iloc[cluster, :])\n",
    "\n",
    "        cKit_12mo_3mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"12mo\"].iloc[cluster, :], affinity_matrices_3mo_ref.iloc[cluster, :])\n",
    "        cKit_12mo_20mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"12mo\"].iloc[cluster, :], affinity_matrices_20mo_ref.iloc[cluster, :])\n",
    "\n",
    "        cKit_20mo_3mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"20mo\"].iloc[cluster, :], affinity_matrices_3mo_ref.iloc[cluster, :])\n",
    "        cKit_20mo_12mo_ces[cluster] = calculate_cross_entropy(affinity_matrices_selected[\"20mo\"].iloc[cluster, :], affinity_matrices_12mo_ref.iloc[cluster, :])\n",
    "        \n",
    "        \n",
    "    \n",
    "    # 3.2 Compute the cluster size and cluster weights for the selected datasets\n",
    "    # Compute cluster sizes for each selected dataset using the function calculate_cluster_sizes\n",
    "    cluster_sizes_selected = {}\n",
    "    for transformed_df, condition in zip([selected_3mo_df, selected_12mo_df, selected_20mo_df],\n",
    "                                        [\"3mo_selected\", \"12mo_selected\", \"20mo_selected\"]):\n",
    "        cluster_sizes_selected[condition] = calculate_cluster_sizes(transformed_df)\n",
    "\n",
    "    # Compute cluster sizes for the reference data (for normalization)\n",
    "    cluster_sizes_ref = {}\n",
    "    for transformed_df, condition in zip([ref_3mo_df, ref_12mo_df, ref_20mo_df],\n",
    "                                        [\"3mo_ref\", \"12mo_ref\", \"20mo_ref\"]):\n",
    "        cluster_sizes_ref[condition] = calculate_cluster_sizes(transformed_df)\n",
    "    \n",
    "    # Define the sources and clusters to use for weighted averaging.\n",
    "    sources = [\"cKits\", \"HSCs\"]  # adjust if you use additional sources\n",
    "    clusters_list = list(range(10))\n",
    "    \n",
    "    # Compute weighted cluster sizes for each age condition using compute_weighted_cluster_sizes.\n",
    "    cluster_size_3mo_selected = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                            cluster_sizes=cluster_sizes_selected,\n",
    "                                                            condition=\"3mo_selected\",\n",
    "                                                            sources=sources,\n",
    "                                                            clusters=clusters_list)\n",
    "    cluster_size_12mo_selected = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                            cluster_sizes=cluster_sizes_selected,\n",
    "                                                            condition=\"12mo_selected\",\n",
    "                                                            sources=sources,\n",
    "                                                            clusters=clusters_list)\n",
    "    cluster_size_20mo_selected = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                            cluster_sizes=cluster_sizes_selected,\n",
    "                                                            condition=\"20mo_selected\",\n",
    "                                                            sources=sources,\n",
    "                                                            clusters=clusters_list)\n",
    "    \n",
    "    # Optionally, save the computed cluster sizes for the selected datasets for later inspection.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(epoch_folder, \"cluster_sizes_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_sizes_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"weighted_cluster_size_3mo_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_size_3mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"weighted_cluster_size_12mo_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_size_12mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"weighted_cluster_size_20mo_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_size_20mo_selected, f)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # with keys \"cKits\" and \"HSCs\" that map cluster indices (0..9) to normalized weights.\n",
    "    # -------------------------------\n",
    "    # Weighted average for cKit KDE-based features (KL divergence)\n",
    "    average_cKit_3mo_3mo_kl_div = np.average(list(cKit_3mo_3mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_3mo_12mo_kl_div = np.average(list(cKit_3mo_12mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_3mo_20mo_kl_div = np.average(list(cKit_3mo_20mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "\n",
    "    average_cKit_12mo_3mo_kl_div = np.average(list(cKit_12mo_3mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_12mo_12mo_kl_div = np.average(list(cKit_12mo_12mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_12mo_20mo_kl_div = np.average(list(cKit_12mo_20mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "\n",
    "    average_cKit_20mo_3mo_kl_div = np.average(list(cKit_20mo_3mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_20mo_12mo_kl_div = np.average(list(cKit_20mo_12mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_20mo_20mo_kl_div = np.average(list(cKit_20mo_20mo_kl_divs.values()), \n",
    "                                            weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Weighted average for cKit affinity-based features (Cross Entropy)\n",
    "    average_cKit_3mo_3mo_ce = np.average(list(cKit_3mo_3mo_ces.values()), \n",
    "                                        weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_3mo_12mo_ce = np.average(list(cKit_3mo_12mo_ces.values()), \n",
    "                                        weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_3mo_20mo_ce = np.average(list(cKit_3mo_20mo_ces.values()), \n",
    "                                        weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "\n",
    "    average_cKit_12mo_3mo_ce = np.average(list(cKit_12mo_3mo_ces.values()), \n",
    "                                        weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_12mo_12mo_ce = np.average(list(cKit_12mo_12mo_ces.values()), \n",
    "                                        weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_12mo_20mo_ce = np.average(list(cKit_12mo_20mo_ces.values()), \n",
    "                                        weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "\n",
    "    average_cKit_20mo_3mo_ce = np.average(list(cKit_20mo_3mo_ces.values()), \n",
    "                                        weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_20mo_12mo_ce = np.average(list(cKit_20mo_12mo_ces.values()), \n",
    "                                        weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    average_cKit_20mo_20mo_ce = np.average(list(cKit_20mo_20mo_ces.values()), \n",
    "                                        weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    \n",
    "    # 4. Add features and ground truth values to the training data dataframe\n",
    "    # -------------------------------\n",
    "    # Construct the training data row for this epoch\n",
    "\n",
    "\n",
    "    training_data_row_3mo = {\n",
    "        \"epoch\": epoch_index,\n",
    "        \"cKit Density Divergence (vs. 3mo)\": average_cKit_3mo_3mo_kl_div,\n",
    "        \"cKit Density Divergence (vs. 12mo)\": average_cKit_3mo_12mo_kl_div,\n",
    "        \"cKit Density Divergence (vs. 20mo)\": average_cKit_3mo_20mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 3mo)\": HSC_3mo_3mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 12mo)\": HSC_3mo_12mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 20mo)\": HSC_3mo_20mo_kl_div,\n",
    "        \"HSC Count\": HSC_3mo_selected_num,\n",
    "        \"HSC Composition (vs. 3mo)\": HSC_3mo_3mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 12mo)\": HSC_3mo_12mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 20mo)\": HSC_3mo_20mo_cluster_comp,\n",
    "        \"cKit Neighborhood Affinity (vs. 3mo)\": average_cKit_3mo_3mo_ce,\n",
    "        \"cKit Neighborhood Affinity (vs. 12mo)\": average_cKit_3mo_12mo_ce,\n",
    "        \"cKit Neighborhood Affinity (vs. 20mo)\": average_cKit_3mo_20mo_ce,\n",
    "        \"ground_truth\": 3\n",
    "    }\n",
    "    training_data_row_12mo = {\n",
    "        \"epoch\": epoch_index,\n",
    "        \"cKit Density Divergence (vs. 3mo)\": average_cKit_12mo_3mo_kl_div,\n",
    "        \"cKit Density Divergence (vs. 12mo)\": average_cKit_12mo_12mo_kl_div,\n",
    "        \"cKit Density Divergence (vs. 20mo)\": average_cKit_12mo_20mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 3mo)\": HSC_12mo_3mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 12mo)\": HSC_12mo_12mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 20mo)\": HSC_12mo_20mo_kl_div,\n",
    "        \"HSC Count\": HSC_12mo_selected_num,\n",
    "        \"HSC Composition (vs. 3mo)\": HSC_12mo_3mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 12mo)\": HSC_12mo_12mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 20mo)\": HSC_12mo_20mo_cluster_comp,\n",
    "        \"cKit Neighborhood Affinity (vs. 3mo)\": average_cKit_12mo_3mo_ce,\n",
    "        \"cKit Neighborhood Affinity (vs. 12mo)\": average_cKit_12mo_12mo_ce,\n",
    "        \"cKit Neighborhood Affinity (vs. 20mo)\": average_cKit_12mo_20mo_ce,\n",
    "        \"ground_truth\": 12\n",
    "    }\n",
    "    training_data_row_20mo = {\n",
    "        \"epoch\": epoch_index,\n",
    "        \"cKit Density Divergence (vs. 3mo)\": average_cKit_20mo_3mo_kl_div,\n",
    "        \"cKit Density Divergence (vs. 12mo)\": average_cKit_20mo_12mo_kl_div,\n",
    "        \"cKit Density Divergence (vs. 20mo)\": average_cKit_20mo_20mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 3mo)\": HSC_20mo_3mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 12mo)\": HSC_20mo_12mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 20mo)\": HSC_20mo_20mo_kl_div,\n",
    "        \"HSC Count\": HSC_20mo_selected_num,\n",
    "        \"HSC Composition (vs. 3mo)\": HSC_20mo_3mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 12mo)\": HSC_20mo_12mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 20mo)\": HSC_20mo_20mo_cluster_comp,\n",
    "        \"cKit Neighborhood Affinity (vs. 3mo)\": average_cKit_20mo_3mo_ce,\n",
    "        \"cKit Neighborhood Affinity (vs. 12mo)\": average_cKit_20mo_12mo_ce,\n",
    "        \"cKit Neighborhood Affinity (vs. 20mo)\": average_cKit_20mo_20mo_ce,\n",
    "        \"ground_truth\": 20\n",
    "    }\n",
    "\n",
    "\n",
    "    # Append rows to the list\n",
    "    training_data_list.append(training_data_row_3mo)\n",
    "    training_data_list.append(training_data_row_12mo)\n",
    "    training_data_list.append(training_data_row_20mo)\n",
    "    print(f\"Epoch {epoch_index} completed.\")\n",
    "    \n",
    "    epoch_index += 1\n",
    "# End of LOO-CV loop\n",
    "\n",
    "training_data_df = pd.DataFrame(training_data_list)\n",
    "# training_data_df.to_csv(os.path.join(results_dir, \"training_data_cluster_comp.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7931d",
   "metadata": {},
   "source": [
    "### 5.4 Weight optimization for the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LOO-CV to estimate the optimal weights for each epoch (with the interpolation assumtion)\n",
    "# ========================\n",
    "# Main LOO-CV Loop\n",
    "# ========================\n",
    "epoch_index = 1\n",
    "affinity_flag = \"CE\" # cross entropy\n",
    "for test_3mo, test_12mo, test_20mo in loo_combinations:\n",
    "    # Create a subfolder for this epoch\n",
    "    epoch_folder = os.path.join(results_dir, f\"epoch_{epoch_index}\")\n",
    "    if not os.path.exists(epoch_folder):\n",
    "        os.makedirs(epoch_folder)\n",
    "    \n",
    "    print(f\"Epoch {epoch_index}: Test datasets: 3mo={test_3mo}, 12mo={test_12mo}, 20mo={test_20mo}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Separate test and reference data\n",
    "    # -------------------------------\n",
    "    # For each age group, test data = rows with dataset == test_x, reference = rows with dataset != test_x.\n",
    "    selected_3mo_df = transformed_3mo_bones_df[transformed_3mo_bones_df[\"dataset\"] == test_3mo]\n",
    "    selected_12mo_df = transformed_12mo_bones_df[transformed_12mo_bones_df[\"dataset\"] == test_12mo]\n",
    "    selected_20mo_df = transformed_20mo_bones_df[transformed_20mo_bones_df[\"dataset\"] == test_20mo]\n",
    "\n",
    "    # Optional: Save the selected test datasets for record\n",
    "    # selected_3mo_df.to_csv(os.path.join(epoch_folder, \"selected_3mo_bones_inside_df.csv\"), index=False)\n",
    "    # selected_12mo_df.to_csv(os.path.join(epoch_folder, \"selected_12mo_bones_inside_df.csv\"), index=False)\n",
    "    # selected_20mo_df.to_csv(os.path.join(epoch_folder, \"selected_20mo_bones_inside_df.csv\"), index=False)\n",
    "    \n",
    "    # Define reference datasets (exclude the test dataset)\n",
    "    ref_3mo_df = transformed_3mo_bones_df[transformed_3mo_bones_df[\"dataset\"] != test_3mo]\n",
    "    ref_12mo_df = transformed_12mo_bones_df[transformed_12mo_bones_df[\"dataset\"] != test_12mo]\n",
    "    ref_20mo_df = transformed_20mo_bones_df[transformed_20mo_bones_df[\"dataset\"] != test_20mo]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 2. Process the reference data to compute features\n",
    "    # -------------------------------\n",
    "    \n",
    "    kde_results_3mo_ref, _ = kde_for_clusters(ref_3mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_12mo_ref, _ = kde_for_clusters(ref_12mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_20mo_ref, _ = kde_for_clusters(ref_20mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    \n",
    "    kde_results_3mo_selected, _ = kde_for_clusters(selected_3mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_12mo_selected, _ = kde_for_clusters(selected_12mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_20mo_selected, _ = kde_for_clusters(selected_20mo_df, bone_outline=ref_outline, binsize=10)\n",
    "    \n",
    "    # Optional: save the KDE results\n",
    "    \"\"\"\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_ref.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_3mo_ref, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_ref.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_12mo_ref, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_ref.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_20mo_ref, f)\n",
    "        \n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_3mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_12mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(kde_results_20mo_selected, f)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" If you have saved the kde results, you can load them here (approx. 17 hours)\n",
    "    # Read the KDE results from the saved files\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_ref.pkl\"), \"rb\") as f:\n",
    "        kde_results_3mo_ref = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_ref.pkl\"), \"rb\") as f:\n",
    "        kde_results_12mo_ref = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_ref.pkl\"), \"rb\") as f:\n",
    "        kde_results_20mo_ref = pickle.load(f)\n",
    "        \n",
    "    with open(os.path.join(epoch_folder, \"kde_results_3mo_bones_selected.pkl\"), \"rb\") as f:\n",
    "        kde_results_3mo_selected = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_12mo_bones_selected.pkl\"), \"rb\") as f:\n",
    "        kde_results_12mo_selected = pickle.load(f)\n",
    "    with open(os.path.join(epoch_folder, \"kde_results_20mo_bones_selected.pkl\"), \"rb\") as f:\n",
    "        kde_results_20mo_selected = pickle.load(f)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Instead of using hsc histograms, we will use hsc cluster composition instead\n",
    "    hsc_cluster_comp_3mo_ref = compute_proportions_hsc(ref_3mo_df)\n",
    "    hsc_cluster_comp_12mo_ref = compute_proportions_hsc(ref_12mo_df)\n",
    "    hsc_cluster_comp_20mo_ref = compute_proportions_hsc(ref_20mo_df)\n",
    "    hsc_cluster_comp_3mo_selected = compute_proportions_hsc(selected_3mo_df)\n",
    "    hsc_cluster_comp_12mo_selected = compute_proportions_hsc(selected_12mo_df)\n",
    "    hsc_cluster_comp_20mo_selected = compute_proportions_hsc(selected_20mo_df)\n",
    "    \n",
    "    # Save the HSC cluster composition results as csv\n",
    "    \"\"\"\n",
    "    hsc_cluster_comp_3mo_ref.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_3mo_ref.csv\"), index=False)\n",
    "    hsc_cluster_comp_12mo_ref.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_12mo_ref.csv\"), index=False)\n",
    "    hsc_cluster_comp_20mo_ref.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_20mo_ref.csv\"), index=False)\n",
    "    hsc_cluster_comp_3mo_selected.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_3mo_selected.csv\"), index=False)\n",
    "    hsc_cluster_comp_12mo_selected.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_12mo_selected.csv\"), index=False)\n",
    "    hsc_cluster_comp_20mo_selected.to_csv(os.path.join(epoch_folder, \"hsc_cluster_comp_20mo_selected.csv\"), index=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Process affinity matrices for reference.\n",
    "    # The overall affinity matrices are assumed to be stored in a dictionary \"affinity_matrices\"\n",
    "    # and the test (selected) affinity matrices have been loaded into affinity_matrices_selected.\n",
    "    # Since you have 4 bones per age, the reference matrix is computed as:\n",
    "    #   (overall affinity matrix - test affinity matrix) / 3\n",
    "    # -------------------------------\n",
    "    affinity_matrices_selected = {}\n",
    "    affinity_mat_selected_dict = {\"3mo\": test_3mo, \"12mo\": test_12mo, \"20mo\": test_20mo}\n",
    "\n",
    "    # Read the affinity matrices for the selected datasets\n",
    "    for age, bone_name in affinity_mat_selected_dict.items():\n",
    "        # Read the affinity matrix without header or index\n",
    "        affinity_matrix = pd.read_csv(f\"{affinity_matrices_dir}/affinity_age_{age}_bone_{bone_name}.csv\", header=None, delimiter=r\"\\s+\")\n",
    "        affinity_matrix.columns = range(10)\n",
    "        affinity_matrices_selected[age] = affinity_matrix\n",
    "    \n",
    "\n",
    "    affinity_matrices_3mo_ref = (affinity_matrices.get(\"3mo\") - affinity_matrices_selected[\"3mo\"]) / 3\n",
    "    affinity_matrices_12mo_ref = (affinity_matrices.get(\"12mo\") - affinity_matrices_selected[\"12mo\"]) / 3\n",
    "    affinity_matrices_20mo_ref = (affinity_matrices.get(\"20mo\") - affinity_matrices_selected[\"20mo\"]) / 3\n",
    "    \n",
    "    # Normalize the affinity matrices by row to fit the assumption of Wasserstein distance\n",
    "    affinity_matrices_3mo_ref = affinity_matrices_3mo_ref.div(affinity_matrices_3mo_ref.sum(axis=1), axis=0)\n",
    "    affinity_matrices_12mo_ref = affinity_matrices_12mo_ref.div(affinity_matrices_12mo_ref.sum(axis=1), axis=0)\n",
    "    affinity_matrices_20mo_ref = affinity_matrices_20mo_ref.div(affinity_matrices_20mo_ref.sum(axis=1), axis=0)\n",
    "\n",
    "    # Save the reference affinity matrices to CSV files (without header or index)\n",
    "    affinity_matrices_3mo_ref.to_csv(os.path.join(epoch_folder, \"affinity_matrices_3mo_ref_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_12mo_ref.to_csv(os.path.join(epoch_folder, \"affinity_matrices_12mo_ref_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_20mo_ref.to_csv(os.path.join(epoch_folder, \"affinity_matrices_20mo_ref_normalized.csv\"), index=False, header=False)\n",
    "    \n",
    "    # Normalize the selected affinity matrices by row (for CE and MSE)\n",
    "    affinity_matrices_selected[\"3mo\"] = affinity_matrices_selected[\"3mo\"].div(affinity_matrices_selected[\"3mo\"].sum(axis=1), axis=0)\n",
    "    affinity_matrices_selected[\"12mo\"] = affinity_matrices_selected[\"12mo\"].div(affinity_matrices_selected[\"12mo\"].sum(axis=1), axis=0)\n",
    "    affinity_matrices_selected[\"20mo\"] = affinity_matrices_selected[\"20mo\"].div(affinity_matrices_selected[\"20mo\"].sum(axis=1), axis=0)\n",
    "    \n",
    "    # Save the affinity matrices for the selected datasets\n",
    "    affinity_matrices_selected[\"3mo\"].to_csv(os.path.join(epoch_folder, \"affinity_matrices_3mo_selected_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_selected[\"12mo\"].to_csv(os.path.join(epoch_folder, \"affinity_matrices_12mo_selected_normalized.csv\"), index=False, header=False)\n",
    "    affinity_matrices_selected[\"20mo\"].to_csv(os.path.join(epoch_folder, \"affinity_matrices_20mo_selected_normalized.csv\"), index=False, header=False)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 3. Perform interpolation of the features\n",
    "    # This block creates the dictionaries:\n",
    "    #   interpolated_kde_ckits, interpolated_histograms_hsc, interpolated_affinity_matrices, interpolated_kde_hsc_merged\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Define the real time points corresponding to your reference ages\n",
    "    real_times = np.array([3, 12, 20])\n",
    "    # Create a fine-grained time grid (e.g., 100 points between 3 and 20)\n",
    "    fine_times = np.linspace(3, 20, 100)\n",
    "    \n",
    "    # Normalize the KDE results for cKits and HSCs\n",
    "    kde_results_3mo_ref_normalized = normalize_kde_values(kde_results_3mo_ref)\n",
    "    kde_results_12mo_ref_normalized = normalize_kde_values(kde_results_12mo_ref)\n",
    "    kde_results_20mo_ref_normalized = normalize_kde_values(kde_results_20mo_ref)\n",
    "    \n",
    "    # Normalize the KDE results for selected bones\n",
    "    kde_results_3mo_selected_normalized = normalize_kde_values(kde_results_3mo_selected)\n",
    "    kde_results_12mo_selected_normalized = normalize_kde_values(kde_results_12mo_selected)\n",
    "    kde_results_20mo_selected_normalized = normalize_kde_values(kde_results_20mo_selected)\n",
    "    \n",
    "    # Initialize dictionaries to hold the interpolated results\n",
    "    interpolated_kde_ckits = {}\n",
    "    interpolated_histograms_hsc = {}\n",
    "    interpolated_affinity_matrices = {}\n",
    "\n",
    "    # Define the list of clusters (0 to 9)\n",
    "    clusters = list(range(10))\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 3a. Interpolate the KDEs for cKits per cluster\n",
    "    # -------------------------------\n",
    "    for cluster in clusters:\n",
    "        # For each age group, extract the KDE for the given cluster from the normalized S2 results\n",
    "        age_kdes = np.array([\n",
    "            kde_results_3mo_ref_normalized[(\"cKits\", cluster)],\n",
    "            kde_results_12mo_ref_normalized[(\"cKits\", cluster)],\n",
    "            kde_results_20mo_ref_normalized[(\"cKits\", cluster)]\n",
    "        ])\n",
    "        # Create a linear interpolator for this cluster (along the age dimension)\n",
    "        f_kde = interp1d(real_times, age_kdes, axis=0, kind=\"linear\")\n",
    "        # Evaluate the interpolator at the fine-grained time points and store in the dictionary\n",
    "        # interpolated_kde_ckits[cluster] = f_kde(fine_times)\n",
    "        \n",
    "        # Evaluate the interpolator at the fine-grained time points\n",
    "        interp_values_kde = f_kde(fine_times)\n",
    "        # Normalize each interpolated pdf (each row) so that its sum is 1\n",
    "        interp_values_normalized_kde = interp_values_kde / np.sum(interp_values_kde, axis=(1,2), keepdims=True)\n",
    "        # Store the normalized result in the dictionary\n",
    "        interpolated_kde_ckits[cluster] = interp_values_normalized_kde\n",
    "    \n",
    "\n",
    "    # -------------------------------\n",
    "    # 3b. Interpolate the HSC cluster composition (as the alternative of the histograms)\n",
    "    # -------------------------------\n",
    "    age_hsc_cluster_comp = stack_proportions(hsc_cluster_comp_3mo_ref, hsc_cluster_comp_12mo_ref, hsc_cluster_comp_20mo_ref)\n",
    "    # Create a linear interpolator for the HSC cluster composition\n",
    "    f_hsc_cluster_comp = interp1d(real_times, age_hsc_cluster_comp, axis=1, kind=\"linear\", fill_value=\"extrapolate\") # It is correct here to use axis=1\n",
    "    \n",
    "    # Step 3 (revised): Interpolate and normalize each timepoint\n",
    "    interp_values_hsc_cluster_comp = f_hsc_cluster_comp(fine_times)  # shape: (100 timepoints, 10 clusters)\n",
    "\n",
    "    # Step 4 Normalize to make sure each vector sums to 1\n",
    "    interp_values_hsc_cluster_comp = interp_values_hsc_cluster_comp.T\n",
    "    interp_values_hsc_cluster_comp = interp_values_hsc_cluster_comp / interp_values_hsc_cluster_comp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3c. Interpolate the affinity matrices row-wise per cluster\n",
    "    # -------------------------------\n",
    "    for cluster in clusters:\n",
    "        # For each age group, extract the row corresponding to the cluster from the reference affinity matrices.\n",
    "        # We convert each row to a NumPy array.\n",
    "        age_affinity_matrices = np.array([\n",
    "            affinity_matrices_3mo_ref.iloc[cluster, :].values,\n",
    "            affinity_matrices_12mo_ref.iloc[cluster, :].values,\n",
    "            affinity_matrices_20mo_ref.iloc[cluster, :].values\n",
    "        ])\n",
    "        # Create a linear interpolator for this row\n",
    "        f_affinity = interp1d(real_times, age_affinity_matrices, axis=0, kind=\"linear\")\n",
    "        # Evaluate the interpolator at the fine time grid and store the result\n",
    "        # interpolated_affinity_matrices[cluster] = f_affinity(fine_times)\n",
    "        \n",
    "        # Normalize the interpolated values as probabilities\n",
    "        # Evaluate the interpolator at the fine time grid\n",
    "        interp_values_affinity = f_affinity(fine_times)\n",
    "        # Re-normalize each interpolated row so that it sums to 1:\n",
    "        interp_values_normalized_affinity = interp_values_affinity / np.sum(interp_values_affinity, axis=1, keepdims=True)\n",
    "        # Store the normalized interpolated row\n",
    "        interpolated_affinity_matrices[cluster] = interp_values_normalized_affinity\n",
    "        \n",
    "    # -------------------------------\n",
    "    # 3d. Interpolate merged HSC KDEs\n",
    "    # For the entire \"HSCs\" key, assume the normalized KDE results are arrays.\n",
    "    # -------------------------------\n",
    "    age_kdes_hsc_merged = np.array([\n",
    "        kde_results_3mo_ref_normalized[\"HSCs\"],\n",
    "        kde_results_12mo_ref_normalized[\"HSCs\"],\n",
    "        kde_results_20mo_ref_normalized[\"HSCs\"]\n",
    "    ])\n",
    "    f_kde_hsc_merged = interp1d(real_times, age_kdes_hsc_merged, axis=0, kind=\"linear\")\n",
    "    # interpolated_kde_hsc_merged = f_kde_hsc_merged(fine_times)\n",
    "    \n",
    "    # Normalize the interpolated values\n",
    "    interp_values_kde_hsc_merged = f_kde_hsc_merged(fine_times)\n",
    "    interp_values_normalized_hsc_merged = interp_values_kde_hsc_merged / np.sum(interp_values_kde_hsc_merged, axis=(1,2), keepdims=True)\n",
    "    interpolated_kde_hsc_merged = interp_values_normalized_hsc_merged\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # 4. Compute the \"optimal input\" for each age condition\n",
    "    # -------------------------------\n",
    "\n",
    "    # Initialize containers for the per-modality, per-age estimates.\n",
    "    # For cKits (KDE-based estimates) we store a list (one value per cluster)\n",
    "    age_ckit_3mo_selected = []\n",
    "    age_ckit_12mo_selected = []\n",
    "    age_ckit_20mo_selected = []\n",
    "\n",
    "    # For cKit affinity, we store a list per condition\n",
    "    age_affinity_3mo_selected = []\n",
    "    age_affinity_12mo_selected = []\n",
    "    age_affinity_20mo_selected = []\n",
    "    \n",
    "    # For HSC numbers (exponential regression) we have a single value per condition (without cluster)\n",
    "    age_num_3mo_selected = 0\n",
    "    age_num_12mo_selected = 0\n",
    "    age_num_20mo_selected = 0\n",
    "    \n",
    "    # HSC numbers-based age estimates using exponential regression (no clusters)\n",
    "    age_groups_numeric = np.array([3, 12, 20])  # Numeric representation of age groups\n",
    "    conditions_ref = [\"3mo_ref\", \"12mo_ref\", \"20mo_ref\"]\n",
    "    conditions_selected = [\"3mo_selected\", \"12mo_selected\", \"20mo_selected\"]\n",
    "    \n",
    "    hsc_nums_ref = {}\n",
    "    hsc_nums_selected = {}\n",
    "\n",
    "\n",
    "    # Calculate the number of cells per cluster for each condition (ref)\n",
    "    for transformed_df, condition in zip(\n",
    "        [ref_3mo_df, ref_12mo_df, ref_20mo_df],\n",
    "        conditions_ref,\n",
    "    ):\n",
    "        hsc_condition = len(transformed_df[transformed_df[\"source\"] == \"HSCs\"])\n",
    "        # Normalize by the number of datasets\n",
    "        num_datasets = len(transformed_df[\"dataset\"].unique())\n",
    "        hsc_condition = hsc_condition / num_datasets\n",
    "        hsc_nums_ref[condition] = hsc_condition\n",
    "        \n",
    "    # Calculate the number of cells per cluster for each condition (selected)\n",
    "    for transformed_df, condition in zip(\n",
    "        [selected_3mo_df, selected_12mo_df, selected_20mo_df],\n",
    "        conditions_selected,\n",
    "    ):\n",
    "        hsc_condition = len(transformed_df[transformed_df[\"source\"] == \"HSCs\"])\n",
    "        # Normalize by the number of datasets\n",
    "        num_datasets = len(transformed_df[\"dataset\"].unique())\n",
    "        hsc_condition = hsc_condition / num_datasets\n",
    "        hsc_nums_selected[condition] = hsc_condition\n",
    "        \n",
    "    hsc_nums_matrix_ref = np.array([hsc_nums_ref[condition] for condition in conditions_ref])  # Convert dictionary to matrix\n",
    "    hsc_nums_matrix_selected = np.array([hsc_nums_selected[condition] for condition in conditions_selected])  # Convert dictionary to matrix\n",
    "\n",
    "    # For 3mo:\n",
    "    try:\n",
    "        popt_exp, _ = curve_fit(exponential_func, age_groups_numeric, hsc_nums_matrix_ref[:3], maxfev=10000)\n",
    "        exp_func = lambda x: exponential_func(x, *popt_exp)\n",
    "        intersection_3mo = find_intersection(exp_func, hsc_nums_matrix_selected[0], fine_times, method=\"closest\")\n",
    "    except Exception as e:\n",
    "        intersection_3mo = np.nan\n",
    "    age_num_3mo_selected = intersection_3mo\n",
    "\n",
    "    # For 12mo:\n",
    "    try:\n",
    "        popt_exp, _ = curve_fit(exponential_func, age_groups_numeric, hsc_nums_matrix_ref[:3], maxfev=10000)\n",
    "        exp_func = lambda x: exponential_func(x, *popt_exp)\n",
    "        intersection_12mo = find_intersection(exp_func, hsc_nums_matrix_selected[1], fine_times, method=\"closest\")\n",
    "    except Exception as e:\n",
    "        intersection_12mo = np.nan\n",
    "    age_num_12mo_selected = intersection_12mo\n",
    "\n",
    "    # For 20mo:\n",
    "    try:\n",
    "        popt_exp, _ = curve_fit(exponential_func, age_groups_numeric, hsc_nums_matrix_ref[:3], maxfev=10000)\n",
    "        exp_func = lambda x: exponential_func(x, *popt_exp)\n",
    "        intersection_20mo = find_intersection(exp_func, hsc_nums_matrix_selected[2], fine_times, method=\"closest\")\n",
    "    except Exception as e:\n",
    "        intersection_20mo = np.nan\n",
    "    age_num_20mo_selected = intersection_20mo\n",
    "\n",
    "    # For merged HSC KDEs, we compute the KL divergence over the entire \"HSCs\" array.\n",
    "    kl_div_3mo_hsc_merged_selected = np.array([\n",
    "        calculate_kl_divergence(kde_results_3mo_selected_normalized[\"HSCs\"], interpolated_kde_hsc_merged[i])\n",
    "        for i in range(len(fine_times))\n",
    "    ])\n",
    "    kl_div_12mo_hsc_merged_selected = np.array([\n",
    "        calculate_kl_divergence(kde_results_12mo_selected_normalized[\"HSCs\"], interpolated_kde_hsc_merged[i])\n",
    "        for i in range(len(fine_times))\n",
    "    ])\n",
    "    kl_div_20mo_hsc_merged_selected = np.array([\n",
    "        calculate_kl_divergence(kde_results_20mo_selected_normalized[\"HSCs\"], interpolated_kde_hsc_merged[i])\n",
    "        for i in range(len(fine_times))\n",
    "    ])\n",
    "    \n",
    "    # Choose the fine time (age) that minimizes KL divergence for merged HSC KDEs:\n",
    "    age_3mo_hsc_merged_selected = fine_times[np.argmin(kl_div_3mo_hsc_merged_selected)]\n",
    "    age_12mo_hsc_merged_selected = fine_times[np.argmin(kl_div_12mo_hsc_merged_selected)]\n",
    "    age_20mo_hsc_merged_selected = fine_times[np.argmin(kl_div_20mo_hsc_merged_selected)]\n",
    "    \n",
    "    \n",
    "    # For hsc cluster composition, we compute the jensenshannon divergence over the entire \"HSCs\" array.\n",
    "    p_3mo_selected = hsc_cluster_comp_3mo_selected.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    p_12mo_selected = hsc_cluster_comp_12mo_selected.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    p_20mo_selected = hsc_cluster_comp_20mo_selected.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    \n",
    "    jsd_3mo_hsc_cluster_comp_selected = np.array([\n",
    "        jensenshannon(p_3mo_selected, q) ** 2 for q in interp_values_hsc_cluster_comp\n",
    "    ])\n",
    "    jsd_12mo_hsc_cluster_comp_selected = np.array([\n",
    "        jensenshannon(p_12mo_selected, q) ** 2 for q in interp_values_hsc_cluster_comp\n",
    "    ])\n",
    "    jsd_20mo_hsc_cluster_comp_selected = np.array([\n",
    "        jensenshannon(p_20mo_selected, q) ** 2 for q in interp_values_hsc_cluster_comp\n",
    "    ])\n",
    "    \n",
    "    # Choose the fine time (age) that minimizes JS divergence for hsc cluster composition:\n",
    "    age_3mo_hsc_cluster_comp_selected = fine_times[np.argmin(jsd_3mo_hsc_cluster_comp_selected)]\n",
    "    age_12mo_hsc_cluster_comp_selected = fine_times[np.argmin(jsd_12mo_hsc_cluster_comp_selected)]\n",
    "    age_20mo_hsc_cluster_comp_selected = fine_times[np.argmin(jsd_20mo_hsc_cluster_comp_selected)]\n",
    "    \n",
    "    \n",
    "    # Loop over clusters (assumed 0 to 9)\n",
    "    for cluster in clusters:\n",
    "        \n",
    "        # --------\n",
    "        # a) cKit KDE-based age estimates using KL divergence\n",
    "        # For 3mo:\n",
    "        kl_div_3mo = np.array([\n",
    "            calculate_kl_divergence(kde_results_3mo_selected_normalized[(\"cKits\", cluster)],\n",
    "                                    interpolated_kde_ckits[cluster][i])\n",
    "            for i in range(len(fine_times))\n",
    "        ])\n",
    "        best_time_3mo_ckit = fine_times[np.argmin(kl_div_3mo)]\n",
    "        age_ckit_3mo_selected.append(best_time_3mo_ckit)\n",
    "        # For 12mo:\n",
    "        kl_div_12mo = np.array([\n",
    "            calculate_kl_divergence(kde_results_12mo_selected_normalized[(\"cKits\", cluster)],\n",
    "                                    interpolated_kde_ckits[cluster][i])\n",
    "            for i in range(len(fine_times))\n",
    "        ])\n",
    "        best_time_12mo_ckit = fine_times[np.argmin(kl_div_12mo)]\n",
    "        age_ckit_12mo_selected.append(best_time_12mo_ckit)\n",
    "        # For 20mo:\n",
    "        kl_div_20mo = np.array([\n",
    "            calculate_kl_divergence(kde_results_20mo_selected_normalized[(\"cKits\", cluster)],\n",
    "                                    interpolated_kde_ckits[cluster][i])\n",
    "            for i in range(len(fine_times))\n",
    "        ])\n",
    "        best_time_20mo_ckit = fine_times[np.argmin(kl_div_20mo)]\n",
    "        age_ckit_20mo_selected.append(best_time_20mo_ckit)\n",
    "\n",
    "        if affinity_flag == \"MSE\":\n",
    "            # --------\n",
    "            # c) cKit affinity-based age estimates using mean squared error (choose minimum distance)\n",
    "            # For 3mo:\n",
    "            mse_3mo = np.array([\n",
    "                calculate_mse(\n",
    "                    affinity_matrices_selected[\"3mo\"].iloc[cluster, :].values,\n",
    "                    interpolated_affinity_matrices[cluster][i]\n",
    "                )\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            best_time_3mo_affinity = fine_times[np.argmin(mse_3mo)]\n",
    "            age_affinity_3mo_selected.append(best_time_3mo_affinity)\n",
    "            # For 12mo:\n",
    "            mse_12mo = np.array([\n",
    "                calculate_mse(\n",
    "                    affinity_matrices_selected[\"12mo\"].iloc[cluster, :].values,\n",
    "                    interpolated_affinity_matrices[cluster][i]\n",
    "                )\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            best_time_12mo_affinity = fine_times[np.argmin(mse_12mo)]\n",
    "            age_affinity_12mo_selected.append(best_time_12mo_affinity)\n",
    "            # For 20mo:\n",
    "            mse_20mo = np.array([\n",
    "                calculate_mse(\n",
    "                    affinity_matrices_selected[\"20mo\"].iloc[cluster, :].values,\n",
    "                    interpolated_affinity_matrices[cluster][i]\n",
    "                )\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            best_time_20mo_affinity = fine_times[np.argmin(mse_20mo)]\n",
    "            age_affinity_20mo_selected.append(best_time_20mo_affinity)\n",
    "            \n",
    "        elif affinity_flag == \"CE\":\n",
    "            # --------\n",
    "            # c) cKit affinity-based age estimates using cross-entropy (choose minimum distance)\n",
    "            # For 3mo:\n",
    "            cross_entropy_3mo = np.array([\n",
    "                calculate_cross_entropy(\n",
    "                    affinity_matrices_selected[\"3mo\"].iloc[cluster, :].values,\n",
    "                    interpolated_affinity_matrices[cluster][i]\n",
    "                )\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            best_time_3mo_affinity = fine_times[np.argmin(cross_entropy_3mo)]\n",
    "            age_affinity_3mo_selected.append(best_time_3mo_affinity)\n",
    "            # For 12mo:\n",
    "            cross_entropy_12mo = np.array([\n",
    "                calculate_cross_entropy(\n",
    "                    affinity_matrices_selected[\"12mo\"].iloc[cluster, :].values,\n",
    "                    interpolated_affinity_matrices[cluster][i]\n",
    "                )\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            best_time_12mo_affinity = fine_times[np.argmin(cross_entropy_12mo)]\n",
    "            age_affinity_12mo_selected.append(best_time_12mo_affinity)\n",
    "            # For 20mo:\n",
    "            cross_entropy_20mo = np.array([\n",
    "                calculate_cross_entropy(\n",
    "                    affinity_matrices_selected[\"20mo\"].iloc[cluster, :].values,\n",
    "                    interpolated_affinity_matrices[cluster][i]\n",
    "                )\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            best_time_20mo_affinity = fine_times[np.argmin(cross_entropy_20mo)]\n",
    "            age_affinity_20mo_selected.append(best_time_20mo_affinity)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid affinity flag. Choose 'MSE' or 'CE'.\") \n",
    "    \n",
    "\n",
    "    # -------------------------------\n",
    "    # 4e. Compute weighted averages for each modality using cluster sizes.\n",
    "\n",
    "    # Compute cluster sizes for each selected dataset using the function calculate_cluster_sizes\n",
    "    cluster_sizes_selected = {}\n",
    "    for transformed_df, condition in zip([selected_3mo_df, selected_12mo_df, selected_20mo_df],\n",
    "                                        [\"3mo_selected\", \"12mo_selected\", \"20mo_selected\"]):\n",
    "        cluster_sizes_selected[condition] = calculate_cluster_sizes(transformed_df)\n",
    "\n",
    "    # Compute cluster sizes for the reference data (for normalization)\n",
    "    cluster_sizes_ref = {}\n",
    "    for transformed_df, condition in zip([ref_3mo_df, ref_12mo_df, ref_20mo_df],\n",
    "                                        [\"3mo_ref\", \"12mo_ref\", \"20mo_ref\"]):\n",
    "        cluster_sizes_ref[condition] = calculate_cluster_sizes(transformed_df)\n",
    "    \n",
    "    # Define the sources and clusters to use for weighted averaging.\n",
    "    sources = [\"cKits\", \"HSCs\"]  # adjust if you use additional sources\n",
    "    clusters_list = list(range(10))\n",
    "    \n",
    "    # Compute weighted cluster sizes for each age condition using compute_weighted_cluster_sizes.\n",
    "    cluster_size_3mo_selected = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                            cluster_sizes=cluster_sizes_selected,\n",
    "                                                            condition=\"3mo_selected\",\n",
    "                                                            sources=sources,\n",
    "                                                            clusters=clusters_list)\n",
    "    cluster_size_12mo_selected = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                            cluster_sizes=cluster_sizes_selected,\n",
    "                                                            condition=\"12mo_selected\",\n",
    "                                                            sources=sources,\n",
    "                                                            clusters=clusters_list)\n",
    "    cluster_size_20mo_selected = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                            cluster_sizes=cluster_sizes_selected,\n",
    "                                                            condition=\"20mo_selected\",\n",
    "                                                            sources=sources,\n",
    "                                                            clusters=clusters_list)\n",
    "    \n",
    "    # Optionally, save the computed cluster sizes for the selected datasets for later inspection.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(epoch_folder, \"cluster_sizes_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_sizes_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"weighted_cluster_size_3mo_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_size_3mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"weighted_cluster_size_12mo_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_size_12mo_selected, f)\n",
    "    with open(os.path.join(epoch_folder, \"weighted_cluster_size_20mo_selected.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cluster_size_20mo_selected, f)\n",
    "    \n",
    "    print(\"Cluster sizes computed for selected datasets in epoch\", epoch_index)\n",
    "    \"\"\"\n",
    "    \n",
    "    # with keys \"cKits\" and \"HSCs\" that map cluster indices (0..9) to normalized weights.\n",
    "    # -------------------------------\n",
    "    # Weighted average for cKit KDE-based age estimates:\n",
    "    average_age_3mo_cKits_selected = np.average(list(age_ckit_3mo_selected), \n",
    "                                                weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "    average_age_12mo_cKits_selected = np.average(list(age_ckit_12mo_selected), \n",
    "                                                weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "    average_age_20mo_cKits_selected = np.average(list(age_ckit_20mo_selected), \n",
    "                                                weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    \n",
    "\n",
    "    # Weighted average for cKit affinity-based estimates:\n",
    "    average_age_3mo_affinity_selected = np.average(list(age_affinity_3mo_selected), \n",
    "                                                weights=list(cluster_size_3mo_selected[\"cKits\"].values()))\n",
    "    average_age_12mo_affinity_selected = np.average(list(age_affinity_12mo_selected), \n",
    "                                                weights=list(cluster_size_12mo_selected[\"cKits\"].values()))\n",
    "    average_age_20mo_affinity_selected = np.average(list(age_affinity_20mo_selected), \n",
    "                                                weights=list(cluster_size_20mo_selected[\"cKits\"].values()))\n",
    "    \n",
    "\n",
    "    # -------------------------------\n",
    "    # Construct the optimal_input dictionary using all five components.\n",
    "    # -------------------------------\n",
    "    optimal_input = {\n",
    "        \"3mo\": [average_age_3mo_cKits_selected, age_3mo_hsc_merged_selected, average_age_3mo_affinity_selected,\n",
    "                age_num_3mo_selected, age_3mo_hsc_cluster_comp_selected],\n",
    "        \"12mo\": [average_age_12mo_cKits_selected, age_12mo_hsc_merged_selected, average_age_12mo_affinity_selected,\n",
    "                age_num_12mo_selected, age_12mo_hsc_cluster_comp_selected],\n",
    "        \"20mo\": [average_age_20mo_cKits_selected, age_20mo_hsc_merged_selected, average_age_20mo_affinity_selected,\n",
    "                age_num_20mo_selected, age_20mo_hsc_cluster_comp_selected]\n",
    "    }\n",
    "    \n",
    "    # Save the optimal_input dictionary for inspection\n",
    "    with open(os.path.join(epoch_folder, f\"optimal_input_NoNumCluster_{affinity_flag}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(optimal_input, f)\n",
    "    \n",
    "    # print(\"Optimal input computed for epoch\", epoch_index)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5. Optimize the weights (5-feature model with L2 regularization)\n",
    "    # -------------------------------\n",
    "    initial_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    bounds = [(0, 1) for _ in range(5)]\n",
    "    constraints = {\"type\": \"eq\", \"fun\": weight_constraint}\n",
    "    res_full = minimize(\n",
    "        total_loss,\n",
    "        initial_weights,\n",
    "        args=(optimal_input, ground_truth, lambda_reg),\n",
    "        method=\"SLSQP\",  # Using SLSQP instead of Nelder-Mead\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={\"disp\": True, \"maxiter\": 100}\n",
    "    )\n",
    "\n",
    "    if res_full.success:\n",
    "        full_weights = res_full.x / np.sum(res_full.x)  # normalize to sum to 1\n",
    "        full_loss = res_full.fun\n",
    "    else:\n",
    "        full_weights = None\n",
    "        full_loss = None\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # 6. Save the results for this epoch in the epoch folder.\n",
    "    # Save the optimized weights (for both 5-feature and 4-feature models) and the loss.\n",
    "    # -------------------------------\n",
    "    weights_dict_full = {\n",
    "        \"cKit_pdf_weight\": full_weights[0] if full_weights is not None else None,\n",
    "        \"HSC_pdf_weight\": full_weights[1] if full_weights is not None else None,\n",
    "        \"cKit_affinity_weight\": full_weights[2] if full_weights is not None else None,\n",
    "        \"HSC_num_weight\": full_weights[3] if full_weights is not None else None,\n",
    "        \"HSC_cluster_comp_weight\": full_weights[4] if full_weights is not None else None,\n",
    "        \"Total_loss\": full_loss\n",
    "    }\n",
    "    weights_df_full = pd.DataFrame(list(weights_dict_full.items()), columns=[\"Variable\", \"Weight\"])\n",
    "    weights_df_full.to_csv(os.path.join(epoch_folder, f\"optimal_input_weights_full_NoNumCluster_{affinity_flag}.csv\"), index=False)\n",
    "\n",
    "    print(f\"Epoch {epoch_index} completed. Full-model weights: {full_weights}\")\n",
    "    \n",
    "    epoch_index += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc94ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the results from all epochs\n",
    "# Load the results from each epoch and combine them into a single DataFrame\n",
    "weights_full_list = []\n",
    "\n",
    "# Get the all the epoch folders under results_dir\n",
    "epoch_folders = [os.path.join(results_dir, folder) for folder in os.listdir(results_dir) if folder.startswith(\"epoch\")]\n",
    "\n",
    "for epoch_folder in epoch_folders:\n",
    "    weights_full_df = pd.read_csv(os.path.join(epoch_folder, f\"optimal_input_weights_full_NoNumCluster_{affinity_flag}.csv\"))\n",
    "    weights_full_list.append(weights_full_df)\n",
    "\n",
    "# Combine the results from all epochs into a single DataFrame\n",
    "weights_full_df = pd.concat(weights_full_list)\n",
    "\n",
    "weights_full_mean = weights_full_df.groupby(\"Variable\")[\"Weight\"].mean()\n",
    "weights_full_std = weights_full_df.groupby(\"Variable\")[\"Weight\"].std()\n",
    "\n",
    "weights_full_mean = weights_full_mean.drop(\"Total_loss\")\n",
    "weights_full_std = weights_full_std.drop(\"Total_loss\")\n",
    "\n",
    "# Normalize the mean/median weights to sum to 1\n",
    "weights_full_mean_normalized = weights_full_mean / weights_full_mean.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb83523",
   "metadata": {},
   "source": [
    "### 5.5 Bone age estimation with linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref data preparation\n",
    "# Load the final optimal weights from the results directory\n",
    "\n",
    "cKit_pdf_weight = weights_full_mean_normalized.loc[\"cKit_pdf_weight\"].values[0]\n",
    "HSC_pdf_weight = weights_full_mean_normalized.loc[\"HSC_pdf_weight\"].values[0]\n",
    "HSC_num_weight = weights_full_mean_normalized.loc[\"HSC_num_weight\"].values[0]\n",
    "HSC_cluster_comp_weight = weights_full_mean_normalized.loc[\"HSC_cluster_comp_weight\"].values[0]\n",
    "cKit_affinity_weight = weights_full_mean_normalized.loc[\"cKit_affinity_weight\"].values[0]\n",
    "\n",
    "# final_weights = np.array([cKit_pdf_weight, HSC_pdf_weight, cKit_affinity_weight, HSC_num_weight, HSC_histogram_weight])\n",
    "final_weights = np.array([cKit_pdf_weight, HSC_pdf_weight, cKit_affinity_weight, HSC_num_weight, HSC_cluster_comp_weight])\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Read the transformed data for reference and test datasets if necessary\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Process the Reference Data: Compute Features and Build Interpolators\n",
    "# ----------------------------\n",
    "\n",
    "# 2a. Compute KDEs for reference\n",
    "\n",
    "kde_results_3mo_ref, ref_grid = kde_for_clusters(transformed_3mo_bones_df, bone_outline=ref_outline, binsize=10)\n",
    "kde_results_12mo_ref, _ = kde_for_clusters(transformed_12mo_bones_df, bone_outline=ref_outline, binsize=10)\n",
    "kde_results_20mo_ref, _ = kde_for_clusters(transformed_20mo_bones_df, bone_outline=ref_outline, binsize=10)\n",
    "\n",
    "# Save the KDE results for reference, required for the visualization later\n",
    "with open(os.path.join(results_dir, \"kde_results_3mo_ref.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kde_results_3mo_ref, f)\n",
    "with open(os.path.join(results_dir, \"kde_results_12mo_ref.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kde_results_12mo_ref, f)\n",
    "with open(os.path.join(results_dir, \"kde_results_20mo_ref.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kde_results_20mo_ref, f)\n",
    "    \n",
    "with open(f\"{results_dir}/ref_grid.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ref_grid, f)\n",
    "\n",
    "# Normalize the KDE dictionaries for reference\n",
    "kde_results_3mo_ref_normalized = normalize_kde_values(kde_results_3mo_ref)\n",
    "kde_results_12mo_ref_normalized = normalize_kde_values(kde_results_12mo_ref)\n",
    "kde_results_20mo_ref_normalized = normalize_kde_values(kde_results_20mo_ref)\n",
    "\n",
    "\n",
    "hsc_cluster_comp_3mo_ref = compute_proportions_hsc(transformed_3mo_bones_df)\n",
    "hsc_cluster_comp_12mo_ref = compute_proportions_hsc(transformed_12mo_bones_df)\n",
    "hsc_cluster_comp_20mo_ref = compute_proportions_hsc(transformed_20mo_bones_df)\n",
    "\n",
    "# Save the HSC cluster compositions for reference as csv files\n",
    "hsc_cluster_comp_3mo_ref.to_csv(os.path.join(results_dir, \"hsc_cluster_comp_3mo_ref.csv\"), index=False)\n",
    "hsc_cluster_comp_12mo_ref.to_csv(os.path.join(results_dir, \"hsc_cluster_comp_12mo_ref.csv\"), index=False)\n",
    "hsc_cluster_comp_20mo_ref.to_csv(os.path.join(results_dir, \"hsc_cluster_comp_20mo_ref.csv\"), index=False)\n",
    "\n",
    "# 2c. Process Affinity Matrices for reference.\n",
    "# Load reference affinity matrices\n",
    "affinity_matrices_ref = {\n",
    "    \"3mo\": affinity_matrices.get(\"3mo\").div(affinity_matrices.get(\"3mo\").sum(axis=1), axis=0),\n",
    "    \"12mo\": affinity_matrices.get(\"12mo\").div(affinity_matrices.get(\"12mo\").sum(axis=1), axis=0),\n",
    "    \"20mo\": affinity_matrices.get(\"20mo\").div(affinity_matrices.get(\"20mo\").sum(axis=1), axis=0)\n",
    "}\n",
    "\n",
    "\n",
    "# 2d. Build Interpolation Dictionaries.\n",
    "# We use the same real times and a fine grid for interpolation.\n",
    "real_times = np.array([3, 12, 20])\n",
    "fine_times = np.linspace(3, 20, 100)\n",
    "\n",
    "# Create dictionaries for each modality:\n",
    "interpolated_kde_ckits = {}        # For cKit KDEs, per cluster\n",
    "interpolated_histograms_hsc = {}     # For HSC histograms, per cluster\n",
    "interpolated_affinity_matrices = {}  # For affinity matrices (row-wise per cluster)\n",
    "interpolated_kde_hsc_merged = None   # For merged HSC KDEs\n",
    "\n",
    "clusters = list(range(10))\n",
    "\n",
    "# 2d.i. Interpolate cKit KDEs (from the normalized reference KDEs)\n",
    "for cluster in clusters:\n",
    "    age_kdes = np.array([\n",
    "        kde_results_3mo_ref_normalized[(\"cKits\", cluster)],\n",
    "        kde_results_12mo_ref_normalized[(\"cKits\", cluster)],\n",
    "        kde_results_20mo_ref_normalized[(\"cKits\", cluster)]\n",
    "    ])\n",
    "    f_kde = interp1d(real_times, age_kdes, axis=0, kind=\"linear\")\n",
    "    # interpolated_kde_ckits[cluster] = f_kde(fine_times)\n",
    "\n",
    "    # Evaluate the interpolator at the fine-grained time points\n",
    "    interp_values_kde = f_kde(fine_times)\n",
    "    # Normalize each interpolated pdf (each row) so that its sum is 1\n",
    "    interp_values_normalized_kde = interp_values_kde / np.sum(interp_values_kde, axis=(1,2), keepdims=True)\n",
    "    # Store the normalized result in the dictionary\n",
    "    interpolated_kde_ckits[cluster] = interp_values_normalized_kde\n",
    "    \n",
    "# 2d.ii. Interpolate HSC cluster compositions\n",
    "age_hsc_cluster_comp = stack_proportions(hsc_cluster_comp_3mo_ref, hsc_cluster_comp_12mo_ref, hsc_cluster_comp_20mo_ref)\n",
    "# Create a linear interpolator for the HSC cluster composition\n",
    "f_hsc_cluster_comp = interp1d(real_times, age_hsc_cluster_comp, axis=1, kind=\"linear\", fill_value=\"extrapolate\") # It is correct here to use axis=1\n",
    "\n",
    "# Step 3 (revised): Interpolate and normalize each timepoint\n",
    "interp_hsc_cluster_comp = f_hsc_cluster_comp(fine_times)  # shape: (100 timepoints, 10 clusters)\n",
    "\n",
    "# Step 4 Normalize to make sure each vector sums to 1\n",
    "interp_hsc_cluster_comp = interp_hsc_cluster_comp.T\n",
    "interp_hsc_cluster_comp = interp_hsc_cluster_comp / interp_hsc_cluster_comp.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# 2d.iii. Interpolate affinity matrices (row-wise, per cluster)\n",
    "for cluster in clusters:\n",
    "    age_affinity = np.array([\n",
    "        affinity_matrices_ref[\"3mo\"].iloc[cluster, :].values,\n",
    "        affinity_matrices_ref[\"12mo\"].iloc[cluster, :].values,\n",
    "        affinity_matrices_ref[\"20mo\"].iloc[cluster, :].values\n",
    "    ])\n",
    "    f_affinity = interp1d(real_times, age_affinity, axis=0, kind=\"linear\")\n",
    "    # interpolated_affinity_matrices[cluster] = f_affinity(fine_times)\n",
    "    \n",
    "    # Evaluate the interpolator at the fine-grained time points\n",
    "    interp_values_affinity = f_affinity(fine_times)\n",
    "    # Normalize each interpolated row so that its sum is 1\n",
    "    interp_values_normalized_affinity = interp_values_affinity / np.sum(interp_values_affinity, axis=1, keepdims=True)\n",
    "    # Store the normalized result in the dictionary\n",
    "    interpolated_affinity_matrices[cluster] = interp_values_normalized_affinity\n",
    "\n",
    "# 2d.iv. Interpolate merged HSC KDEs (for the entire \"HSCs\" key)\n",
    "age_kdes_hsc_merged = np.array([\n",
    "    kde_results_3mo_ref_normalized[\"HSCs\"],\n",
    "    kde_results_12mo_ref_normalized[\"HSCs\"],\n",
    "    kde_results_20mo_ref_normalized[\"HSCs\"]\n",
    "])\n",
    "f_kde_hsc_merged = interp1d(real_times, age_kdes_hsc_merged, axis=0, kind=\"linear\")\n",
    "# interpolated_kde_hsc_merged = f_kde_hsc_merged(fine_times)\n",
    "\n",
    "# Evaluate the interpolator at the fine-grained time points\n",
    "interp_values_kde_hsc_merged = f_kde_hsc_merged(fine_times)\n",
    "# Normalize each interpolated pdf (each row) so that its sum is 1\n",
    "interp_values_normalized_kde_hsc_merged = interp_values_kde_hsc_merged / np.sum(interp_values_kde_hsc_merged, axis=(1,2), keepdims=True)\n",
    "# Store the normalized result in the dictionary\n",
    "interpolated_kde_hsc_merged = interp_values_normalized_kde_hsc_merged\n",
    "\n",
    "# 2d.v. Prepare the HSCs nums matrix for exponential regression\n",
    "age_groups_numeric = np.array([3, 12, 20])  # Numeric representation of age groups\n",
    "conditions_ref = [\"3mo_ref\", \"12mo_ref\", \"20mo_ref\"]\n",
    "\n",
    "hsc_nums_ref = {}\n",
    "# Calculate the number of cells for each condition (ref, without clusters)\n",
    "for transformed_df, condition in zip(\n",
    "    [transformed_3mo_bones_df, transformed_12mo_bones_df, transformed_20mo_bones_df],\n",
    "    conditions_ref,\n",
    "):\n",
    "    hsc_condition = len(transformed_df[transformed_df[\"source\"] == \"HSCs\"])\n",
    "    # Normalize by the number of datasets\n",
    "    num_datasets = len(transformed_df[\"dataset\"].unique())\n",
    "    hsc_condition = hsc_condition / num_datasets\n",
    "    hsc_nums_ref[condition] = hsc_condition\n",
    "hsc_nums_matrix_ref = np.array([hsc_nums_ref[condition] for condition in conditions_ref])  # Convert dictionary to matrix\n",
    "\n",
    "# Save the interpolated data\n",
    "\n",
    "with open(os.path.join(results_dir, \"interpolated_kde_ckits.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(interpolated_kde_ckits, f)\n",
    "with open(os.path.join(results_dir, \"interpolated_affinity_matrices.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(interpolated_affinity_matrices, f)\n",
    "with open(os.path.join(results_dir, \"interpolated_kde_hsc_merged.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(interpolated_kde_hsc_merged, f)\n",
    "with open(os.path.join(results_dir, \"hsc_nums_ref.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(hsc_nums_ref, f)\n",
    "with open(os.path.join(results_dir, \"interpolated_hsc_cluster_comp.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(interp_hsc_cluster_comp, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b88cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cluster weights\n",
    "# Compute cluster sizes for each ref dataset using the function calculate_cluster_sizes\n",
    "cluster_sizes_ref = {}\n",
    "for transformed_df, condition in zip([transformed_3mo_bones_df, transformed_12mo_bones_df, transformed_20mo_bones_df],\n",
    "                                    conditions_ref):\n",
    "    cluster_sizes_ref[condition] = calculate_cluster_sizes(transformed_df)\n",
    "\n",
    "# Compute cluster sizes for the 5fu30d and 5fu60d datasets\n",
    "cluster_sizes_5fu = {}\n",
    "for transformed_df, condition in zip([transformed_5fu30d_bones_df, transformed_5fu60d_bones_df],\n",
    "                                    [\"5fu30d\", \"5fu60d\"]):\n",
    "    cluster_sizes_5fu[condition] = calculate_cluster_sizes(transformed_df)\n",
    "\n",
    "# Save the cluster sizes for reference and test datasets for later inspection.\n",
    "\"\"\"\n",
    "with open(os.path.join(results_dir, \"cluster_sizes_ref.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cluster_sizes_ref, f)\n",
    "with open(os.path.join(results_dir, \"cluster_sizes_5fu.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cluster_sizes_5fu, f)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08594a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Test Data\n",
    "def process_test_data(transformed_test_df, cluster_size_test, condition, ref_outline, save_dir, affinity_flag):\n",
    "    \"\"\"\n",
    "    Process a test dataset (e.g., 5fu30d or 5fu60d) and compute the five component age estimates.\n",
    "    cluster_size_test: dictionary with weighted cluster sizes from reference data,\n",
    "                            used as weights in averaging per-cluster estimates.\n",
    "    Returns:\n",
    "        A list of 5 estimated ages in the order:\n",
    "        [cKit_KDE_est, merged_HSC_KDE_est, cKit_affinity_est, HSC_numbers_est, HSC_histogram_est]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute KDEs for test data and normalize\n",
    "    kde_results_test, _ = kde_for_clusters(transformed_test_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_test_normalized = normalize_kde_values(kde_results_test)\n",
    "    \n",
    "    # Save the kde_results_test when savedir is provided\n",
    "    # if save_dir:\n",
    "    #     with open(os.path.join(save_dir, f\"kde_results_test_{condition}.pkl\"), \"wb\") as f:\n",
    "    #         pickle.dump(kde_results_test_normalized, f)\n",
    "    \n",
    "    # Compute HSC cluster compositions for test data\n",
    "    hsc_cluster_comp_test = compute_proportions_hsc(transformed_test_df)\n",
    "    hsc_cluster_comp_test = hsc_cluster_comp_test.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    \n",
    "    # For affinity, assume that you have a method to compute the affinity matrix for test data.\n",
    "    test_affinity_matrix = affinity_matrices.get(condition)  \n",
    "    \n",
    "    # Normalize the test affinity matrix by row (for MSE and CE)\n",
    "    test_affinity_matrix = test_affinity_matrix.div(test_affinity_matrix.sum(axis=1), axis=0)\n",
    "    \n",
    "\n",
    "    # For HSC numbers, compute counts per cluster for source \"HSCs\"\n",
    "    hsc_conditions = len(transformed_test_df[transformed_test_df[\"source\"] == \"HSCs\"])\n",
    "    hsc_counts = hsc_conditions / len(transformed_test_df[\"dataset\"].unique())\n",
    "    \n",
    "    # Initialize containers for per-cluster estimates for each modality.\n",
    "    age_ckit_estimates = []\n",
    "\n",
    "    age_affinity_estimates = []\n",
    "    age_hsc_num_estimates = {}  # dictionary: key = cluster, value = estimated age\n",
    "    \n",
    "    kl_divs = []\n",
    "    mse_valss = []\n",
    "    ce_valss = []\n",
    "\n",
    "    \n",
    "    # Read the interpolated data if needed\n",
    "    with open(os.path.join(save_dir, \"interpolated_kde_ckits.pkl\"), \"rb\") as f:\n",
    "        interpolated_kde_ckits = pickle.load(f)\n",
    "    with open(os.path.join(save_dir, \"interpolated_hsc_cluster_comp.pkl\"), \"rb\") as f:\n",
    "        interpolated_hsc_cluster_comp = pickle.load(f)\n",
    "    with open(os.path.join(save_dir, \"interpolated_affinity_matrices.pkl\"), \"rb\") as f:\n",
    "        interpolated_affinity_matrices = pickle.load(f)\n",
    "    with open(os.path.join(save_dir, \"interpolated_kde_hsc_merged.pkl\"), \"rb\") as f:\n",
    "        interpolated_kde_hsc_merged = pickle.load(f)\n",
    "\n",
    "    \n",
    "    # Component 1: cKit KDE-based estimates (using KL divergence)\n",
    "    for cluster in clusters:\n",
    "        kl_div = np.array([\n",
    "            calculate_kl_divergence(kde_results_test_normalized[(\"cKits\", cluster)],\n",
    "                                    interpolated_kde_ckits[cluster][i])\n",
    "            for i in range(len(fine_times))\n",
    "        ])\n",
    "        kl_divs.append(kl_div)\n",
    "        best_time = fine_times[np.argmin(kl_div)]\n",
    "        age_ckit_estimates.append(best_time)\n",
    "    avg_ckit_est = np.average(age_ckit_estimates, weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    \n",
    "    # Component 2: Merged HSC KDE-based estimate (using KL divergence)\n",
    "    kl_div_hsc = np.array([\n",
    "        calculate_kl_divergence(kde_results_test_normalized[\"HSCs\"], interpolated_kde_hsc_merged[i])\n",
    "        for i in range(len(fine_times))\n",
    "    ])\n",
    "    avg_hsc_merged_est = fine_times[np.argmin(kl_div_hsc)]\n",
    "\n",
    "    if affinity_flag == \"MSE\":\n",
    "        # Component 3: cKit affinity-based estimates (using mean square error)\n",
    "        for cluster in clusters:\n",
    "            mse_vals = np.array([\n",
    "                calculate_mse(test_affinity_matrix.iloc[cluster, :].values,\n",
    "                            interpolated_affinity_matrices[cluster][i])\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            mse_valss.append(mse_vals)\n",
    "            best_time_aff = fine_times[np.argmin(mse_vals)]\n",
    "            age_affinity_estimates.append(best_time_aff)\n",
    "        avg_affinity_est = np.average(age_affinity_estimates, weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    elif affinity_flag == \"CE\":\n",
    "        # Component 3: cKit affinity-based estimates (using cross entropy)\n",
    "        for cluster in clusters:\n",
    "            ce_vals = np.array([\n",
    "                calculate_cross_entropy(test_affinity_matrix.iloc[cluster, :].values,\n",
    "                                        interpolated_affinity_matrices[cluster][i])\n",
    "                for i in range(len(fine_times))\n",
    "            ])\n",
    "            ce_valss.append(ce_vals)\n",
    "            best_time_aff = fine_times[np.argmin(ce_vals)]\n",
    "            age_affinity_estimates.append(best_time_aff)\n",
    "        avg_affinity_est = np.average(age_affinity_estimates, weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid affinity_flag. Choose 'MSE' or 'CE'.\")\n",
    "\n",
    "    # Component 4: HSC numbers-based estimates (using exponential regression, without clusters)\n",
    "    try:\n",
    "        popt_exp, _ = curve_fit(exponential_func, age_groups_numeric, hsc_nums_matrix_ref[:3], maxfev=10000)\n",
    "        exp_func = lambda x: exponential_func(x, *popt_exp)\n",
    "        intersection_age = find_intersection(exp_func, hsc_counts, fine_times, method=\"closest\")\n",
    "    except Exception as e:\n",
    "        intersection_age = np.nan\n",
    "    age_hsc_num_estimates = intersection_age\n",
    "    avg_hsc_num_est = age_hsc_num_estimates\n",
    "\n",
    "    # Component 5: HSC cluster compositon based estimates (using Jensen-Shannon divergence)\n",
    "    jsd_hsc = np.array([\n",
    "        jensenshannon(hsc_cluster_comp_test, q) ** 2 for q in interpolated_hsc_cluster_comp\n",
    "    ])\n",
    "    avg_hsc_cluster_comp = fine_times[np.argmin(jsd_hsc)]\n",
    "    \n",
    "    # Save the results\n",
    "    if affinity_flag == \"MSE\":\n",
    "        with open(os.path.join(save_dir, f\"{datetime.today().strftime(\"%y%m%d\")}_elements_dist_{condition}_NoNumCluster_{affinity_flag}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"cKit_KL\": kl_divs,\n",
    "                \"merged_HSC_KL\": kl_div_hsc,\n",
    "                \"cKit_MSE\": mse_valss,\n",
    "                \"HSC_cluster_comp_JSD\": jsd_hsc,\n",
    "            }, f)\n",
    "    elif affinity_flag == \"CE\":\n",
    "        with open(os.path.join(save_dir, f\"{datetime.today().strftime(\"%y%m%d\")}_elements_dist_{condition}_NoNumCluster_{affinity_flag}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"cKit_KL\": kl_divs,\n",
    "                \"merged_HSC_KL\": kl_div_hsc,\n",
    "                \"cKit_CE\": ce_valss,\n",
    "                \"HSC_cluster_comp_JSD\": jsd_hsc,\n",
    "            }, f)\n",
    "    \n",
    "    # Save the estimated age list before averaging\n",
    "    with open(os.path.join(save_dir, f\"{datetime.today().strftime(\"%y%m%d\")}_estimated_ages_{condition}_NoNumCluster_{affinity_flag}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"cKit_KDE\": age_ckit_estimates,\n",
    "            \"merged_HSC_KDE\": avg_hsc_merged_est,\n",
    "            \"cKit_affinity\": age_affinity_estimates,\n",
    "            \"HSC_numbers\": age_hsc_num_estimates,\n",
    "            \"HSC_cluster_comp\": avg_hsc_cluster_comp,\n",
    "        }, f)\n",
    "    \n",
    "    # Return the list of 5 component age estimates in order:\n",
    "    return [avg_ckit_est, avg_hsc_merged_est, avg_affinity_est, avg_hsc_num_est, avg_hsc_cluster_comp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "\n",
    "# Define the sources and clusters to use for weighted averaging.\n",
    "sources = [\"cKits\", \"HSCs\"]  # adjust if you use additional sources\n",
    "clusters_list = list(range(10))\n",
    "\n",
    "\n",
    "# Compute weighted cluster sizes for each age condition using compute_weighted_cluster_sizes.\n",
    "\n",
    "cluster_size_5fu30d = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                        cluster_sizes=cluster_sizes_5fu,\n",
    "                                                        condition=\"5fu30d\",\n",
    "                                                        sources=sources,\n",
    "                                                        clusters=clusters_list)\n",
    "cluster_size_5fu60d = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                        cluster_sizes=cluster_sizes_5fu,\n",
    "                                                        condition=\"5fu60d\",\n",
    "                                                        sources=sources,\n",
    "                                                        clusters=clusters_list)\n",
    "\n",
    "\n",
    "\n",
    "optimal_input_5fu30d = process_test_data(transformed_5fu30d_bones_df, cluster_size_5fu30d, \"5fu30d\", ref_outline, results_dir, affinity_flag)\n",
    "optimal_input_5fu60d = process_test_data(transformed_5fu60d_bones_df, cluster_size_5fu60d, \"5fu60d\", ref_outline, results_dir, affinity_flag)\n",
    "\n",
    "# ----------------------------\n",
    "# Use the final optimal weights to estimate the final age for each test dataset.\n",
    "# The final estimated age is the dot product of the 5-element optimal input and final_weights.\n",
    "# ----------------------------\n",
    "final_age_5fu30d = np.dot(final_weights, optimal_input_5fu30d)\n",
    "final_age_5fu60d = np.dot(final_weights, optimal_input_5fu60d)\n",
    "\n",
    "# ----------------------------\n",
    "# Save the final estimated ages to a CSV file\n",
    "# ----------------------------\n",
    "final_estimates = pd.DataFrame({\n",
    "    \"Condition\": [\"5fu30d\", \"5fu60d\"],\n",
    "    \"Estimated_Age\": [final_age_5fu30d, final_age_5fu60d]\n",
    "})\n",
    "# final_estimates.to_csv(os.path.join(results_dir, f\"final_age_estimates_5fu_avg_{weight_flag}_NoNumCluster_{affinity_flag}.csv\"), index=False)\n",
    "\n",
    "print(\"Final estimated ages for 5fu treated bones computed:\")\n",
    "print(final_estimates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f80dbb",
   "metadata": {},
   "source": [
    "### 5.6 Bone age estimation with Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ee01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features of test data for model inference\n",
    "def generate_features(\n",
    "    transformed_test_df, cluster_size_test, condition, affinity_matrix, ref_outline, save_dir,\n",
    "    kde_results_ref, hsc_cluster_comp_ref, affinity_matrices_ref, clusters\n",
    "):\n",
    "    \"\"\"\n",
    "    Process test data and compute features for model inference.\n",
    "\n",
    "    Args:\n",
    "        transformed_test_df (DataFrame): Preprocessed test data.\n",
    "        cluster_size_test (dict): Cluster sizes for test condition.\n",
    "        condition (str): Condition name (e.g., \"test_condition\").\n",
    "        affinity_matrix (DataFrame): Affinity matrix for test data.\n",
    "        ref_outline (array): Reference bone outline for KDE calculation.\n",
    "        save_dir (str): Directory to save intermediate results.\n",
    "        kde_results_ref (dict): Precomputed reference KDEs (normalized).\n",
    "        histogram_results_ref (dict): Precomputed reference histograms.\n",
    "        affinity_matrices_ref (dict): Precomputed reference affinity matrices.\n",
    "        clusters (list): List of clusters.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing computed feature values for the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute KDEs for test data and normalize\n",
    "    \n",
    "    kde_results_test, _ = kde_for_clusters(transformed_test_df, bone_outline=ref_outline, binsize=10)\n",
    "    kde_results_test_normalized = normalize_kde_values(kde_results_test)\n",
    "\n",
    "    # Save KDE results if save_dir is provided\n",
    "    \"\"\"\n",
    "    if save_dir:\n",
    "        with open(os.path.join(save_dir, f\"kde_results_test_{condition}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(kde_results_test_normalized, f)\n",
    "    \"\"\"      \n",
    "\n",
    "    # Step 2: Compute HSC Cluster Composition for test data\n",
    "    hsc_cluster_comp_test = compute_proportions_hsc(transformed_test_df)\n",
    "    hsc_cluster_comp_test = hsc_cluster_comp_test.sort_values(\"clusters\")[\"proportion\"].values\n",
    "    \n",
    "    \n",
    "    # Step 3: Compute KDE-based Features (KL Divergence) - For cKit and HSCs\n",
    "    cKit_test_3mo_kl_divs = {}\n",
    "    cKit_test_12mo_kl_divs = {}\n",
    "    cKit_test_20mo_kl_divs = {}\n",
    "\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cKit_test_3mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_test_normalized[(\"cKits\", cluster)], kde_results_ref[\"3mo\"][(\"cKits\", cluster)])\n",
    "        cKit_test_12mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_test_normalized[(\"cKits\", cluster)], kde_results_ref[\"12mo\"][(\"cKits\", cluster)])\n",
    "        cKit_test_20mo_kl_divs[cluster] = calculate_kl_divergence(kde_results_test_normalized[(\"cKits\", cluster)], kde_results_ref[\"20mo\"][(\"cKits\", cluster)])\n",
    "\n",
    "    HSC_test_3mo_kl_div = calculate_kl_divergence(kde_results_test_normalized[\"HSCs\"], kde_results_ref[\"3mo\"][\"HSCs\"])\n",
    "    HSC_test_12mo_kl_div = calculate_kl_divergence(kde_results_test_normalized[\"HSCs\"], kde_results_ref[\"12mo\"][\"HSCs\"])\n",
    "    HSC_test_20mo_kl_div = calculate_kl_divergence(kde_results_test_normalized[\"HSCs\"], kde_results_ref[\"20mo\"][\"HSCs\"])\n",
    "\n",
    "    # Step 4: Compute HSC Cluster Composition\n",
    "    HSC_test_3mo_cluster_comp = jensenshannon(hsc_cluster_comp_test, hsc_cluster_comp_ref[\"3mo\"]) **2\n",
    "    HSC_test_12mo_cluster_comp = jensenshannon(hsc_cluster_comp_test, hsc_cluster_comp_ref[\"12mo\"]) **2\n",
    "    HSC_test_20mo_cluster_comp = jensenshannon(hsc_cluster_comp_test, hsc_cluster_comp_ref[\"20mo\"]) **2\n",
    "\n",
    "    \n",
    "    # Step 5: Compute HSC Numbers\n",
    "    HSC_test_num = transformed_test_df[transformed_test_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "    HSC_test_num = HSC_test_num / len(transformed_test_df[\"dataset\"].unique())\n",
    "\n",
    "    # Step 6: Compute Affinity-based Features (Cross Entropy)\n",
    "    cKit_test_3mo_ces = {}\n",
    "    cKit_test_12mo_ces = {}\n",
    "    cKit_test_20mo_ces = {}\n",
    "\n",
    "    test_affinity_matrix = affinity_matrix\n",
    "    test_affinity_matrix = test_affinity_matrix.div(test_affinity_matrix.sum(axis=1), axis=0)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cKit_test_3mo_ces[cluster] = calculate_cross_entropy(test_affinity_matrix.iloc[cluster, :], affinity_matrices_ref[\"3mo\"].iloc[cluster, :])\n",
    "        cKit_test_12mo_ces[cluster] = calculate_cross_entropy(test_affinity_matrix.iloc[cluster, :], affinity_matrices_ref[\"12mo\"].iloc[cluster, :])\n",
    "        cKit_test_20mo_ces[cluster] = calculate_cross_entropy(test_affinity_matrix.iloc[cluster, :], affinity_matrices_ref[\"20mo\"].iloc[cluster, :])\n",
    "\n",
    "    # Step 7: Weighted Averaging of Features\n",
    "    cKit_3mo_kl_div_avg = np.average(list(cKit_test_3mo_kl_divs.values()), weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    cKit_12mo_kl_div_avg = np.average(list(cKit_test_12mo_kl_divs.values()), weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    cKit_20mo_kl_div_avg = np.average(list(cKit_test_20mo_kl_divs.values()), weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    cKit_3mo_ce_avg = np.average(list(cKit_test_3mo_ces.values()), weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    cKit_12mo_ce_avg = np.average(list(cKit_test_12mo_ces.values()), weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    cKit_20mo_ce_avg = np.average(list(cKit_test_20mo_ces.values()), weights=list(cluster_size_test[\"cKits\"].values()))\n",
    "    \n",
    "    # Step 8: Return the computed features as a dictionary\n",
    "    \n",
    "    test_features = {\n",
    "        \"cKit Density Divergence (vs. 3mo)\": cKit_3mo_kl_div_avg,\n",
    "        \"cKit Density Divergence (vs. 12mo)\": cKit_12mo_kl_div_avg,\n",
    "        \"cKit Density Divergence (vs. 20mo)\": cKit_20mo_kl_div_avg,\n",
    "        \"HSC Density Divergence (vs. 3mo)\": HSC_test_3mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 12mo)\": HSC_test_12mo_kl_div,\n",
    "        \"HSC Density Divergence (vs. 20mo)\": HSC_test_20mo_kl_div,\n",
    "        \"HSC Count\": HSC_test_num,\n",
    "        \"HSC Composition (vs. 3mo)\": HSC_test_3mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 12mo)\": HSC_test_12mo_cluster_comp,\n",
    "        \"HSC Composition (vs. 20mo)\": HSC_test_20mo_cluster_comp,\n",
    "        \"cKit Neighborhood Affinity (vs. 3mo)\": cKit_3mo_ce_avg,\n",
    "        \"cKit Neighborhood Affinity (vs. 12mo)\": cKit_12mo_ce_avg,\n",
    "        \"cKit Neighborhood Affinity (vs. 20mo)\": cKit_20mo_ce_avg\n",
    "    }\n",
    "\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sources and clusters to use for weighted averaging.\n",
    "sources = [\"cKits\", \"HSCs\"]  # adjust if you use additional sources\n",
    "clusters_list = list(range(10))\n",
    "\n",
    "# Compute weighted cluster sizes for each age condition using compute_weighted_cluster_sizes.\n",
    "\n",
    "cluster_size_5fu30d = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                        cluster_sizes=cluster_sizes_5fu,\n",
    "                                                        condition=\"5fu30d\",\n",
    "                                                        sources=sources,\n",
    "                                                        clusters=clusters_list)\n",
    "cluster_size_5fu60d = compute_weighted_cluster_sizes(ref_cluster_sizes=cluster_sizes_ref,\n",
    "                                                        cluster_sizes=cluster_sizes_5fu,\n",
    "                                                        condition=\"5fu60d\",\n",
    "                                                        sources=sources,\n",
    "                                                        clusters=clusters_list)\n",
    "\n",
    "\n",
    "affinity_matrix_5fu30d = affinity_matrices.get(\"5fu30d\")\n",
    "affinity_matrix_5fu60d = affinity_matrices.get(\"5fu60d\")\n",
    "\n",
    "kde_results_ref = {\n",
    "    \"3mo\": normalize_kde_values(kde_results_3mo_ref),\n",
    "    \"12mo\": normalize_kde_values(kde_results_12mo_ref),\n",
    "    \"20mo\": normalize_kde_values(kde_results_20mo_ref),\n",
    "}\n",
    "\n",
    "hsc_cluster_comp_ref = {\n",
    "    \"3mo\": hsc_cluster_comp_3mo_ref[\"proportion\"].values,\n",
    "    \"12mo\": hsc_cluster_comp_12mo_ref[\"proportion\"].values,\n",
    "    \"20mo\": hsc_cluster_comp_20mo_ref[\"proportion\"].values\n",
    "}\n",
    "\n",
    "\n",
    "affinity_matrices_ref = {\n",
    "    \"3mo\": affinity_matrices.get(\"3mo\").div(affinity_matrices.get(\"3mo\").sum(axis=1), axis=0),\n",
    "    \"12mo\": affinity_matrices.get(\"12mo\").div(affinity_matrices.get(\"12mo\").sum(axis=1), axis=0),\n",
    "    \"20mo\": affinity_matrices.get(\"20mo\").div(affinity_matrices.get(\"20mo\").sum(axis=1), axis=0)\n",
    "}\n",
    "\n",
    "\n",
    "features_5fu30d = generate_features(transformed_5fu30d_bones_df, cluster_size_5fu30d, \"5fu30d\", affinity_matrix_5fu30d, ref_outline, results_dir, kde_results_ref, hsc_cluster_comp_ref, affinity_matrices_ref, clusters_list)\n",
    "features_5fu60d = generate_features(transformed_5fu60d_bones_df, cluster_size_5fu60d, \"5fu60d\", affinity_matrix_5fu60d, ref_outline, results_dir, kde_results_ref, hsc_cluster_comp_ref, affinity_matrices_ref, clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-feature columns (we keep only feature columns)\n",
    "X = training_data_df.drop(columns=[\"epoch\", \"ground_truth\"])\n",
    "y = training_data_df[\"ground_truth\"]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_3mo = X_scaled[training_data_df[\"ground_truth\"] == 3]\n",
    "X_scaled_12mo = X_scaled[training_data_df[\"ground_truth\"] == 12]\n",
    "X_scaled_20mo = X_scaled[training_data_df[\"ground_truth\"] == 20]\n",
    "\n",
    "# Define cross-validation strategy\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Read models to compare\n",
    "\n",
    "models = {\n",
    "    \"Gaussian Process\": GaussianProcessRegressor(kernel=C(3.16**2) * RBF(length_scale=3.85), alpha=0.2, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results for metrics\n",
    "metrics = {model: {} for model in models}\n",
    "\n",
    "# Perform cross-validation and compute metrics\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X_scaled, y, cv=kf)\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    evs = explained_variance_score(y, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    metrics[name] = {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R Score\": r2,\n",
    "        \"Explained Variance\": evs\n",
    "    }\n",
    "\n",
    "# Convert metrics to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "\n",
    "# Print results\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208837fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model on the full dataset and save it\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_scaled, y)  # Train on the full dataset\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(model_save_dir, f\"model.pkl\")\n",
    "    joblib.dump(model, model_path) # don\"t save the model again since we already saved it\n",
    "    print(f\"Saved {name} model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance using permutation importance\n",
    "gpr_model = trained_models[\"Gaussian Process\"]\n",
    "perm_importance = permutation_importance(gpr_model, X_scaled, y, n_repeats=1000, random_state=42)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": perm_importance.importances_mean\n",
    "}).sort_values(by=\"Importance\", ascending=False)  # Sort by importance\n",
    "\n",
    "# Print the permutation importance results\n",
    "print(\"\\n Feature Importance Based on Permutation Importance:\")\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Highlight the most and least important features\n",
    "print(\"\\n Most Important Features (Highest Impact on Model):\")\n",
    "print(perm_importance_df.head())\n",
    "\n",
    "print(\"\\n Least Important Features (Lowest Impact on Model):\")\n",
    "print(perm_importance_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories by removing age comparisons\n",
    "def categorize_feature(feature):\n",
    "    if \"HSC Count\" in feature:\n",
    "        return \"HSC numbers\"\n",
    "    elif \"cKit Density Divergence\" in feature:\n",
    "        return \"HeM distribution\"\n",
    "    elif \"HSC Density Divergence\" in feature:\n",
    "        return \"HSC distribution\"\n",
    "    elif \"HSC Spatial Similarity\" in feature:\n",
    "        return \"HSC-HeM association\"\n",
    "    elif \"cKit Neighborhood Affinity\" in feature:\n",
    "        return \"HeM neighborhood\"\n",
    "    elif \"HSC Composition\" in feature:\n",
    "        return \"HSC composition\"\n",
    "    else:\n",
    "        return feature  # Keep the original name if no match\n",
    "\n",
    "# Apply categorization to the features\n",
    "perm_importance_df[\"Feature Category\"] = perm_importance_df[\"Feature\"].apply(categorize_feature)\n",
    "\n",
    "# Group by Feature Category and sum the importance\n",
    "perm_importance_grouped = (\n",
    "    perm_importance_df.groupby(\"Feature Category\")[\"Importance\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"Importance\", ascending=True)\n",
    ")\n",
    "\n",
    "# Normalize it before visualization\n",
    "perm_importance_grouped[\"Importance\"] /= perm_importance_grouped[\"Importance\"].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16508e11",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207023e8",
   "metadata": {},
   "source": [
    "### 6.1 Element-wise bone age visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9408881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plot function (KL div, SSIM, CE, MSE) for elements\n",
    "def plot_lineplot(data ,method, num_clusters, labels, y_label, results_dir, colors):\n",
    "    # Create a figure and set of subplots\n",
    "    fig, axes = plt.subplots(nrows=num_clusters, ncols=1, figsize=(8, 1 * num_clusters), sharex=True)\n",
    "    # If there\"s only one cluster, axes is not an array, so we handle that case\n",
    "    if num_clusters == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Plot each cluster as a ridgeline plot\n",
    "    for i, ax in enumerate(axes):\n",
    "        for j in range(len(labels)):\n",
    "            sns.lineplot(x=range(0, 100), y=data[j][i], ax=ax, label=labels[j] if i == 0 else \"\", c=colors[j])\n",
    "            if method == \"min\":\n",
    "                # selected_kl_div = np.min(data[j][i])\n",
    "                # selected_kl_div_index = np.argmin(data[j][i])\n",
    "                # Ignore the NaN values\n",
    "                selected_kl_div = np.nanmin(data[j][i])\n",
    "                selected_kl_div_index = np.nanargmin(data[j][i])\n",
    "            elif method == \"max\":\n",
    "                # selected_kl_div = np.max(data[j][i])\n",
    "                # selected_kl_div_index = np.argmax(data[j][i])\n",
    "                # Ignore the NaN values\n",
    "                selected_kl_div = np.nanmax(data[j][i])\n",
    "                selected_kl_div_index = np.nanargmax(data[j][i])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid method. Please choose either 'min' or 'max'\")\n",
    "            sns.lineplot(x=[selected_kl_div_index], y=[selected_kl_div], marker=\"o\", markersize=5, ax=ax, c=colors[j])\n",
    "\n",
    "        ax.set_ylabel(f\"Cluster {i + 1}\", fontsize=12)\n",
    "        ax.set_xlim([-2, 101])\n",
    "        ax.set_xticks([0, 52.4, 99])\n",
    "        ax.set_xticklabels([3, 12, 20], fontsize=12)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        # ax.spines[\"left\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.tick_params(bottom=False) #left=False, \n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)  # Change tick label size\n",
    "\n",
    "\n",
    "\n",
    "    axes[-1].spines[\"bottom\"].set_visible(True)\n",
    "    axes[-1].tick_params(bottom=True)\n",
    "\n",
    "    # Set common labels\n",
    "    fig.text(0.5, 0.01, \"Estimated Age (months)\", ha=\"center\", va=\"center\")\n",
    "    fig.text(0, 0.5, y_label, ha=\"center\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "    # Add title\n",
    "    fig.suptitle(f\"{y_label}\", fontsize=16)\n",
    "\n",
    "    # Add legend\n",
    "    axes[0].legend(bbox_to_anchor=(1.5, 1), loc=\"upper right\", fontsize=12)\n",
    "    # Adjust layout to fit titles and labels and bring subplots closer\n",
    "    plt.subplots_adjust(hspace=0.5, left=0.12, right=0.95, top=0.95, bottom=0.05)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Precalculated when generating the reference data)\n",
    "with open(f\"{results_dir}/{datetime.today().strftime(\"%y%m%d\")}_elements_dist_5fu30d_NoNumCluster_{affinity_flag}.pkl\", \"rb\") as f:\n",
    "    elements_dist_5fu30d = pickle.load(f)\n",
    "with open(f\"{results_dir}/{datetime.today().strftime(\"%y%m%d\")}_elements_dist_5fu60d_NoNumCluster_{affinity_flag}.pkl\", \"rb\") as f:\n",
    "    elements_dist_5fu60d = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For merged HSC KL divergence    \n",
    "merged_HSC_kl_5fu30d = elements_dist_5fu30d[\"merged_HSC_KL\"]\n",
    "merged_HSC_kl_5fu60d = elements_dist_5fu60d[\"merged_HSC_KL\"]\n",
    "\n",
    "# Plot the KL divergence as a ridgeline plot for HSCs\n",
    "labels = [\"HSC KL div 5fu30d\", \"HSC KL div 5fu60d\"]\n",
    "colors = [\"red\", \"blue\"]\n",
    "y_label = \"KL Divergence\"\n",
    "num_clusters = 1\n",
    "method = \"min\"\n",
    "\n",
    "# Create a figure and set of subplots\n",
    "fig, axes = plt.subplots(nrows=num_clusters, ncols=1, figsize=(8, 1 * num_clusters), sharex=True)\n",
    "# If there\"s only one cluster, axes is not an array, so we handle that case\n",
    "if num_clusters == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot each cluster as a ridgeline plot\n",
    "for i, ax in enumerate(axes):\n",
    "\n",
    "    # For 5fu30d\n",
    "    sns.lineplot(x=range(0, 100), y=merged_HSC_kl_5fu30d, ax=ax, label=labels[0], c=colors[0])\n",
    "    selected_kl_div = np.min(merged_HSC_kl_5fu30d)\n",
    "    selected_kl_div_index = np.argmin(merged_HSC_kl_5fu30d)\n",
    "    sns.lineplot(x=[selected_kl_div_index], y=[selected_kl_div], marker=\"o\", markersize=5, ax=ax, c=colors[0])\n",
    "    \n",
    "    # For 5fu60d\n",
    "    sns.lineplot(x=range(0, 100), y=merged_HSC_kl_5fu60d, ax=ax, label=labels[1], c=colors[1])\n",
    "    selected_kl_div = np.min(merged_HSC_kl_5fu60d)\n",
    "    selected_kl_div_index = np.argmin(merged_HSC_kl_5fu60d)\n",
    "    sns.lineplot(x=[selected_kl_div_index], y=[selected_kl_div], marker=\"o\", markersize=5, ax=ax, c=colors[1])\n",
    "    \n",
    "\n",
    "    #ax.set_ylabel(f\"Cluster {i + 1}\", fontsize=12)\n",
    "    ax.set_xlim([0, 99])\n",
    "    ax.set_xticks([0, 52.4, 99])\n",
    "    ax.set_xticklabels([3, 12, 20], fontsize=12)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    # ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.tick_params(bottom=False) #left=False, \n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)  # Change tick label size\n",
    "\n",
    "\n",
    "\n",
    "axes[-1].spines[\"bottom\"].set_visible(True)\n",
    "axes[-1].tick_params(bottom=True)\n",
    "\n",
    "# Set common labels\n",
    "fig.text(0.5, -0.5, \"Estimated Age (months)\", ha=\"center\", va=\"center\")\n",
    "fig.text(0, 0.5, y_label, ha=\"center\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "# Add title\n",
    "fig.suptitle(f\"{y_label} \", fontsize=16, y=1.2)\n",
    "\n",
    "# Add legend\n",
    "axes[0].legend(bbox_to_anchor=(1.5, 1), loc=\"upper right\", fontsize=12)\n",
    "# Adjust layout to fit titles and labels and bring subplots closer\n",
    "plt.subplots_adjust(hspace=0.5, left=0.12, right=0.95, top=0.95, bottom=0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f401da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-grained x-values for interpolation\n",
    "fine_ages = np.linspace(3, 20, 100)\n",
    "# For HSC numbers\n",
    "HSC_number_3mo = transformed_3mo_bones_df[transformed_3mo_bones_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "HSC_number_12mo = transformed_12mo_bones_df[transformed_12mo_bones_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "HSC_number_20mo = transformed_20mo_bones_df[transformed_20mo_bones_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "\n",
    "HSC_number_5fu30d = transformed_5fu30d_bones_df[transformed_5fu30d_bones_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "HSC_number_5fu60d = transformed_5fu60d_bones_df[transformed_5fu60d_bones_df[\"source\"] == \"HSCs\"].shape[0]\n",
    "\n",
    "# Plot the exponential regression based on the HSC numbers\n",
    "# Apply the exponential regression to the HSCs data without the cluster\n",
    "hsc_nums_matrix_all = np.array([HSC_number_3mo, HSC_number_12mo, HSC_number_20mo, HSC_number_5fu30d, HSC_number_5fu60d])\n",
    "\n",
    "# Exponential regression\n",
    "try:\n",
    "    popt_exp_all, _ = curve_fit(exponential_func, age_groups_numeric, hsc_nums_matrix_all[:3], maxfev=10000)\n",
    "    exponential_regression_results_all = exponential_func(fine_ages, *popt_exp_all)\n",
    "except RuntimeError:\n",
    "    # If fitting fails, use a fallback\n",
    "    exponential_regression_results_all = np.interp(fine_ages, age_groups_numeric, hsc_nums_matrix_all[:3])\n",
    "\n",
    "# Plotting results\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Scatter plot of the HSC numbers\n",
    "ax.scatter(\n",
    "    age_groups_numeric,\n",
    "    hsc_nums_matrix_all[:3],\n",
    "    label=f\"HSCs number\",\n",
    "    color=\"black\"\n",
    ")\n",
    "\n",
    "# Exponential regression\n",
    "ax.plot(\n",
    "    fine_ages, \n",
    "    exponential_regression_results_all, \n",
    "    label=f\"Exponential Regression\", \n",
    "    linestyle=\"-\", color=\"black\", alpha=0.7\n",
    ")\n",
    "\n",
    "# Horizontal lines for 30d and 60d\n",
    "ax.axhline(y=hsc_nums_matrix_all[3], color=\"red\", linestyle=\"-\", label=\"HSCs number of 5fu30d\")\n",
    "ax.axhline(y=hsc_nums_matrix_all[4], color=\"blue\", linestyle=\"-\", label=\"HSCs number of 5fu60d\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"HSCs number\")\n",
    "\n",
    "# Set the x ticks and labels\n",
    "ax.set_xticks([3, 12, 20])\n",
    "ax.set_xticklabels([\"3\", \"12\", \"20\"])\n",
    "\n",
    "# Set shared x-axis label\n",
    "plt.xlabel(\"Estimated Age (Months)\")\n",
    "plt.ylabel(\"Number of HSCs\")\n",
    "\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.5, 1), loc=\"upper right\", fontsize=12)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ba2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the HSCs number per condition with barplot and error bars\n",
    "# For each condition, we show the average HSC number and the standard deviation by source\n",
    "# Prepare the data for plotting\n",
    "# HSC numbers for each condition\n",
    "hsc_numbers_per_condition_3mo = transformed_3mo_bones_df[transformed_3mo_bones_df[\"source\"] == \"HSCs\"].groupby(\"dataset\").size()\n",
    "hsc_numbers_per_condition_12mo = transformed_12mo_bones_df[transformed_12mo_bones_df[\"source\"] == \"HSCs\"].groupby(\"dataset\").size()\n",
    "hsc_numbers_per_condition_20mo = transformed_20mo_bones_df[transformed_20mo_bones_df[\"source\"] == \"HSCs\"].groupby(\"dataset\").size()\n",
    "hsc_numbers_per_condition_5fu30d = transformed_5fu30d_bones_df[transformed_5fu30d_bones_df[\"source\"] == \"HSCs\"].groupby(\"dataset\").size()\n",
    "hsc_numbers_per_condition_5fu60d = transformed_5fu60d_bones_df[transformed_5fu60d_bones_df[\"source\"] == \"HSCs\"].groupby(\"dataset\").size()\n",
    "\n",
    "hsc_numbers_per_condition = {\n",
    "    \"3mo\": hsc_numbers_per_condition_3mo,\n",
    "    \"12mo\": hsc_numbers_per_condition_12mo,\n",
    "    \"20mo\": hsc_numbers_per_condition_20mo,\n",
    "    \"5fu30d\": hsc_numbers_per_condition_5fu30d,\n",
    "    \"5fu60d\": hsc_numbers_per_condition_5fu60d\n",
    "}\n",
    "\n",
    "# Calculate the mean and standard deviation for each condition\n",
    "hsc_numbers_mean = {key: value.mean() for key, value in hsc_numbers_per_condition.items()}\n",
    "hsc_numbers_std = {key: value.std() for key, value in hsc_numbers_per_condition.items()}\n",
    "\n",
    "# Plot the HSC numbers per condition with barplot and error bars\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.bar(hsc_numbers_mean.keys(), hsc_numbers_mean.values(), yerr=hsc_numbers_std.values(), capsize=5, color=\"skyblue\")\n",
    "# show the dots for each condition\n",
    "for i, key in enumerate(hsc_numbers_per_condition.keys()):\n",
    "    y = hsc_numbers_per_condition[key]\n",
    "    x = np.random.normal(i, 0.05, size=len(y))\n",
    "    ax.plot(x, y, \"o\", color=\"black\", alpha=1, markersize=5)\n",
    "ax.set_title(\"HSC Numbers per Condition\")\n",
    "ax.set_ylabel(\"HSC Number\")\n",
    "ax.set_xlabel(\"Condition\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cKit KL divergence\n",
    "cKit_KL_5fu30d = elements_dist_5fu30d[\"cKit_KL\"]\n",
    "cKit_KL_5fu60d = elements_dist_5fu60d[\"cKit_KL\"]\n",
    "\n",
    "# Define the labels and colors for the lineplot\n",
    "labels = [\"cKit KL div 5fu30d\", \"cKit KL div 5fu60d\"]\n",
    "colors = [\"red\", \"blue\"]\n",
    "y_label = \"KL Divergence\"\n",
    "num_clusters = 10\n",
    "method = \"min\"\n",
    "\n",
    "plot_lineplot([cKit_KL_5fu30d, cKit_KL_5fu60d], method, num_clusters, labels, y_label, results_dir, colors)\n",
    "\n",
    "# For the ckit neighborhood\n",
    "if affinity_flag == \"MSE\":\n",
    "    # For cKit neighborhood MSE\n",
    "    cKit_MSE_5fu30d = elements_dist_5fu30d[\"cKit_MSE\"]\n",
    "    cKit_MSE_5fu60d = elements_dist_5fu60d[\"cKit_MSE\"]\n",
    "\n",
    "    # Define the labels and colors for the lineplot\n",
    "    labels = [\"cKit MSE 5fu30d\", \"cKit MSE 5fu60d\"]\n",
    "    colors = [\"red\", \"blue\"]\n",
    "    y_label = \"MSE\"\n",
    "    num_clusters = 10\n",
    "    method = \"min\"\n",
    "\n",
    "    plot_lineplot([cKit_MSE_5fu30d, cKit_MSE_5fu60d], method, num_clusters, labels, y_label, results_dir, colors)\n",
    "\n",
    "elif affinity_flag == \"CE\":\n",
    "    # For cKit neighborhood cross entropy (CE)\n",
    "    cKit_cross_entropy_5fu30d = elements_dist_5fu30d[\"cKit_CE\"]\n",
    "    cKit_cross_entropy_5fu60d = elements_dist_5fu60d[\"cKit_CE\"]\n",
    "\n",
    "    # Define the labels and colors for the lineplot\n",
    "    labels = [\"cKit CE 5fu30d\", \"cKit CE 5fu60d\"]\n",
    "    colors = [\"red\", \"blue\"]\n",
    "    y_label = \"Cross Entropy\"\n",
    "    num_clusters = 10\n",
    "    method = \"min\"\n",
    "\n",
    "    plot_lineplot([cKit_cross_entropy_5fu30d, cKit_cross_entropy_5fu60d], method, num_clusters, labels, y_label, results_dir, colors)\n",
    "else:\n",
    "    raise ValueError(\"Invalid affinity flag. Please choose either 'MSE' or 'CE'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b546a6",
   "metadata": {},
   "source": [
    "### 6.2 Spatial density map of HSCs, RDs, and cKits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c30c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KDE for the metabone of the HSCs, RDs\n",
    "def plot_SPDM(df, kde_results, common_grid, save_dir, color_map, bone_outline, filename=None, mesh = True, scatter = False, anno_text = None):\n",
    "\n",
    "\n",
    "    # Ensure that the sources are aligned with the keys of mk_color_map\n",
    "    # First, get the color map keys, excluding \"Bone\"\n",
    "    color_map_keys = [key for key in color_map.keys() if key != \"GFP\"]\n",
    "    \n",
    "    # Reorder sources to match the order of color_map_keys\n",
    "    sources = color_map_keys\n",
    "    \n",
    "    fig, axs = plt.subplots(len(sources), 1, figsize=(15 , 5 *len(sources)), sharex=True, sharey=True, constrained_layout=True)\n",
    "    if len(sources) == 1:\n",
    "        axs = [axs]  # since we only have one row, make it iterable\n",
    "\n",
    "    for i, source in enumerate(sources):\n",
    "        ax = axs[i]\n",
    "        \n",
    "\n",
    "        xi, yi, _ = common_grid # TODO: use the dict to save the common grid so we can use string to index\n",
    "        zi = kde_results[source]\n",
    "        # Masked the values outside the bone outline\n",
    "        mask = points_in_polygon(xi.flatten(), yi.flatten(), bone_outline).reshape(xi.shape)\n",
    "        zi[~mask] = 0\n",
    "\n",
    "        # Filter only the non-zero values of zi for percentile calculation (i.e., exclude outside the mask)\n",
    "        zi_inside_mask = zi[mask]\n",
    "\n",
    "        # Normalize the values of zi for the full plot\n",
    "        norm = Normalize(vmin=zi_inside_mask.min(), vmax=zi_inside_mask.max())\n",
    "        normed_z = norm(zi)\n",
    "        normed_z[~mask] = 0\n",
    "        # Set the alpha transparency to 1 for test only\n",
    "        # normed_z[mask] = 1    \n",
    "        colors = np.array(plt.cm.colors.hex2color(color_map[source]))\n",
    "        rgba_colors = np.zeros((*zi.shape, 4))\n",
    "        rgba_colors[..., :3] = colors[:3]  # RGB values\n",
    "        rgba_colors[..., -1] = normed_z  # Alpha transparency \n",
    "        \n",
    "        # rgba_colors[zi < np.percentile(zi, 80)] = 0\n",
    "        if anno_text is None:\n",
    "            ax.set_title(source)\n",
    "        else:\n",
    "            ax.set_title(f\"{source}, {anno_text[i]}\")\n",
    "        xlim_min, ylim_min = bone_outline.min(axis=0)\n",
    "        xlim_max, ylim_max = bone_outline.max(axis=0)\n",
    "        # Convert them into integers\n",
    "        xlim_min, xlim_max = int(xlim_min)-500, int(xlim_max)+500\n",
    "        ylim_min, ylim_max = int(ylim_min)-300, int(ylim_max)+300\n",
    "        \n",
    "        ax.set_xlim(xlim_min, xlim_max)\n",
    "        ax.set_ylim(ylim_min, ylim_max)\n",
    "        ax.set_xlabel(\"Position.X\")\n",
    "        ax.set_ylabel(\"Position.Y\")\n",
    "        if mesh:\n",
    "            ax.pcolormesh(xi, yi, rgba_colors, shading=\"auto\", rasterized=True)\n",
    "        \n",
    "        percentiles = np.arange(0, 81, 20)\n",
    "        contour_levels = np.unique(np.percentile(norm(zi_inside_mask), percentiles))\n",
    "        colors = color_map[source]\n",
    "        if len(contour_levels) > 1:\n",
    "\n",
    "            contour = ax.contour(xi, yi, normed_z, levels=contour_levels, linewidths=1, colors=\"black\", alpha=0.5)\n",
    "        \n",
    "        # Plot the x, y of each bone group as the background\n",
    "        if scatter:\n",
    "            source_positions = df[df[\"source\"] == source]\n",
    "            ax.scatter(source_positions[\"Position.X\"], source_positions[\"Position.Y\"], s=1, c=color_map[source], alpha=1)\n",
    "        \n",
    "\n",
    "        ax.plot(bone_outline[:,0], bone_outline[:, 1], color=\"black\")\n",
    "        ax.grid(False)\n",
    "\n",
    "    # Save the figure for the current bone group in the corresponding directory\n",
    "    fig.suptitle(f\"Spatial Distribution Estimation of {filename}\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_results_5fu30d_bones, _ = kde_for_clusters(transformed_5fu30d_bones_df, bone_outline=ref_outline, binsize=10)\n",
    "kde_results_5fu60d_bones, _ = kde_for_clusters(transformed_5fu60d_bones_df, bone_outline=ref_outline, binsize=10)\n",
    "\n",
    "with open(f\"{results_dir}/kde_results_5fu30d_bones.pkl\", \"wb\") as f:\n",
    "    pickle.dump(kde_results_5fu30d_bones, f)\n",
    "with open(f\"{results_dir}/kde_results_5fu60d_bones.pkl\", \"wb\") as f:\n",
    "    pickle.dump(kde_results_5fu60d_bones, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the kde results from the pickle files\n",
    "with open(f\"{results_dir}/kde_results_3mo_ref.pkl\", \"rb\") as f:\n",
    "    kde_results_3mo_bones = pickle.load(f)\n",
    "with open(f\"{results_dir}/kde_results_12mo_ref.pkl\", \"rb\") as f:\n",
    "    kde_results_12mo_bones = pickle.load(f)\n",
    "with open(f\"{results_dir}/kde_results_20mo_ref.pkl\", \"rb\") as f:\n",
    "    kde_results_20mo_bones = pickle.load(f)\n",
    "with open(f\"{results_dir}/kde_results_5fu30d_bones.pkl\", \"rb\") as f:\n",
    "    kde_results_5fu30d_bones = pickle.load(f)\n",
    "with open(f\"{results_dir}/kde_results_5fu60d_bones.pkl\", \"rb\") as f:\n",
    "    kde_results_5fu60d_bones = pickle.load(f)\n",
    "    \n",
    "with open(f\"{results_dir}/ref_grid.pkl\", \"rb\") as f:\n",
    "    ref_grid = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSCs, RDs\n",
    "plot_SPDM(transformed_3mo_bones_df, kde_results_3mo_bones, ref_grid, results_dir, hsc_rd_colors, ref_outline, \"3mo_HSCs_RDs\", mesh = True, scatter = True)\n",
    "plot_SPDM(transformed_12mo_bones_df, kde_results_12mo_bones, ref_grid, results_dir, hsc_rd_colors, ref_outline, \"12mo_HSCs_RDs\", mesh = True, scatter = True)\n",
    "plot_SPDM(transformed_20mo_bones_df, kde_results_20mo_bones, ref_grid, results_dir, hsc_rd_colors, ref_outline, \"20mo_HSCs_RDs\", mesh = True, scatter = True)\n",
    "plot_SPDM(transformed_5fu30d_bones_df, kde_results_5fu30d_bones, ref_grid, results_dir, hsc_rd_colors, ref_outline, \"5fu30d_HSCs_RDs\", mesh = True, scatter = True)\n",
    "plot_SPDM(transformed_5fu60d_bones_df, kde_results_5fu60d_bones, ref_grid, results_dir, hsc_rd_colors, ref_outline, \"5fu60d_HSCs_RDs\", mesh = True, scatter = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f35c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cKits\n",
    "plot_SPDM(transformed_3mo_bones_df, kde_results_3mo_bones, ref_grid, results_dir, ckits_color_map, ref_outline, \"3mo_cKits\", mesh = True, scatter = False)\n",
    "plot_SPDM(transformed_12mo_bones_df, kde_results_12mo_bones, ref_grid, results_dir, ckits_color_map, ref_outline, \"12mo_cKits\", mesh = True, scatter = False)\n",
    "plot_SPDM(transformed_20mo_bones_df, kde_results_20mo_bones, ref_grid, results_dir, ckits_color_map, ref_outline, \"20mo_cKits\", mesh = True, scatter = False)\n",
    "plot_SPDM(transformed_5fu30d_bones_df, kde_results_5fu30d_bones, ref_grid, results_dir, ckits_color_map, ref_outline, \"5fu30d_cKits\", mesh = True, scatter = False)\n",
    "plot_SPDM(transformed_5fu60d_bones_df, kde_results_5fu60d_bones, ref_grid, results_dir, ckits_color_map, ref_outline, \"5fu60d_cKits\", mesh = True, scatter = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2ff29",
   "metadata": {},
   "source": [
    "### 6.3 Heatmap of the cKits affinity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the interpolated affinity matrices \n",
    "def plot_affinity_heatmap(df, title, save_path=None, diagonal_text=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for the given interpolated affinity matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Interpolated affinity matrix dataframe.\n",
    "    - title (str): Title for the heatmap.\n",
    "    - save_path (str, optional): Path to save the figure. If None, the figure is not saved.\n",
    "    - diagonal_text (dict, optional): Dictionary mapping row indices to text to display on the diagonal.\n",
    "                                    If None, the diagonal will remain black without text.\n",
    "    \"\"\"\n",
    "    # Convert matrix to DataFrame if needed\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "    # Set diagonal to NaN so it can be masked\n",
    "    np.fill_diagonal(df.values, np.nan)\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Create a mask for NaN values\n",
    "    mask = np.isnan(df)\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(df, ax=ax, cmap=\"coolwarm\", annot=False, square=True, mask=mask, \n",
    "                cbar_kws={\"shrink\": 0.7, \"aspect\": 30, \"ticks\":[0.0, 0.2, 0.4, 0.55]}, rasterized=True, vmin=0, vmax=0.55)\n",
    "\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Set x and y labels\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Neighborhood\")\n",
    "\n",
    "    # Get matrix size\n",
    "    matrix_size = df.shape[0]\n",
    "\n",
    "    # Add black horizontal lines between rows\n",
    "    for i in range(1, matrix_size):\n",
    "        ax.hlines(i, *ax.get_xlim(), colors=\"black\", linewidth=1)\n",
    "\n",
    "    # Overlay black patches for diagonal and optional text\n",
    "    for i in range(matrix_size):\n",
    "        ax.add_patch(plt.Rectangle((i, i), 1, 1, color=\"black\", ec=None))  # Black diagonal\n",
    "        \n",
    "        if diagonal_text:  # If diagonal_text is provided, add text\n",
    "            ax.text(i + 0.5, i + 0.5, diagonal_text.get(i, \"\"), ha=\"center\", va=\"center\", \n",
    "                    color=\"white\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "    # Set x-ticks and y-ticks from 1 to matrix_size\n",
    "    ax.set_xticks(np.arange(matrix_size) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_size) + 0.5)\n",
    "    ax.set_xticklabels(np.arange(1, matrix_size + 1))\n",
    "    ax.set_yticklabels(np.arange(1, matrix_size + 1))\n",
    "\n",
    "    # Save the figure if save_path is provided\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized affinity matrix for all the data\n",
    "for key in [\"3mo\", \"12mo\", \"20mo\", \"5fu30d\", \"5fu60d\"]:\n",
    "    # Normalize the affinity matrix by row\n",
    "    affinity_matrix_plot = affinity_matrices[key].copy()\n",
    "    affinity_matrix_plot = affinity_matrix_plot / affinity_matrix_plot.sum(axis=1).to_numpy()[:, np.newaxis]\n",
    "    # Pick specific rows\n",
    "    row_picked = [4, 8]\n",
    "    \n",
    "    file_path = os.path.join(results_dir, f\"{datetime.today().strftime(\"%y%m%d\")}_HeM-HeM Association_{key}.pdf\")\n",
    "    plot_affinity_heatmap(affinity_matrix_plot, f\"Normalized HeM-HeM Association for {key}\", save_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf42092",
   "metadata": {},
   "source": [
    "### 6.4 Bone age visualization with the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae520cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the weights we rescale the size of the plot\n",
    "ckit_pdf_size = final_weights[0] * 100\n",
    "hsc_pdf_size = final_weights[1] * 100\n",
    "ckit_affinity_size = final_weights[2] * 100\n",
    "hsc_num_size = final_weights[3] * 100\n",
    "hsc_cluster_comp_size = final_weights[4] * 100\n",
    "\n",
    "# Set minimum size as 1 based on the smallest weight\n",
    "# Scale the other size with log5\n",
    "min_size = 1\n",
    "ckit_pdf_size = min_size + np.log(ckit_pdf_size) / np.log(2)\n",
    "hsc_pdf_size = min_size + np.log(hsc_pdf_size) / np.log(2)\n",
    "ckit_affinity_size = min_size + np.log(ckit_affinity_size) / np.log(2)\n",
    "hsc_num_size = min_size + np.log(hsc_num_size) / np.log(2)\n",
    "hsc_cluster_comp_size = min_size + np.log(hsc_cluster_comp_size) / np.log(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "size_factor = 1000\n",
    "final_weights = np.array([cKit_pdf_weight, HSC_pdf_weight, cKit_affinity_weight, HSC_num_weight, HSC_cluster_comp_weight])\n",
    "\n",
    "y_scale = 1.3\n",
    "alpha = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8*y_scale)) \n",
    "\n",
    "# Read pkl files for the estimated ages\n",
    "# Read the estimated ages for 5fu30d and 5fu60d with HSC cluster composition\n",
    "with open(os.path.join(results_dir, f\"{datetime.today().strftime(\"%y%m%d\")}_estimated_ages_5fu30d_NoNumCluster_{affinity_flag}.pkl\"), \"rb\") as f:\n",
    "    estimated_ages_5fu30d = pickle.load(f)\n",
    "with open(os.path.join(results_dir, f\"{datetime.today().strftime(\"%y%m%d\")}_estimated_ages_5fu60d_NoNumCluster_{affinity_flag}.pkl\"), \"rb\") as f:\n",
    "    estimated_ages_5fu60d = pickle.load(f)\n",
    "    \n",
    "# Plot the estimated ages for 5fu30d and 5fu60d as scatter plots\n",
    "scatter_5fu30d = ax.scatter(estimated_ages_5fu30d[\"cKit_KDE\"], [c+3 for c in clusters], color=\"red\", marker=\"o\", s=[int(s*size_factor*ckit_pdf_size) for s in cluster_size_5fu30d[\"cKits\"].values()], label=\"5fu30d HeM distribution\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu30d[\"merged_HSC_KDE\"], 1, color=\"red\", marker=\"X\", s=50*hsc_pdf_size, label=\"5fu30d HSC distribution\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu30d[\"cKit_affinity\"], [c+3 for c in clusters], color=\"red\", marker=\"^\",s=[int(s*size_factor*ckit_affinity_size) for s in cluster_size_5fu30d[\"cKits\"].values()], label=\"5fu30d HeM neighborhood\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu30d[\"HSC_numbers\"], 0, color=\"red\", marker=\"D\", s=50*hsc_num_size, label=\"5fu30d HSC numbers\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu30d[\"HSC_cluster_comp\"], 2, color=\"red\", marker=\"s\", s=50*hsc_cluster_comp_size, label=\"5fu30d HSC Cluster Composition\", alpha = alpha, edgecolors=\"black\") \n",
    "\n",
    "ax.scatter(estimated_ages_5fu60d[\"cKit_KDE\"], [c+3 for c in clusters], color=\"blue\", marker=\"o\", s=[int(s*size_factor*ckit_pdf_size) for s in cluster_size_5fu60d[\"cKits\"].values()], label=\"5fu60d HeM distribution\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu60d[\"merged_HSC_KDE\"], 1, color=\"blue\", marker=\"X\", s=50*hsc_pdf_size, label=\"5fu60d HSC distribution\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu60d[\"cKit_affinity\"], [c+3 for c in clusters], color=\"blue\", marker=\"^\", s=[int(s*size_factor*ckit_affinity_size) for s in cluster_size_5fu60d[\"cKits\"].values()], label=\"5fu60d HeM neighborhood\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu60d[\"HSC_numbers\"], 0, color=\"blue\", marker=\"D\", s=50*hsc_num_size, label=\"5fu60d HSC numbers\", alpha = alpha, edgecolors=\"black\")\n",
    "ax.scatter(estimated_ages_5fu60d[\"HSC_cluster_comp\"], 2, color=\"blue\", marker=\"s\", s=50*hsc_cluster_comp_size, label=\"5fu60d HSC Cluster Composition\", alpha = alpha, edgecolors=\"black\")\n",
    "\n",
    "# Add the final estimated ages for 5fu30d and 5fu60d\n",
    "ax.axvline(final_age_5fu30d, color=\"red\", linestyle=\"--\", label=f\"Weighted Average 5fu30d:{final_age_5fu30d:.2f}\", alpha = alpha)\n",
    "ax.axvline(final_age_5fu60d, color=\"blue\", linestyle=\"--\", label=f\"Weighted Average 5fu60d:{final_age_5fu60d:.2f}\", alpha = alpha)\n",
    "\n",
    "# Reference vertical lines\n",
    "ax.axvline(3, color=\"black\", linestyle=\":\", label=\"3mo\", alpha = 0.3)\n",
    "ax.axvline(12, color=\"black\", linestyle=\":\", label=\"12mo\", alpha = 0.3)\n",
    "ax.axvline(20, color=\"black\", linestyle=\":\", label=\"20mo\", alpha = 0.3)\n",
    "\n",
    "# Axes labels, title, and limits\n",
    "ax.set_xlim(2, 21)\n",
    "ax.set_xticks([3, 12, 20])\n",
    "ax.set_xlabel(\"Age (months)\")\n",
    "ax.set_ylabel(\"Cluster\")\n",
    "ax.set_title(\"Estimated Age of 5fu30d and 5fu60d\") # and 5fu60d\n",
    "ax.set_yticks(clusters + [len(clusters), len(clusters)+1, len(clusters)+2])\n",
    "ax.set_yticklabels([\"HSC numbers\"]+[\"HSC distribution\"]+[\"HSC Cluster Composition\"]+[f\"HeM# {clusters+1}\" for clusters in clusters])\n",
    "# Invert the yticklabels, starting with HSCs num, HSCs pdf, and then the clusters from 10 to 1\n",
    "ax.invert_yaxis()\n",
    "\n",
    "\n",
    "\n",
    "# Combine legends\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "labels2 = [f\"HeM distribution weight: {final_weights[0]:.4f}\", f\"HSC distribution weight: {final_weights[1]:.4f}\", \n",
    "        f\"HeM neighborhood weight: {final_weights[2]:.4f}\", f\"HSC numbers weight: {final_weights[3]:.4f}\", f\"HSC cluster composition weight: {final_weights[4]:.4f}\"]\n",
    "\n",
    "\n",
    "# Create custom handles for labels2 and labels3\n",
    "handles2 = [Line2D([0], [0], marker=\"o\", color=\"w\", label=label,\n",
    "                        markerfacecolor=\"white\", markersize=10) for label in labels2]\n",
    "\n",
    "\n",
    "# Create combined legend\n",
    "handles = handles1 + handles2\n",
    "labels = labels1 + labels2 \n",
    "\n",
    "ax.legend(handles, labels, title=\"Estimated Age and Cluster Size\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=14, title_fontsize=14)\n",
    "\n",
    "# Grid\n",
    "ax.yaxis.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Save and show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3126d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap (adjusting the order)\n",
    "heatmap_data = np.zeros((2, 23))\n",
    "\n",
    "# HeM Distribution (1-10)\n",
    "for cluster in range(10):\n",
    "    heatmap_data[0, cluster] = estimated_ages_5fu30d[\"cKit_KDE\"][cluster] \n",
    "    heatmap_data[1, cluster] = estimated_ages_5fu60d[\"cKit_KDE\"][cluster] \n",
    "\n",
    "# HeM-HeM Neighborhood (1-10)\n",
    "for cluster in range(10):\n",
    "    heatmap_data[0, 10 + cluster] = estimated_ages_5fu30d[\"cKit_affinity\"][cluster] \n",
    "    heatmap_data[1, 10 + cluster] = estimated_ages_5fu60d[\"cKit_affinity\"][cluster] \n",
    "\n",
    "# HSC Distribution\n",
    "heatmap_data[0, 20] = estimated_ages_5fu30d[\"merged_HSC_KDE\"] \n",
    "heatmap_data[1, 20] = estimated_ages_5fu60d[\"merged_HSC_KDE\"] \n",
    "\n",
    "# HSC Numbers\n",
    "heatmap_data[0, 21] = estimated_ages_5fu30d[\"HSC_numbers\"] \n",
    "heatmap_data[1, 21] = estimated_ages_5fu60d[\"HSC_numbers\"] \n",
    "\n",
    "# HSC Cluster Composition\n",
    "heatmap_data[0, 22] = estimated_ages_5fu30d[\"HSC_cluster_comp\"] \n",
    "heatmap_data[1, 22] = estimated_ages_5fu60d[\"HSC_cluster_comp\"] \n",
    "\n",
    "# Plotting the heatmap\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "# Custom x-tick labels: Cluster numbers 1-10\n",
    "# x_labels = [str(i + 1) for i in range(10)] + [str(i + 1) for i in range(10)] + [\"\", \"\"] + [str(i + 1) for i in range(10)]\n",
    "x_labels = [str(i + 1) for i in range(10)] + [str(i + 1) for i in range(10)] + [\"\", \"\", \"\"]\n",
    "\n",
    "# Generate heatmap with square cells and specified color bar range\n",
    "sns.heatmap(heatmap_data, cmap=\"Greens\", xticklabels=x_labels, yticklabels=[\"5FU30d\", \"5FU60d\"],\n",
    "            cbar_kws={\"label\": \"Estimated Age (months)\", \"ticks\": [3, 6, 9, 12, 15, 18, 20]}, \n",
    "            vmin=3, vmax=20, square=True, ax=ax)\n",
    "\n",
    "# Adjust x-tick rotation to 0 degrees for cluster numbers\n",
    "ax.set_xticklabels(x_labels, rotation=0)\n",
    "\n",
    "# Adding sub-labels in the middle of each feature block\n",
    "ax.text(5, 3.5, \"HeM distribution\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.text(15, 3.5, \"HeM-HeM neighborhood\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.text(20.5, 3.5, \"HSC distribution\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.text(21.5, 3, \"HSC numbers\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.text(22.5, 2.5, \"HSC cluster composition\", ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Adding arrowhead annotations for values smaller than 4.5 using Matplotlib triangle marker\n",
    "for i in range(2):  # Rows: 5FU30d and 5FU60d\n",
    "    for j in range(23):  # Columns: All features\n",
    "        # Left arrowhead for values < 1.5\n",
    "        if heatmap_data[i, j] < 4.5:\n",
    "            ax.scatter(j + 0.5, i + 0.5, marker=\"<\", color=\"black\", s=100)\n",
    "        \n",
    "        # First mask: Difference > 15.5 (use a red star marker)\n",
    "        if heatmap_data[i, j] > 18.5:\n",
    "            ax.scatter(j + 0.5, i + 0.5, marker=\">\", color=\"black\", s=100)\n",
    "        \n",
    "        # Second mask: Difference between 7.5 and 10.5 (use an orange plus marker)\n",
    "        if 10.5 < heatmap_data[i, j] < 13.5:\n",
    "            ax.scatter(j + 0.5, i + 0.5, marker=\"o\", color=\"black\", s=100)\n",
    "        \n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker=\"<\", color=\"w\", label=\"3mo conserve\", markerfacecolor=\"black\", markersize=10),\n",
    "    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"12mo conserve\", markerfacecolor=\"black\", markersize=10),\n",
    "    Line2D([0], [0], marker=\">\", color=\"w\", label=\"20mo conserve\", markerfacecolor=\"black\", markersize=10),\n",
    "    \n",
    "]\n",
    "\n",
    "# Add the legend to the plot (horizontal at the bottom)\n",
    "ax.legend(handles=legend_elements, loc=\"lower center\", bbox_to_anchor=(0.5, -2), \n",
    "        title=\"Marker Legend\", fontsize=12, title_fontsize=14, ncol=3, frameon=False)\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Estimated Biological Age of 5FU-Treated Bone Marrow Samples\")\n",
    "plt.ylabel(\"Treatment\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3bebe1",
   "metadata": {},
   "source": [
    "### 6.5 SHAP (Gaussian model) visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5721dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure SHAP only uses mean predictions from GPR\n",
    "explainer = shap.Explainer(lambda x: gpr_model.predict(x, return_std=False), X_scaled, feature_names=list(X.columns.map(features_name_dict)))\n",
    "# explainer = shap.LinearExplainer(ridge_model, X_scaled, feature_names=X.columns)\n",
    "\n",
    "for i, features_test in enumerate([features_5fu30d, features_5fu60d]):\n",
    "    dataset_name = \"5fu30d\" if i == 0 else \"5fu60d\"\n",
    "    print(f\"Test Condition: {dataset_name}\")\n",
    "\n",
    "    # Convert test sample to DataFrame and standardize it\n",
    "    features_test = pd.DataFrame([features_test])\n",
    "    test_features_scaled = scaler.transform(features_test)\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer(test_features_scaled)\n",
    "    \n",
    "    # Create SHAP waterfall plot\n",
    "    shap.plots.waterfall(shap_values[0], max_display=len(X.columns), show=False)\n",
    "\n",
    "    # Get mean prediction and standard deviation (uncertainty)\n",
    "    y_pred, y_std = gpr_model.predict(test_features_scaled, return_std=True)\n",
    "    \n",
    "    # Update title\n",
    "    # plt.title(f\"SHAP Waterfall Plot for {dataset_name} with Ridge Regression\", fontsize=14)\n",
    "    plt.title(f\"SHAP Waterfall Plot for {dataset_name}\\nPrediction: {y_pred[0]:.2f}  {y_std[0]:.2f}\", fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
